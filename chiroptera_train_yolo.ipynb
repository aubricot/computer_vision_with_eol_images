{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "chiroptera_train_yolo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/object_detection_for_image_cropping/blob/master/chiroptera_train_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB",
        "colab_type": "text"
      },
      "source": [
        "# Training YOLO in Darkflow to detect bats (Chiroptera) from EOL images\n",
        "---\n",
        "*Last Updated 11 February 2020*   \n",
        "Use images and annotation files to train YOLO in Darkflow to detect bats from EOL images.\n",
        "\n",
        "Datasets exported from [preprocessing.ipynb](https://colab.research.google.com/github/aubricot/object_detection_for_image_cropping/blob/master/preprocessing.ipynb) were converted to xml formatted annotation files before use in this notebook. Images were already downloaded to Google Drive in preprocessing.ipynb. \n",
        "\n",
        "Annotations should be uploaded to Google Drive for use in this notebook after installing darkflow (under Installs below).\n",
        "\n",
        "Exported detection results (json files) can be used to calculate model precision for comparison with Faster-RCNN and SSD models using [calculate_error_mAP.ipynb](https://colab.research.google.com/github/aubricot/object_detection_for_image_cropping/blob/master/calculate_error_mAP.ipynb). \n",
        "\n",
        "Notes:   \n",
        "* For each 24 hour period on Google Colab, you have up to 12 hours of GPU access. Training the object detection model on bats took 30 hours split into 3 days.\n",
        "\n",
        "* Make sure to set the runtime to Python 2 with GPU Hardware Accelerator.   \n",
        "\n",
        "References:   \n",
        "* [Official Darkflow training instructions](https://github.com/thtrieu/darkflow)   \n",
        "* [Medium Blog on training using YOLO via Darkflow in Colab](https://medium.com/coinmonks/detecting-custom-objects-in-images-video-using-yolo-with-darkflow-1ff119fa002f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJz5m4BKmJZD",
        "colab_type": "text"
      },
      "source": [
        "## Installs\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj3-JKOsIPtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change to your working directory\n",
        "%cd drive/My Drive/fall19_smithsonian_informatics/train\n",
        "\n",
        "# Install libraries\n",
        "# Make sure you are using Python 3.6\n",
        "!python --version\n",
        "!pip install tensorflow-gpu==1.15.0rc2\n",
        "!pip install cython\n",
        "!pip install opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKYxuyq5Ib_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download darkflow (the tensorflow implementation of YOLO)\n",
        "import os\n",
        "import pathlib\n",
        "import shutil \n",
        "\n",
        "if os.path.exists(\"darkflow-master\"):\n",
        "  %cd darkflow-master/darkflow\n",
        "  !pwd\n",
        "\n",
        "elif not os.path.exists(\"darkflow-master\"):\n",
        "    !git clone --depth 1 https://github.com/thtrieu/darkflow.git\n",
        "    # Compile darkflow\n",
        "    %cd darkflow\n",
        "    !python setup.py build_ext --inplace\n",
        "    # Rename darkflow to darkflow-master to distinguish between folder names\n",
        "    shutil.move('/content/drive/My Drive/fall19_smithsonian_informatics/train/darkflow', \n",
        "              '/content/drive/My Drive/fall19_smithsonian_informatics/train/darkflow-master')\n",
        "\n",
        "# Change wd to darkflow-master\n",
        "%cd ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JoiWSqU6CBY",
        "colab_type": "text"
      },
      "source": [
        "#### Before proceeding to the next steps, you should manually upload annotations to your Google Drive. Test annotations should be uploaded to train/test_ann. Train annotations should be uploaded to darkflow-master/test/training/annotations. After uploading, return to this notebook and click refresh in the file browser on the left."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz",
        "colab_type": "text"
      },
      "source": [
        "### Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd darkflow-master\n",
        "\n",
        "# For importing/exporting files, working with arrays, etc\n",
        "from google.colab import files\n",
        "import os\n",
        "import pathlib\n",
        "import imageio\n",
        "import time\n",
        "import csv\n",
        "import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For the actual object detection\n",
        "from darkflow.net.build import TFNet\n",
        "\n",
        "# For drawing onto and plotting the images\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fF0fSxmJZR",
        "colab_type": "text"
      },
      "source": [
        "### Model Preparation (only need to run these once)\n",
        "---   \n",
        "For detailed instructions on training YOLO using a custom dataset, see the [Darkflow GitHub Repository](https://github.com/thtrieu/darkflow)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268I2Ev_mJZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test installation, you should see an output with different parameters for flow\n",
        "!python flow --h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SXQ5bjUzA0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload yolo.weights, pre-trained weights file (for YOLO v2) from Google drive \n",
        "weights = 'bin/yolo'\n",
        "weights_file = weights + '.weights'\n",
        "if not os.path.exists('weights_file'): \n",
        "  !gdown --id 0B1tW_VtY7oniTnBYYWdqSHNGSUU\n",
        "  !mkdir bin\n",
        "  !mv yolo.weights bin\n",
        "\n",
        "# Make new label file/overwrite existing labels.txt downloaded with darkflow\n",
        "!echo \"Chiroptera\" > labels.txt\n",
        "\n",
        "# Download model config file edited for training darkflow to identify bats (yolo-1c = yolo to identify 1 class)\n",
        "mod_config = 'cfg/yolo-1c'\n",
        "mod_config_file = config + '.cfg'\n",
        "if not os.path.exists('mod_config_file'):\n",
        "  %cd cfg\n",
        "  !gdown --id 1bjt5Mqvf4AZSLNARgtgmZsfHZSyFj2yx\n",
        "  %cd ../"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3ouL5RM-mpX",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lOocNCc6uLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List different parameters for flow\n",
        "!python flow --h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn4U3HOTgPVX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train model (yolo-1c.cfg) using pre-trained weights from basal layers of yolo.weights, the top layer will be trained from scracth to detect Lepidoptera\n",
        "# Change the dataset and annotation directories to your paths in Google Drive\n",
        "%cd darkflow-master\n",
        "!python flow --model cfg/yolo-1c.cfg --train --trainer adam --load bin/yolo.weights --gpu 0.8 --epoch 3000 --dataset \"/content/drive/My Drive/fall19_smithsonian_informatics/train/images\" --annotation \"test/training/annotations\" --savepb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZoKtS-xhbQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resume training from last checkpoint\n",
        "!python flow --load -1 --model cfg/yolo-1c.cfg --train --savepb --trainer adam --gpu 0.8 --epoch 3000 --dataset \"/content/drive/My Drive/fall19_smithsonian_informatics/train/images\" --annotation \"test/training/annotations\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqA_lW7tbLeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the last checkpoint to protobuf file\n",
        "!python flow --model cfg/yolo-1c.cfg --load -1 --savepb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1nmQ2e77AEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resume training from protobuf file\n",
        "!python flow --load -1 --pbLoad built_graph/yolo-1c.pb --metaLoad built_graph/yolo-1c.meta --train --savepb --trainer adam --gpu 0.8 --epoch 3000 --dataset \"/content/drive/My Drive/fall19_smithsonian_informatics/train/images\" --annotation \"test/training/annotations\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zxZK7on7GM3",
        "colab_type": "text"
      },
      "source": [
        "### When finished training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GhWXLLlPNrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Export detection results as json files fo calculating mAP (mean average precision, a performance measure to compare models) using calculate_error_mAP.ipynb\n",
        "!python flow --pbLoad built_graph/yolo-1c.pb --gpu 0.8 --metaLoad built_graph/yolo-1c.meta --imgdir \"/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images\" --json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrFEPsSfeSg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optional: if want to run test images through detector AND SAVE OUTPUT IMAGES with detection boxes in test_images/out\n",
        "# If you want to only view detection boxes on images and not save images with detection boxes, go to \"Run test images\" below\n",
        "!python flow --pbLoad built_graph/yolo-1c.pb --gpu 0.8 --metaLoad built_graph/yolo-1c.meta --imgdir \"/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1zL-PctVuqZv"
      },
      "source": [
        "## Run test images through the trained object detector\n",
        "---\n",
        "Test image detection boxes are only needed for calculating mAP (mean average precision, a performance measure to compare models) and not for cropping. The functions below will only display resulting detection boxes on test images for visualization, but does not save their coordinates to a spreadsheet. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy3lSNqWOjq0",
        "colab_type": "text"
      },
      "source": [
        "### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PXMk2Rtsnk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For loading images into computer-readable format\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Function for loading images from urls\n",
        "def url_to_image(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image\n",
        "\n",
        "# For drawing bounding boxes around detected objects on images\n",
        "def boxing(image, predictions):\n",
        "    newImage = np.copy(image)\n",
        "    im_height, im_width, im_depth = image.shape\n",
        "\n",
        "    # Organize results of object detection for plotting and export\n",
        "    for result in predictions:\n",
        "        xmin = result['topleft']['x']\n",
        "        ymin = result['topleft']['y']\n",
        "\n",
        "        xmax = result['bottomright']['x']\n",
        "        ymax = result['bottomright']['y']\n",
        "\n",
        "        confidence = result['confidence']\n",
        "        label = result['label'] + \" \" + str(round(confidence, 3))\n",
        "\n",
        "        # Only show boxes that are above set confidence and for the label Chiroptera\n",
        "        # Optional: change confidence and label values\n",
        "        if confidence > 0 and result['label'] == 'Chiroptera' :\n",
        "            # Draw boxes on images\n",
        "            fontScale = min(im_width,im_height)/(600)\n",
        "            newImage = cv2.rectangle(newImage, (xmin, ymax), (xmax, ymin), (255, 0, 157), 3)\n",
        "            newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (153, 255, 255), 5, cv2.LINE_AA)\n",
        "    return newImage\n",
        "\n",
        "# Define parameters for \"flow\"ing the images through the model\n",
        "# Optional: adjust detection confidence threshold\n",
        "params = {\n",
        "    'model': 'cfg/yolo-1c.cfg',\n",
        "    'load': 'bin/yolo.weights',\n",
        "    'gpu': 0.8,\n",
        "    #'threshold': 0.1, \n",
        "    'pbLoad': 'built_graph/yolo-1c.pb', \n",
        "    'metaLoad': 'built_graph/yolo-1c.meta' \n",
        "}\n",
        "\n",
        "# Run the model\n",
        "tfnet = TFNet(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxI1a4iRSNQU",
        "colab_type": "text"
      },
      "source": [
        "### Run test images through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UbyA9e1omJZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test trained model on test images\n",
        "from PIL import Image\n",
        "\n",
        "# Update path to your test images\n",
        "PATH_TO_TEST_IMAGES_DIR = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images'\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS[:5], start=1):\n",
        "\n",
        "    # Load in image\n",
        "    image = Image.open(im_path)\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image_np)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on image\n",
        "    boxing(image_np, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 145 test images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(boxing(image_np, result))\n",
        "    plt.title('{}) Inference time: {}'.format(im_num, format(end_time-start_time, '.2f')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErD78FKVr0sc",
        "colab_type": "text"
      },
      "source": [
        "### Run other images (from individual URLs) through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szb8S54dr5Qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test trained model on test images\n",
        "from PIL import Image\n",
        "\n",
        "# Put your urls here\n",
        "image_urls = [\"https://upload.wikimedia.org/wikipedia/commons/b/be/Batman_%28retouched%29.jpg\",\n",
        "              \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Bela_Lugosi_as_Dracula%2C_anonymous_photograph_from_1931%2C_Universal_Studios.jpg/690px-Bela_Lugosi_as_Dracula%2C_anonymous_photograph_from_1931%2C_Universal_Studios.jpg\"]\n",
        "\n",
        "# Loops through image_urls\n",
        "for im_num, image_url in enumerate(image_urls, start=1):\n",
        "  try:\n",
        "    # Load in image\n",
        "    image_np = url_to_image(image_url)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image_np)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on image\n",
        "    boxing(image_np, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 2 test images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(boxing(image_np, result))\n",
        "    plt.title('{}) Inference time: {}'.format(im_num, format(end_time-start_time, '.2f')))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kaj5WWJF8K9-",
        "colab_type": "text"
      },
      "source": [
        "## Run EOL image bundles through the trained object detector & save results for cropping\n",
        "---\n",
        "Display resulting detection boxes on images and save their coordinates to lepidoptera_det_crops_yolo.tsv for use cropping EOL images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-S86Vuvz8PkM",
        "colab_type": "text"
      },
      "source": [
        "### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5GiH_HB8VA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For loading images into computer-readable format\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Function for loading images from urls\n",
        "def url_to_image(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image\n",
        "\n",
        "# For drawing bounding boxes around detected objects on images\n",
        "def boxing(image, predictions):\n",
        "    newImage = np.copy(image)\n",
        "    im_height, im_width, im_depth = image.shape\n",
        "\n",
        "    # Organize results of object detection for plotting and export\n",
        "    for result in predictions:\n",
        "        xmin = result['topleft']['x']\n",
        "        ymin = result['topleft']['y']\n",
        "\n",
        "        xmax = result['bottomright']['x']\n",
        "        ymax = result['bottomright']['y']\n",
        "\n",
        "        confidence = result['confidence']\n",
        "        label = result['label'] + \" \" + str(round(confidence, 3))\n",
        "\n",
        "        # Only show boxes that are above set confidence and for the label Chiroptera\n",
        "        if confidence > 0 and result['label'] == 'Chiroptera' :\n",
        "            # Draw boxes on images\n",
        "            fontScale = min(im_width,im_height)/(600)\n",
        "            newImage = cv2.rectangle(newImage, (xmin, ymax), (xmax, ymin), (255, 0, 157), 3)\n",
        "            newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (153, 255, 255), 5, cv2.LINE_AA)\n",
        "\n",
        "            # Export detection results to det_crops_yolo.tsv\n",
        "            # Change filename here if using 1000 or 20000 images dataset\n",
        "            with open('/content/drive/My Drive/fall19_smithsonian_informatics/chiroptera_det_crops_yolo_1000.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([image_url, im_height, im_width, \n",
        "                            xmin, ymin, xmax, ymax])\n",
        "\n",
        "    return newImage\n",
        "\n",
        "# Define parameters for \"flow\"ing the images through the model\n",
        "# Optional: adjust detection confidence threshold\n",
        "params = {\n",
        "    'model': 'cfg/yolo-1c.cfg',\n",
        "    'load': 'bin/yolo.weights',\n",
        "    'gpu': 0.8,\n",
        "    #'threshold': 0.1, \n",
        "    'pbLoad': 'built_graph/yolo-1c.pb', \n",
        "    'metaLoad': 'built_graph/yolo-1c.meta' \n",
        "}\n",
        "\n",
        "# Run the model\n",
        "tfnet = TFNet(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1URR0OD8SHA",
        "colab_type": "text"
      },
      "source": [
        "### Run images (from EOL image URL bundles) through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgT2QQE38k5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use URLs from EOL image URL bundles\n",
        "# Comment out to use either 1000 or 20000 image bundles\n",
        "# 1000 Lepidoptera images\n",
        "urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Chiroptera_breakdown_download_000001.txt'\n",
        "# 20000 Lepidoptera images\n",
        "#urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Chiroptera_20K_breakdown_download_000001.txt'\n",
        "\n",
        "df = pd.read_csv(urls)\n",
        "df.columns = [\"link\"]\n",
        "pd.DataFrame.head(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTujo29e8l7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write header row of output crops file\n",
        "# For 1000 or 20000 image datasets, change filename here and in \"Prepare object detection functions and settings -> def boxing -> Export detection results\" above\n",
        "with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/chiroptera_det_crops_1000.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"image_url\", \"im_height\", \"im_width\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_75Bltr8s9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "for i, row in df.head(5).itertuples(index=True, name='Pandas'):\n",
        "\n",
        "# For ranges of rows or all rows, use df.iloc\n",
        "# Can be useful if running detection in batches\n",
        "#for i, row in df.iloc[500:800].iterrows():\n",
        "\n",
        "  try:\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Load in image\n",
        "    image_url = df1.get_value(i, \"link\")\n",
        "    image = url_to_image(image_url)\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image_np)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on image\n",
        "    boxing(image_np, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 1000 test images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(boxing(image_np, result))\n",
        "    plt.title('{}) Inference time: {}'.format(i+1, format(end_time-start_time, '.2f')))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}