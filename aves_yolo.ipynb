{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "aves_yolo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/object_detection_for_image_cropping/blob/master/aves_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB",
        "colab_type": "text"
      },
      "source": [
        "# Using YOLO v2 in Darkflow to detect birds from images\n",
        "---\n",
        "*Last Updated 17 February 2020*   \n",
        "Using YOLO via Darkflow as a method to do customized, large-scale image processing. Using the location and dimensions of the detected birds, images will be cropped to square dimensions that are centered and padded around the detection box. Pre-trained models are used for \"out of the box\" inference on images of birds of varying dimensions and resolutions.\n",
        "\n",
        "This notebook is meant to be run enitrely in Google Colab and doesn't require any software installations or downloads to your local machine. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJz5m4BKmJZD",
        "colab_type": "text"
      },
      "source": [
        "## Installs\n",
        "---\n",
        "Install required libraries directly to this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yps_toNBmJZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure you are using Python 3.6\n",
        "# Install packages using pip\n",
        "!python --version\n",
        "!pip install tensorflow-gpu==1.15.0rc2\n",
        "!pip install cython\n",
        "!pip install opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTDwgHVFmJZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and build darkflow (the tensorflow implementation of YOLO)\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "if \"darkflow-master\" in pathlib.Path.cwd().parts:\n",
        "  while \"darkflow-master\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path(\"darkflow-master\").exists():\n",
        "  !git clone --depth 1 https://github.com/thtrieu/darkflow.git\n",
        "  # Compile darkflow\n",
        "  %cd darkflow\n",
        "  !python setup.py build_ext --inplace\n",
        "  # Change darkflow to darkflow-master to distinguish between folder names\n",
        "  %cd ../\n",
        "  !mv darkflow darkflow-master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (Optional) Mount google drive to export detection results as tsv\n",
        "# You can also test everything without running this cell block, detection results just won't be exported\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz",
        "colab_type": "text"
      },
      "source": [
        "### Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd darkflow-master\n",
        "\n",
        "%tensorflow_version 1.15.0rc2\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# For importing/exporting files, working with arrays, etc\n",
        "import pathlib\n",
        "import time\n",
        "import csv\n",
        "import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For the actual object detection\n",
        "from darkflow.net.build import TFNet\n",
        "\n",
        "# For drawing onto and plotting the images\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fF0fSxmJZR",
        "colab_type": "text"
      },
      "source": [
        "### Model Preparation\n",
        "---   \n",
        "**Uploads**: The models are already in darkflow/cfg, but the pre-trained weights associated with these models need to be uploaded to this notebook from https://drive.google.com/drive/folders/0B1tW_VtY7onidEwyQ2FtQVplWEU. \n",
        "\n",
        "**\"Flowing\" images through the model**: Ignore the warning messages about deprecated names, they still work at the time this last updated. Code for parameters is based on https://github.com/thtrieu/darkflow (\"Using darkflow from another python application\").\n",
        "\n",
        "Your output should be a table of values like those shown below:\n",
        "\n",
        "Source | Train? | Layer description                | Output size\n",
        "------- |:--------:|:----------------------------------:| ---------------\n",
        "       |        | input                            | (?, 448, 448, 3)\n",
        " Load  |  Yep!  | scale to (-1, 1)                 | (?, 448, 448, 3)\n",
        " Load  |  Yep!  | conv 3x3p1_1    leaky            | (?, 448, 448, 16)\n",
        "\n",
        "**Define boxing function**: You can adjust the parameters so that bounding boxes are only shown for certain confidence or class values. Here boxes are shown when confidence > 0.45 and object class is 'bird'. This function is modified from here https://gist.github.com/deep-diver/40f092ad56525189674a86b6fde6d304."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268I2Ev_mJZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test installation, you should see an output with different parameters for flow\n",
        "%cd darkflow-master\n",
        "!python flow --h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2wnfMcDmJZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload yolo.weights, pre-trained weights file (for YOLO v2) from Google drive \n",
        "# For directions to upload other weights files, see the wiki for this repository\n",
        "weights = 'yolo'\n",
        "weights_file = weights + '.weights'\n",
        "if not os.path.exists('weights_file'):\n",
        "  !gdown --id 0B1tW_VtY7oniTnBYYWdqSHNGSUU\n",
        "  !mkdir bin\n",
        "  !mv yolo.weights bin\n",
        "\n",
        "# Define parameters for \"flow\"ing the images through the model\n",
        "# Can change detection confidence threshold here\n",
        "params = {\n",
        "    'model': 'cfg/yolo.cfg',\n",
        "    'load': 'bin/yolo.weights',\n",
        "    'threshold': 0.45, \n",
        "    'gpu': 1.0\n",
        "}\n",
        "\n",
        "# Run the model\n",
        "tfnet = TFNet(params)\n",
        "\n",
        "# For uploading an image from url\n",
        "# Modified from https://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/\n",
        "def url_to_image(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        " \n",
        "  return image\n",
        "\n",
        "# For drawing bounding boxes around detected objects on images\n",
        "def boxing(image, predictions):\n",
        "    newImage = np.copy(image)\n",
        "    im_height, im_width, im_depth = image.shape\n",
        "        \n",
        "    for result in predictions:\n",
        "        xmin = result['topleft']['x']\n",
        "        ymin = result['topleft']['y']\n",
        "\n",
        "        xmax = result['bottomright']['x']\n",
        "        ymax = result['bottomright']['y']\n",
        "\n",
        "        confidence = result['confidence']\n",
        "        label = result['label'] + \" \" + str(round(confidence, 3))\n",
        "\n",
        "        # only show boxes that are above .1 confidence and for the label, bird\n",
        "        if confidence > 0.45 and result['label'] == 'bird' :\n",
        "            # draw boxes on images\n",
        "            fontScale = min(im_width,im_height)/(600)\n",
        "            newImage = cv2.rectangle(newImage, (xmin, ymax), (xmax, ymin), (255, 0, 157), 3)\n",
        "            newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (153, 255, 255), 5, cv2.LINE_AA)\n",
        "\n",
        "            # Optional: if mounted to Drive, export detection results to aves_det_crops_1000.tsv\n",
        "            # Note: if performing detection on larger image batches, can break up files into multiple parts aves_det_crops_2000_a.tsv\n",
        "            if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/aves_det_crops_1000.tsv'):\n",
        "              with open('/content/drive/My Drive/fall19_smithsonian_informatics/aves_det_crops_1000.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([image_url, im_height, im_width, \n",
        "                            xmin, ymin, xmax, ymax])\n",
        "            \n",
        "        else:\n",
        "          print(\"No birds detected in {}.\".format(image_url))\n",
        "    return newImage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1zL-PctVuqZv"
      },
      "source": [
        "## Load in sample images and 'flow' them through the object detector\n",
        "---\n",
        "You can either **A) Load individual images in by URL**, or for large image batches or **B) Load multiple images from a text file of image URLs**. Other methods for importing to Google Colab are listed [here](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=XDg9OBaYqRMd). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gd4TbiOEXff",
        "colab_type": "text"
      },
      "source": [
        "**A) Load individual images in by URL**\n",
        "Load in images by URL and run the image detector for all images. Plotted results include the image with bounding box around detected objects (birds), class type, and confidence score. Inference times are printed above images. If you \"mounted\" your Google Drive during \"Installs\", the bounding box coordinates will also be written to 'sample_crops_yolo.tsv'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lke_VvPzt71m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_urls = [\"https://content.eol.org/data/media/7e/9c/7a/542.15445377044.jpg\",\n",
        "              \"https://content.eol.org/data/media/81/1c/0d/542.7816025222.jpg\",\n",
        "              \"https://content.eol.org/data/media/7e/3c/0b/542.10578857864.jpg\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNsodQP3t_o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for image_url in image_urls:\n",
        "  image = url_to_image(image_url)\n",
        "\n",
        "  # Use YOLO for object detection  \n",
        "  # Record inference time\n",
        "  start_time = time.time()\n",
        "  result = tfnet.return_predict(image)\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Plot and show detection boxes on images\n",
        "  _, ax = plt.subplots(figsize=(10, 10))\n",
        "  ax.imshow(boxing(image, result))\n",
        "\n",
        "  # Display inference time above images\n",
        "  plt.title('Inference time: {}'.format(format(end_time-start_time, '.2f')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WStwvg5GmJZZ",
        "colab_type": "text"
      },
      "source": [
        "**B) Load multiple images (from EOL image URL bundles) through object detector**   \n",
        "Load in multiple images from a text file of URLS and run the image detector for all images. Plotted results include the image with bounding box around detected objects (birds), class type, and confidence score. Inference times are printed above images. If you \"mounted\" your Google Drive during \"Installs\", the bounding box coordinates will also be written to 'sample_crops_yolo.tsv'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gukCG-BVA6hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For 1000 or 20000 image datasets, change link below\n",
        "# 1000 Aves images\n",
        "urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_breakdown_download_000001.txt'\n",
        "# 20000 Aves images\n",
        "#urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_20K_breakdown_download_000001.txt'\n",
        "df = pd.read_csv(urls)\n",
        "df.columns = [\"link\"]\n",
        "pd.DataFrame.head(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUcRnRmzwJnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write header row of output crops file\n",
        "# For 1000 or 20000 image datasets, change filename here and in \"Prepare object detection functions and settings -> def boxing -> Export detection results\" above\n",
        "# Note: if performing detection on larger image batches, can break up files into multiple parts, ex: aves_det_crops_20000_a.tsv for df.iloc[0:5000].iterrows() below\n",
        "with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/aves_det_crops_1000.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"image_url\", \"im_height\", \"im_width\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UbyA9e1omJZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loops through first 5 image urls from the text file\n",
        "for i, row in df.head(5).itertuples(index=True, name='Pandas'):\n",
        "\n",
        "# For ranges of rows or all rows, use the commands below\n",
        "# Note: can be useful if running large image batches through in multiple parts\n",
        "#for i, row in df.iloc[0:5000].iterrows():\n",
        "#for i, row in df.iterrows():\n",
        "\n",
        "  try:\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    image_url = df.get_value(i, \"link\")\n",
        "    image = url_to_image(image_url)\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on images\n",
        "    boxing(image, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 1,000 images'.format(i+1))\n",
        "  \n",
        "  except:\n",
        "    print('Error: check if web address {} is valid'.format(image_url))\n",
        "  \n",
        "  # Plot and show detection boxes on images\n",
        "  # If running detection on >50 images, comment out this portion\n",
        "  _, ax = plt.subplots(figsize=(10, 10))\n",
        "  ax.imshow(boxing(image, result))\n",
        "  plt.title('{}) Inference time: {}'.format(i+1, format(end_time-start_time, '.2f')))\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}