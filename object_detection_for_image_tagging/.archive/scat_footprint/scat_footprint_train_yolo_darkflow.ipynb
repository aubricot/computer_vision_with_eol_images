{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "scat_footprint_train_yolo_darkflow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X2fF0fSxmJZR",
        "Oov92W3IwHYe"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_tagging/scat_footprint/scat_footprint_train_yolo_darkflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB"
      },
      "source": [
        "# Train YOLOv2 in Darkflow to detect scat and footprints from EOL images\n",
        "---\n",
        "*Last Updated 3 June 2021*  \n",
        "-Runs in Python 2 with Tensorflow 1.x-   \n",
        "--*Update as of 2 June 2021--Darkflow builds are no longer being updated and only support Tensorflow 1.x builds. As a result, this notebook is left in its state from 3 June 2021. Functions may become deprecated or lose functionality. For updated inference of scat and footprints with YOLOv4 in its native state, refer to [scat_footprint_train_yolov4.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_tagging/scat_footprint/scat_footprint_train_yolov4.ipynb).*--     \n",
        "\n",
        "Use images with annotations to train YOLOv2 implemented in Tensorflow (via [thtrieu's darkflow](https://github.com/thtrieu/darkflow)) to detect scat and footprints from EOL images. Detected scat and footprints will be used to add tags to images of birds (Aves), amphibians (Amphibia), reptiles (Reptilia), and mammals (Mammalia).\n",
        "\n",
        "Datasets were downloaded to Google Drive in [scat_footprint_preprocessing.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_tagging/scat_footprint/scat_footprint_preprocessing.ipynb). \n",
        "\n",
        "**YOLOv2 was trained for 4,000 epochs on 5 images to overfit, then for 1,000 epochs at lr=0.001 to reach a stable loss value (3), and finally for 1,000 epochs to refine learning with a slow rate at lr=0.0001.** Scat/footprint object detection models never learned despite adjusting augmentation and model hyperparameters for many training sessions. If successful approaches are found at a later date, steps for adding tags to images will be included. Custom anchor boxes were used to optimize coverage for the dataset and image augmentation was used to increase dataset size from 500 img per class to 1000 img, but loss never decreased below 3 and final mAP was <10%. \n",
        "\n",
        "Notes:   \n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change filepaths/taxon names where you see 'TO DO'     \n",
        "\n",
        "References:   \n",
        "* [Official Darkflow training instructions](https://github.com/thtrieu/darkflow)   \n",
        "* [Medium Blog on training using YOLO via Darkflow in Colab](https://medium.com/coinmonks/detecting-custom-objects-in-images-video-using-yolo-with-darkflow-1ff119fa002f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJz5m4BKmJZD"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj3-JKOsIPtc"
      },
      "source": [
        "# Install libraries\n",
        "# Make sure you are using Python 3.6\n",
        "!python --version\n",
        "!pip install tensorflow-gpu==1.15.0rc2\n",
        "!pip install cython\n",
        "!pip install opencv-python\n",
        "\n",
        "# For importing/exporting files, working with arrays, etc\n",
        "from google.colab import files\n",
        "import os\n",
        "import pathlib\n",
        "import shutil\n",
        "import imageio\n",
        "import time\n",
        "import csv\n",
        "import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For drawing onto and plotting the images\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fF0fSxmJZR"
      },
      "source": [
        "## Model preparation (only run once)\n",
        "---\n",
        "These blocks download and set-up files needed for training object detectors. After running once, you can train and re-train as many times as you'd like.\n",
        "\n",
        "For detailed instructions on training YOLO using a custom dataset, see the [Darkflow GitHub Repository](https://github.com/thtrieu/darkflow)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKYxuyq5Ib_f"
      },
      "source": [
        "# Download train and test dataset annotation files and install darkflow\n",
        "\n",
        "# TO DO: Type in the path to your working directory in form field to right\n",
        "basewd = \"/content/drive/MyDrive/train\" #@param {type:\"string\"}\n",
        "%cd $basewd\n",
        "\n",
        "# Download darkflow (the tensorflow implementation of YOLO)\n",
        "if os.path.exists(\"darkflow-master\"):\n",
        "    %cd darkflow-master\n",
        "    !pwd\n",
        "\n",
        "elif not os.path.exists(\"darkflow-master\"):\n",
        "    !git clone --depth 1 https://github.com/thtrieu/darkflow.git\n",
        "    # Compile darkflow\n",
        "    %cd darkflow\n",
        "    !python setup.py build_ext --inplace\n",
        "    # Rename darkflow to darkflow-master to distinguish between nested folder names\n",
        "    %cd ../\n",
        "    shutil.move('darkflow', 'darkflow-master')\n",
        "\n",
        "wd = 'darkflow-master'\n",
        "%cd $wd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268I2Ev_mJZL"
      },
      "source": [
        "# Test installation, you should see an output with different parameters for flow\n",
        "!python flow --h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SXQ5bjUzA0u"
      },
      "source": [
        "# Download other needed files for training\n",
        "\n",
        "# Upload yolo.weights, pre-trained weights file (for YOLO v2) from Google drive \n",
        "weights_file = 'bin/yolo.weights'\n",
        "if not os.path.exists('weights_file'):\n",
        "    !gdown --id 0B1tW_VtY7oniTnBYYWdqSHNGSUU\n",
        "    !mkdir bin\n",
        "    !mv yolo.weights bin\n",
        "    print('Successfully downloaded ', weights_file)\n",
        "\n",
        "# Make new label file/overwrite existing labels.txt downloaded with darkflow\n",
        "!echo 'scat' >labels.txt\n",
        "!echo 'footprint' >>labels.txt\n",
        "\n",
        "# Download model config file edited for training darkflow to identify 2 classes (yolo-2c = 2 classes)\n",
        "mod_config_file = 'cfg/yolo-2c-slowlr-anch.cfg'\n",
        "if not os.path.exists('mod_config_file'):\n",
        "    %cd cfg\n",
        "    !gdown --id 1wgKwWsnmJDOWzrimp3GTPtpKLoBGoyMg\n",
        "    %cd ../\n",
        "    print('Successfully downloaded ', model_config_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3ouL5RM-mpX"
      },
      "source": [
        "## Train the model\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oov92W3IwHYe"
      },
      "source": [
        "#### Build darkflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnBpq-ytrAxi"
      },
      "source": [
        "# Build darkflow\n",
        "\n",
        "%tensorflow_version 1.0\n",
        "\n",
        "# TO DO: Type in the path to your working directory in form field to right\n",
        "wd = \"/content/drive/MyDrive/train/darkflow-master\" #@param {type:\"string\"}\n",
        "%cd $wd\n",
        "\n",
        "# For the actual object detection\n",
        "!python setup.py build_ext --inplace\n",
        "from darkflow.net.build import TFNet\n",
        "\n",
        "# List different parameters for flow\n",
        "!python flow --h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peYeF3SpvRRj"
      },
      "source": [
        "#### Step 1) Pre-train by overfitting model on 3 images per class for n-epochs with a high learning rate (until loss gets as low as possible and accuracy gets as high as possible)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "UwKRHDziRR-5"
      },
      "source": [
        "import lxml.etree as ET\n",
        "\n",
        "# Make a mini dataset for pre-training\n",
        "%cd ../\n",
        "\n",
        "# Set up directory for mini dataset\n",
        "# TO DO: Type in the folder you would like to contain mini dataset\n",
        "folder = \"pretrain\" #@param {type:\"string\"}\n",
        "if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "    %cd $folder\n",
        "    os.makedirs(\"img\")\n",
        "    os.makedirs(\"ann\")\n",
        "    %cd ../\n",
        "\n",
        "# Move 3 images and annotations per class to pretrain/\n",
        "# To DO: Enter path to training image dataset\n",
        "train_img_path = \"tf2/images/\" #@param {type:\"string\"}\n",
        "train_ann_path = \"tf2/annotations/\" #@param\n",
        "pretrain_ann_dir = 'pretrain/ann/' #@param\n",
        "pretrain_img_dir = 'pretrain/img/' #@param\n",
        "\n",
        "# Randomly pick a pool of 20 annotation files\n",
        "num_files = 20 \n",
        "files = os.listdir(train_ann_path)\n",
        "filenames = np.random.choice(files, num_files)\n",
        "\n",
        "# Find 3 annotations for each image class\n",
        "# TO DO: Enter list of image classes\n",
        "image_classes = ['scat', 'footprint'] #@param \n",
        "class0_xmls = []\n",
        "class1_xmls = []\n",
        "for filename in filenames:\n",
        "    fpath = os.path.join(train_ann_path, filename)\n",
        "    tree = ET.parse(fpath)\n",
        "    root = tree.getroot()\n",
        "    for item in root.iter('name'):\n",
        "        if (item.text == image_classes[0]) and (len(class0_xmls) <= 2):\n",
        "            class0_xmls.append(fpath)\n",
        "        elif (item.text == image_classes[1]) and (len(class1_xmls) <= 2):\n",
        "            class1_xmls.append(fpath)\n",
        "\n",
        "# Move annotation files to pretrain/\n",
        "class_xmls = class0_xmls + class1_xmls\n",
        "for xml in class_xmls:\n",
        "    try:\n",
        "        shutil.move(xml, pretrain_ann_dir)\n",
        "    except:\n",
        "        pass\n",
        "print(\"Found {} annotations for {} & moved to {}\".format(len(class0_xmls), image_classes[0], pretrain_ann_dir))\n",
        "print(\"Found {} annotations for {} & moved to {}\".format(len(class1_xmls), image_classes[1], pretrain_ann_dir))\n",
        "\n",
        "# Get 3 images matching randomly selected xmls for each class\n",
        "def check_train_anns(train_dir, ann_dir):\n",
        "    train_imgs = os.listdir(train_img_path)\n",
        "    corresp_imgs = []\n",
        "    # Loop through train images to see if xml for each one\n",
        "    for train_img in train_imgs:\n",
        "        base = os.path.splitext(os.path.basename(train_img))[0]\n",
        "        train_xml = ann_dir + base + '.xml'\n",
        "        if os.path.exists(train_xml):\n",
        "            corresp_imgs.append(train_img)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    return corresp_imgs\n",
        "\n",
        "# Find images\n",
        "corresp_imgs = check_train_anns(train_img_path, pretrain_ann_dir)\n",
        "\n",
        "# Move images to pretrain/\n",
        "for img in corresp_imgs:\n",
        "    try:\n",
        "        fpath = 'images/' + img\n",
        "        shutil.move(fpath, pretrain_img_dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"\\nFound {} images matching xmls for {} and {} & moved to {}\".format(len(corresp_imgs), image_classes[0], image_classes[1], pretrain_img_dir))\n",
        "print(\"\\nSuccessfully created mini dataset for pretraining models!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKPwv-rjQq0S"
      },
      "source": [
        "# Define training parameters\n",
        "# Note: adjust learning rate by double clicking mod_config_file in Colab file explorer\n",
        "# Note contd: Colab text editor will open and you can adjust values and save before continuing\n",
        "%cd $wd\n",
        "\n",
        "# TO DO: Set up training parameters\n",
        "mod_config_file = \"cfg/yolo-2c-slowlr-anch.cfg\" #@param {type:\"string\"}\n",
        "weights = \"bin/yolo.weights\" #@param {type:\"string\"}\n",
        "ann_path = \"/content/drive/MyDrive/train/pretrain/ann\" #@param {type:\"string\"}\n",
        "img_path = \"/content/drive/My Drive/train/pretrain/img\" #@param {type:\"string\"}\n",
        "trainer = \"Adam\" #@param {type:\"string\"}\n",
        "epochs = 4000 #@param {type:\"integer\"}\n",
        "gpu = 0.8 #@param {type:\"slider\", min:0, max:0.8, step:0.1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zAVMR93vQlY"
      },
      "source": [
        "# Start training\n",
        "\n",
        "# Train model (yolo-2c_slowlr_anch.cfg) using pre-trained weights from basal layers of yolo.weights, the top layer will be trained from scracth to detect scat and footprints\n",
        "# Change the dataset and annotation directories to your paths in Google Drive\n",
        "!python flow --model {mod_config_file} --train --trainer {trainer} --load {weights} --gpu {gpu} --epoch {epochs} --dataset {img_path} --annotation {ann_path} --savepb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIxcPlzAwP2k"
      },
      "source": [
        "# Resume training \n",
        "%cd $wd\n",
        "\n",
        "# TO DO: Choose how many more epochs to train for\n",
        "more_epochs = 1000 #@param {type:\"integer\"}\n",
        "\n",
        "# Resume training from last checkpoint \n",
        "# useful if Google Drive timeout occurs or to train for a few more epochs\n",
        "!python flow --load -1 --model {mod_config_file} --train --savepb --trainer {trainer} --gpu {gpu} --epoch {more_epochs} --dataset {img_path} --annotation {ann_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feTIhGuIvUkZ"
      },
      "source": [
        "#### Step 2) Train on full dataset with intermediate learning rate until loss starts to stabilize (usually at a value b/w 1 - 5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnU9vH5HrjQB"
      },
      "source": [
        "# Define training parameters\n",
        "# Note: adjust learning rate by double clicking mod_config_file in Colab file explorer\n",
        "%cd $wd\n",
        "\n",
        "# TO DO: Set up training parameters\n",
        "mod_config_file = \"cfg/yolo-2c-slowlr-anch.cfg\" #@param {type:\"string\"}\n",
        "weights = \"bin/yolo.weights\" #@param {type:\"string\"}\n",
        "ann_path = \"/content/drive/MyDrive/train/tf2/annotations\" #@param {type:\"string\"}\n",
        "img_path = \"/content/drive/MyDrive/train/tf2/images\" #@param {type:\"string\"}\n",
        "trainer = \"Adam\" #@param {type:\"string\"}\n",
        "epochs = 100 #@param {type:\"integer\"}\n",
        "gpu = 0.8 #@param {type:\"slider\", min:0, max:0.8, step:0.1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn4U3HOTgPVX"
      },
      "source": [
        "# Train model (yolo-2c_slowlr_anch.cfg) using pre-trained weights from basal layers of yolo.weights that were pre-fit in Step 1 above\n",
        "# Change the dataset and annotation directories to your paths in Google Drive\n",
        "%cd $wd\n",
        "!python flow --model {mod_config_file} --train --trainer {trainer} --load {weights} --gpu {gpu} --epoch {epochs} --dataset {img_path} --annotation {ann_path} --savepb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdh2pHhkvXxE"
      },
      "source": [
        "#### Step 3) Train on full dataset with low learning rate (10x lower than step 2) to get best loss/accuracy values (loss <1, accuracy as close to 100% as possible)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZoKtS-xhbQm"
      },
      "source": [
        "# Resume training\n",
        "# Note: adjust learning rate by double clicking mod_config_file in Colab file explorer\n",
        "%cd $wd\n",
        "\n",
        "# TO DO: Choose how many more epochs to train for\n",
        "more_epochs = 100 #@param {type:\"integer\"}\n",
        "\n",
        "!python flow --load -1 --model {mod_config_file} --train --savepb --trainer {trainer} --gpu {gpu} --epoch {more_epochs} --dataset {img_path} --annotation {ann_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL_ZNF6QFQBS"
      },
      "source": [
        "#### Step 4) Save trained model to protobuf file (.pb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqA_lW7tbLeH"
      },
      "source": [
        "# Save the last checkpoint to protobuf file\n",
        "!python flow --model {mod_config_file} --load -1 --savepb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IZw-HlWrVxn"
      },
      "source": [
        "# If decide want to keep training, can resume training from protobuf file using cmds below\n",
        "\n",
        "# TO DO: Enter path to saved model protbuf file\n",
        "pb_file = \"built_graph/yolo-2c_slowlr_anch.pb\" #@param {type:\"string\"}\n",
        "meta_file = \"built_graph/yolo-2c_slowlr_anch.meta\" #@param {type:\"string\"}\n",
        "epochs = 100 #@param {type:\"integer\"}\n",
        "\n",
        "!python flow --load -1 --pbLoad {pb_file} --metaLoad {meta_file} --train --savepb --trainer {trainer} --gpu {gpu} --epoch {more_epochs} --dataset {img_dir} --annotation {ann_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLGOQiwSr-Gd"
      },
      "source": [
        "## Evaluate model accuracy\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Anc0dZjf7os"
      },
      "source": [
        "### Step 1) Export detection results as JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GhWXLLlPNrr"
      },
      "source": [
        "# Export detection results for test images as json files to calculate mAP (mean average precision, a performance measure to compare models) using calculate_error_mAP.ipynb\n",
        "%cd $wd\n",
        "\n",
        "# TO DO: Enter test images directory\n",
        "test_img_dir = \"/content/drive/MyDrive/train/tf2/test_images\" #@param {type:\"string\"}\n",
        "\n",
        "!python flow --pbLoad {pb_file} --gpu {gpu} --metaLoad {meta_file} --imgdir {test_img_dir} --json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqJOkggMgAVA"
      },
      "source": [
        "### Step 2) Use Cartucho's mAP library to evaluate model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj3qsk3BgQob"
      },
      "source": [
        "# Install the mAP repository to calculate error from detection results\n",
        "%cd $wd\n",
        "%cd ../\n",
        "if not os.path.exists(\"eval\"):\n",
        "  !mkdir eval\n",
        "  %cd eval\n",
        "  !git clone https://github.com/Cartucho/mAP\n",
        "  %cd ../\n",
        "\n",
        "# Move yolo detection results (jsons exported above) to detection-results/\n",
        "eval_results = test_img_dir + '/out' + '/*'\n",
        "!mv $eval_results eval/mAP/input/detection-results/\n",
        "eval_results = eval_results.replace('/*', '')\n",
        "!rm -rf $eval_results\n",
        "\n",
        "# Copy image annotations (xmls formatted with ground truth bounding boxes) to ground-truth/\n",
        "test_ann_dir = \"/content/drive/MyDrive/train/tf2/test_ann/\" #@param {type:\"string\"}\n",
        "test_ann_dir = test_ann_dir + '*'\n",
        "!cp $test_ann_dir eval/mAP/input/ground-truth/\n",
        "\n",
        "# Convert jsons to format needed for mAP calc\n",
        "%cd eval/mAP/scripts/extra\n",
        "!python convert_dr_darkflow_json.py\n",
        "# Convert xmls to format needed for mAP calc\n",
        "!python convert_gt_xml.py\n",
        "\n",
        "# Remove sample images in input/images-optional\n",
        "# cd to mAP\n",
        "%cd $wd\n",
        "%cd ../\n",
        "%cd eval/mAP\n",
        "!rm -rf input/images-optional/*\n",
        "\n",
        "# Calculate mAP for detection results\n",
        "# Output will be in mAP/results\n",
        "!python main.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}