{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "classification_display_test.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "TqUTFD4lbw3h",
        "0vb43eI0kbrf",
        "kvpQWx9nDY9-"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/flowers/classification_display_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB"
      },
      "source": [
        "# Display & export results from trained classification models for images\n",
        "---\n",
        "*Last Updated 11 Aug 2020*  \n",
        "*--Update as of 14 Oct 2021--Flower classification pipelines are no longer being updated. As a result, this script is left in its state from 11 Aug 2020. Functions may become deprecated or lose functionality. For updated classification of Flowers/Fruits, [go here](https://github.com/aubricot/computer_vision_with_eol_images/tree/master/classification_for_image_tagging/flower_fruit)--*    \n",
        "\n",
        "Display classification results from trained classification models on images and verify that they are as expected (or to further fine tune the classification model accordingly, ex: adjust hyperparameters from drop-down menus and re-train). Export resulting classifications to file for use as EOLv3 image tags.\n",
        "\n",
        "**Notes**\n",
        "* Change filepaths or information using the form fields to the right of code blocks (also noted in code with 'TO DO')\n",
        "* Make sure to set the runtime to GPU Hardware Accelerator with a High Ram Runtime Shape (Runtime -> Change runtime type) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz"
      },
      "source": [
        "## Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP"
      },
      "source": [
        "# For working with data and reading/displaying images\n",
        "import itertools\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.transform import resize\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# For image classification and training\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikRQ9LLbVHXQ"
      },
      "source": [
        "## Run images from URL through trained classifer without exporting results\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AloUBD0NPkJ"
      },
      "source": [
        "### Define functions and select model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuSAAH9kEV_C"
      },
      "source": [
        "import time\n",
        "# Load trained model from path\n",
        "start = time.time()\n",
        "model_selection = (\"06_inception\", 224) #@param [\"(\\\"20_fromscratch\\\", 150)\", \"(\\\"02_mobilenetssd\\\", 224)\", \"(\\\"06_inception\\\", 224)\"] {type:\"raw\"}\n",
        "TRAIN_SESS, pixels = model_selection\n",
        "saved_model_path = '/content/drive/My Drive/summer20/classification/flowers/saved_models/' + TRAIN_SESS\n",
        "flower_model = tf.keras.models.load_model(saved_model_path)\n",
        "end = time.time()\n",
        "print(\"Build time: {} sec\".format(format(end-start, '.3f')))\n",
        "dataset_labels = ['Branch', 'Entire', 'Flower', 'Fruit', 'Leaf', 'Stem']\n",
        "\n",
        "# Load in image from URL\n",
        "# Modified from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb#scrollTo=JhVecdzJTsKE\n",
        "def image_from_url(url, fn):\n",
        "  file = tf.keras.utils.get_file(fn, url) # Filename doesn't matter\n",
        "  disp_img = tf.keras.preprocessing.image.load_img(file)\n",
        "  img = tf.keras.preprocessing.image.load_img(file, target_size=[pixels, pixels])\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  #x = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
        "    #x[tf.newaxis,...])\n",
        "  x = tf.keras.applications.inception_v3.preprocess_input(\n",
        "    x[tf.newaxis,...])\n",
        "  return x, disp_img\n",
        "\n",
        "# Run image through classifier\n",
        "def run_model(x):\n",
        "  infer = flower_model.signatures[\"serving_default\"]\n",
        "  label_id = infer(tf.constant(x))[flower_model.output_names[0]]\n",
        "  label = dataset_labels[np.argmax(label_id)]\n",
        "  confidence = format(np.amax(label_id)*100, '.3f')\n",
        "  return label, confidence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHoQsoCds1W_"
      },
      "source": [
        "### Single Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3pW2sL7aGSQ"
      },
      "source": [
        "# TO DO: Insert image URL using form field to the right\n",
        "url = \"https://content.eol.org/data/media/80/ce/d7/542.6789991664.jpg\" #@param {type:\"string\"}\n",
        "fn = \"angiosperm_image.jpg\"\n",
        "img, disp_img = image_from_url(url, fn)\n",
        "label, confidence = run_model(img)\n",
        "\n",
        "# Plot and show cropping boxes on images\n",
        "_, ax = plt.subplots(figsize=(10, 10))\n",
        "ax.imshow(disp_img)\n",
        "plt.axis('off')\n",
        "plt.title(\"Prediction: {}, Confidence: {}%\".format(label, confidence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MErprHihvyRl"
      },
      "source": [
        "### Multiple images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O85PMG04vxJT"
      },
      "source": [
        "# TO DO: Enter URLs here\n",
        "url1 = 'https://content.eol.org/data/media/80/ce/d7/542.6789991664.jpg' #@param {type:\"string\"}\n",
        "url2 = 'https://content.eol.org/data/media/80/d7/17/542.6872776289.jpg' #@param {type:\"string\"}\n",
        "url3 = 'https://content.eol.org/data/media/81/12/42/542.7670415710.260x190.jpg' #@param {type:\"string\"}\n",
        "urls = [url1, url2, url3]\n",
        "print(urls)\n",
        "\n",
        "# Classify images from URL and display results\n",
        "for im_num, url in enumerate(urls, start=1):\n",
        "  fn = str(im_num) + '.jpg'\n",
        "  img, disp_img = image_from_url(url, fn)\n",
        "  # Record inference time\n",
        "  start_time = time.time()\n",
        "  # Classify image\n",
        "  label, confidence = run_model(img)\n",
        "  end_time = time.time()\n",
        "  # Display progress message after each image\n",
        "  print('Classification complete in {} of {} images'.format(im_num, len(urls)))\n",
        "\n",
        "  # Plot and show cropping boxes on images\n",
        "  _, ax = plt.subplots(figsize=(10, 10))\n",
        "  ax.imshow(disp_img)\n",
        "  plt.axis('off')\n",
        "  plt.title(\"{}) Prediction: {}, Confidence: {}%, Inference Time: {}\".format(im_num, label, confidence, format(end_time-start_time, '.2f')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqUTFD4lbw3h"
      },
      "source": [
        "### Run images from EOL image bundles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVcDvoMbbz4p"
      },
      "source": [
        "# Read in EOL image bundle dataframe\n",
        "# TO DO: Type in image bundle address using form field to right\n",
        "bundle = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Angiosperms_20K_breakdown_000031.txt' #@param {type:\"string\"}\n",
        "df = pd.read_csv(bundle, sep='\\t', header=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lRMg1XcXL6O"
      },
      "source": [
        "# Optional: Run inference for taxon-specific images only\n",
        "# TO DO: Type in the taxon you'd like to inspect results for using form field to right\n",
        "taxon = \"\" #@param {type:\"string\"}\n",
        "df = df.loc[df.ancestry.str.contains(taxon, case=False, na=False)]\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXj-yvZ2b0a9"
      },
      "source": [
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# TO DO: Set start and end rows to run inference for from EOL image bundle using form field to right\n",
        "start =  815#@param {type:\"integer\"}\n",
        "end =  830#@param {type:\"integer\"}\n",
        "\n",
        "# Loop through EOL image bundle to classify images and generate tags\n",
        "for i, row in df.iloc[start:end].iterrows():\n",
        "  try:\n",
        "    # Get url from image bundle\n",
        "    url = df['eolMediaURL'][i]\n",
        "    # Read in image from url\n",
        "    fn = str(i) + '.jpg'\n",
        "    img, disp_img = image_from_url(url, fn)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Run inference/Classify image\n",
        "    label, confidence = run_model(img)\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Classification complete in {} of {} images'.format(i, len(df)))\n",
        "\n",
        "    # Show classification results for images\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(disp_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"{}) Prediction: {}, Confidence: {}%, Inference Time: {}\".format(i, label, confidence, format(end_time-start_time, '.3f')))\n",
        "\n",
        "    # Export tagging results to tsv\n",
        "    # Define variables for export\n",
        "    identifier = df['identifier'][i]\n",
        "    dataObjectVersionID = df['dataObjectVersionID'][i]\n",
        "    ancestry = df['ancestry'][i]\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vb43eI0kbrf"
      },
      "source": [
        "## Run images from URL through trained classifer & export tagging results to tsv\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv7r_DQbNWwZ"
      },
      "source": [
        "### Define functions and select model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OK1vR8Ukal5"
      },
      "source": [
        "import csv\n",
        "\n",
        "# Load trained model from path\n",
        "model_selection = (\"02_mobilenetssd\", 224) #@param [\"(\\\"20_fromscratch\\\", 150)\", \"(\\\"02_mobilenetssd\\\", 224)\", \"(\\\"06_inception\\\", 224)\"] {type:\"raw\"}\n",
        "TRAIN_SESS, pixels = model_selection\n",
        "saved_model_path = '/content/drive/My Drive/summer20/classification/saved_models/' + TRAIN_SESS\n",
        "flower_model = tf.keras.models.load_model(saved_model_path)\n",
        "dataset_labels = ['Branch', 'Entire', 'Flower', 'Fruit', 'Leaf', 'Stem']\n",
        "\n",
        "# Write header row of output crops file\n",
        "# TO DO: Change file name for each bundle/run abcd if doing 4 batches using form field to right\n",
        "tags_file = 'angiosperm_tags_20k_d' #@param {type:\"string\"}\n",
        "tags_fpath = '/content/drive/My Drive/summer20/classification/results/' + tags_file + '.tsv'\n",
        "with open(tags_fpath, 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"eolMediaURL\", \"identifier\", \"dataObjectVersionID\", \"ancestry\", \"tag\"])\n",
        "\n",
        "# Load in image from URL\n",
        "# Modified from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb#scrollTo=JhVecdzJTsKE\n",
        "def image_from_url(url, fn):\n",
        "  file = tf.keras.utils.get_file(fn, url) # Filename doesn't matter\n",
        "  disp_img = tf.keras.preprocessing.image.load_img(file)\n",
        "  img = tf.keras.preprocessing.image.load_img(file, target_size=[pixels, pixels])\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = tf.keras.applications.inception_v3.preprocess_input(\n",
        "    x[tf.newaxis,...]) # tried using mobilenet_v2 for mobilenet model and made no difference, keep same for all\n",
        "  return x, disp_img\n",
        "\n",
        "# Run image through classifier\n",
        "def run_model(x):\n",
        "  infer = flower_model.signatures[\"serving_default\"]\n",
        "  label_id = infer(tf.constant(x))[flower_model.output_names[0]]\n",
        "  label = dataset_labels[np.argmax(label_id)]\n",
        "  confidence = format(np.amax(label_id)*100, '.2f')\n",
        "  return label, confidence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6oIEkTHlLvy"
      },
      "source": [
        "### Run images from EOL image bundles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsZjmil8lI-9"
      },
      "source": [
        "# Read in EOL image bundle dataframe\n",
        "# TO DO: Type in image bundle address using form field to right\n",
        "bundle = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Angiosperms_20K_breakdown_000031.txt' #@param {type:\"string\"}\n",
        "df = pd.read_csv(bundle, sep='\\t', header=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2l-NHp_l8tt"
      },
      "source": [
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# TO DO: Set start and end rows to run inference for from EOL image bundle using form field to right\n",
        "start =  19136#@param {type:\"integer\"}\n",
        "end = 20000 #@param {type:\"integer\"}\n",
        "\n",
        "# Loop through EOL image bundle to classify images and generate tags\n",
        "for i, row in df.iloc[start:end].iterrows():\n",
        "  try:\n",
        "    # Get url from image bundle\n",
        "    url = df['eolMediaURL'][i]\n",
        "    # Read in image from url\n",
        "    fn = str(i) + '.jpg'\n",
        "    img, disp_img = image_from_url(url, fn)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Run inference/Classify image\n",
        "    label, confidence = run_model(img)\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Classification complete in {} of {} images'.format(i, len(df)))\n",
        "\n",
        "    # Show classification results for images\n",
        "    #_, ax = plt.subplots(figsize=(10, 10))\n",
        "    #ax.imshow(disp_img)\n",
        "    #plt.axis('off')\n",
        "    #plt.title(\"{}) Prediction: {}, Confidence: {}%, Inference Time: {}\".format(i, label, confidence, format(end_time-start_time, '.3f')))\n",
        "\n",
        "    # Export tagging results to tsv\n",
        "    # Define variables for export\n",
        "    if label==\"Flower\":\n",
        "        identifier = df['identifier'][i]\n",
        "        dataObjectVersionID = df['dataObjectVersionID'][i]\n",
        "        ancestry = df['ancestry'][i]\n",
        "        with open(tags_fpath, 'a') as out_file:\n",
        "          tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "          tsv_writer.writerow([url, identifier, dataObjectVersionID, ancestry, label])\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvpQWx9nDY9-"
      },
      "source": [
        "## Display exported tagging results on EOL images\n",
        "---   \n",
        "Verify that results on EOL image bundles are as expected by randomly sampling images from tagging files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmA1LtgcDssj"
      },
      "source": [
        "# Read in EOL image tagging dataset\n",
        "# TO DO: Type in image tag filepath in form field to right\n",
        "tags_file = 'angiosperm_tags_20k_a' #@param {type:\"string\"}\n",
        "tags_fpath = '/content/drive/My Drive/summer20/classification/results/' + tags_file + '.tsv'\n",
        "df = pd.read_csv(tags_fpath, sep='\\t', header=0)\n",
        "print(df.head())\n",
        "\n",
        "# Function to load in image from URL\n",
        "# Modified from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb#scrollTo=JhVecdzJTsKE\n",
        "def image_from_url(url, fn):\n",
        "  file = tf.keras.utils.get_file(fn, url) # Filename doesn't matter\n",
        "  disp_img = tf.keras.preprocessing.image.load_img(file)\n",
        "  img = tf.keras.preprocessing.image.load_img(file, target_size=[224, 224])\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = tf.keras.applications.inception_v3.preprocess_input(\n",
        "    x[tf.newaxis,...]) # tried using mobilenet_v2 for mobilenet model and made no difference, keep same for all\n",
        "  return x, disp_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h-Q91nkG4r_"
      },
      "source": [
        "# Optional: Display tags for taxon-specific images only\n",
        "# TO DO: Type in the taxon you'd like to inspect results for using form field to right\n",
        "taxon = \"\" #@param {type:\"string\"}\n",
        "df = df.loc[df.ancestry.str.contains(taxon, case=False, na=False)]\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqDtNAciES2e"
      },
      "source": [
        "import socket\n",
        "import random\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# TO DO: Set n below to randomly select n images from EOL image bundle tags using form field to right\n",
        "start =  0\n",
        "end = len(df)\n",
        "n = 50 #@param {type:\"slider\", min:0, max:50, step:10}\n",
        "rand = random.sample(range(start, end), n)\n",
        "\n",
        "# Loop through EOL tag dataset to display and evaluate results on images\n",
        "for i, row in df.iloc[rand].iterrows():\n",
        "  try:\n",
        "    # Get url from tag dataset\n",
        "    url = df['eolMediaURL'][i]\n",
        "    # Read in image from url\n",
        "    fn = str(i) + '.jpg'\n",
        "    img, disp_img = image_from_url(url, fn)\n",
        "    # Get tag from dataset\n",
        "    label = df['tag'][i]\n",
        "    # Display progress message after each image\n",
        "    print('{} of {} images loaded'.format(i, len(df)))\n",
        "\n",
        "    # Show classification results for images\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(disp_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"{}) Prediction: {}\".format(i, label))\n",
        "  \n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(url))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}