{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "flower_fruit_train.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "ikRQ9LLbVHXQ",
        "YWylHChz4JTh",
        "LdeXMkjv4EJl",
        "8e8AxN-b113c"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/flower_fruit/flower_fruit_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB"
      },
      "source": [
        "# Training Tensorflow MobileNetSSD v2 and Inception v3 models to classify flowers and fruits using EOL Angiosperm Images as training data\n",
        "---\n",
        "*Last Updated 25 September 2020*   \n",
        "Train [MobileNet SSD v2](https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4) and [Inception v3](https://tfhub.dev/google/imagenet/inception_v3/classification/4) to classify flowers and fruits from EOL images. Images are classified into flower, fruit or null. The training dataset is an image bundle containing up to 30 images per Angiosperm plant family found in EOL that was manually sorted into flower, fruit, or null classes. Classifications will be used to generate image tags to improve searchability of EOLv3 images.\n",
        "\n",
        "EOL max30imgPerFam bundle images were downloaded to Google Drive using [flower_fruit_preprocessing.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/flower_fruit/flower_fruit_preprocessing.ipynb). Then, images were manually sorted into folders based on image class (flower, fruit, null, other/excluded images). Excluded images were moved to an external folder for use training future classifiers before running this notebook.\n",
        "\n",
        "Pre-trained MobileNet SSD v2 and Inception v3 models were fine-tuned to classify flower images. \n",
        "\n",
        "**Best results for each model from 42 total trials:**   \n",
        "* **MobileNet SSD v2 was trained for 3 hours to 60 epochs with Batch Size=16, Lr=0.001, Dropout=0.3. Final validation accuracy = 0.81**\n",
        "* Inception v3 was trained for 3.5 hours to 40 epochs with Batch Size=16, Lr=0.001, Dropout=0.2. Final validation accuracy = 0.72. \n",
        "\n",
        "**Notes**\n",
        "* Change filepaths or information using the form fields to the right of code blocks (also noted in code with 'TO DO')\n",
        "* Make sure to set the runtime to GPU Hardware Accelerator with a High Ram Runtime Shape (Runtime -> Change runtime type)\n",
        "\n",
        "**References**   \n",
        "* https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\n",
        "* https://www.tensorflow.org/tutorials/images/classification\n",
        "* https://medium.com/analytics-vidhya/create-tensorflow-image-classification-model-with-your-own-dataset-in-google-colab-63e9d7853a3e\n",
        "* https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb#scrollTo=umB5tswsfTEQ\n",
        "* https://medium.com/analytics-vidhya/how-to-do-image-classification-on-custom-dataset-using-tensorflow-52309666498e\n",
        "* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "* https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz"
      },
      "source": [
        "## Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP"
      },
      "source": [
        "# For working with data and plotting graphs\n",
        "import itertools\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For image classification and training\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, InputLayer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikRQ9LLbVHXQ"
      },
      "source": [
        "## Train Classification Model(s)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvOpxBvKWCht"
      },
      "source": [
        "### Training Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWylHChz4JTh"
      },
      "source": [
        "#### If using pre-trained classifier\n",
        "Use dropdown menu on the right to choose which pre-trained model to use and set the image batch size for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77MDrSHntvWK"
      },
      "source": [
        "# TO DO: Select pre-trained model to use from Tensorflow Hub Model Zoo\n",
        "module_selection = (\"mobilenet_v2_1.0_224\", 224) #@param [\"(\\\"mobilenet_v2_1.0_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base, pixels = module_selection\n",
        "\n",
        "if handle_base == \"inception_v3\":\n",
        "  MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/inception_v3/classification/4\".format(handle_base)\n",
        "elif handle_base == \"mobilenet_v2_1.0_224\":\n",
        "  MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\".format(handle_base) \n",
        "\n",
        "# TO DO: adjust batch size to make training faster or slower\n",
        "BATCH_SIZE = \"16\" #@param [\"16\", \"32\", \"64\", \"128\"]\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {} and batch size {}\".format(handle_base, IMAGE_SIZE, BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocQ6gDasveZ5"
      },
      "source": [
        "# TO DO: Change directory to wherever images are stored\n",
        "PATH = '/content/drive/My Drive/summer20/classification/flower_fruit/images' #@param {type:\"string\"}\n",
        "TRAINING_DATA_DIR = str(PATH)\n",
        "print(TRAINING_DATA_DIR)\n",
        "\n",
        "# TO DO: Adjust interpolation method and see how training results change\n",
        "interp = \"nearest\" #@param [\"nearest\", \"bilinear\"]\n",
        "\n",
        "# Set data generation and flow parameters\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=int(BATCH_SIZE),\n",
        "                    interpolation = interp)\n",
        "\n",
        "# Make test dataset\n",
        "test_datagen = ImageDataGenerator(**datagen_kwargs)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"validation\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "# Make train dataset using augmentation parameters below\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40, # randomly rotates image 0-40 degrees\n",
        "    horizontal_flip=True, # random horizontal flip\n",
        "    width_shift_range=0.2, height_shift_range=0.2, # randomly distorts height and width\n",
        "    shear_range=0.2, zoom_range=0.2, # randomly clips and zooms in on images\n",
        "    **datagen_kwargs)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR, subset=\"training\", shuffle=True, **dataflow_kwargs)\n",
        "\n",
        "# Learn more about data batches\n",
        "image_batch_train, label_batch_train = next(iter(train_generator))\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(dataset_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdeXMkjv4EJl"
      },
      "source": [
        "#### If creating model from scratch\n",
        "* Select batch size from dropdown menu on the right  \n",
        "* Type in filepath to image directory using form field on the right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Def61kAU4BXz"
      },
      "source": [
        "# TO DO: adjust batch size to make training faster or slower\n",
        "BATCH_SIZE = 16 #@param [\"16\", \"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "\n",
        "# Set input image size for model\n",
        "IMAGE_SIZE = (150, 150)\n",
        "\n",
        "# TO DO: Change directory to wherever images are stored\n",
        "PATH = '/content/drive/My Drive/summer20/classification/flower_fruit/images' #@param {type:\"string\"}\n",
        "TRAINING_DATA_DIR = str(PATH)\n",
        "print(TRAINING_DATA_DIR)\n",
        "\n",
        "# TO DO: Adjust interpolation method and see how training results change\n",
        "interp = \"nearest\" #@param [\"nearest\", \"bilinear\"]\n",
        "\n",
        "# Set data generation and flow parameters\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=int(BATCH_SIZE),\n",
        "                    interpolation = interp)\n",
        "\n",
        "# Make test dataset\n",
        "test_datagen = ImageDataGenerator(**datagen_kwargs)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"validation\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "# Make train dataset using augmentation parameters below\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40, # randomly rotates image 0-40 degrees\n",
        "    horizontal_flip=True, # random horizontal flip\n",
        "    width_shift_range=0.2, height_shift_range=0.2, # randomly distorts height and width\n",
        "    shear_range=0.2, zoom_range=0.2, # randomly clips and zooms in on images\n",
        "    **datagen_kwargs)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR, subset=\"training\", shuffle=True, **dataflow_kwargs)\n",
        "\n",
        "# Learn more about data batches\n",
        "image_batch_train, label_batch_train = next(iter(train_generator))\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(dataset_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72N6UtPiVQNW"
      },
      "source": [
        "### Model Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8AxN-b113c"
      },
      "source": [
        "#### If fine-tuning pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1pGKmlkVS1k",
        "cellView": "both"
      },
      "source": [
        "# Build model\n",
        "print(\"Building model with\", handle_base)\n",
        "# TO DO: If model is overfitting, add/increase dropout rate\n",
        "dropout_rate = 0.4 #@param {type:\"slider\", min:0, max:0.5, step:0.1}\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=True),\n",
        "    Dropout(rate = dropout_rate), \n",
        "    Dense(train_generator.num_classes,\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "  ])\n",
        "  \n",
        "  # Build model\n",
        "  model.build((None,)+IMAGE_SIZE+(3,))\n",
        "  \n",
        "  # Compile model\n",
        "  model.compile(\n",
        "    # Parameters for Adam optimizer\n",
        "    optimizer=tf.keras.optimizers.Adam(\n",
        "      learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "      name='Adam'), \n",
        "      # Categorical cross entropy because 3 exclusive classes\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "    metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Create new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Steps per epoch and testing\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hh-3CQr1yGx"
      },
      "source": [
        "#### Create new model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMpY5U9k_Tk-"
      },
      "source": [
        "# Build model\n",
        "print(\"Building model from scratch\")\n",
        "# Modified from TF Image Classification Tutorial\n",
        "\n",
        "# To DO: Adjust model perforamance using below hyperparameters\n",
        "# See blog post for explanations https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
        "\n",
        "# If model is overfitting, add/increase dropout rate\n",
        "dropout_rate = 0.2 #@param {type:\"slider\", min:0, max:0.7, step:0.1}\n",
        "# Layer 1: Start with smaller number of filters and increase number if performance too low\n",
        "no_filters_lay1 = 64 #@param [\"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "# Layer 2: Use either the same number of layers as Layer 1, or 2x as many\n",
        "no_filters_lay2 = no_filters_lay1 * 2 #@param [\"no_filters_lay1\", \"no_filters_lay1 * 2\"] {type:\"raw\"}\n",
        "# Layer 3: Use 2x as many layers as Layer 2\n",
        "no_filters_lay3 = no_filters_lay2 * 2\n",
        "# Layer 1: If input image size >128, may need to use initial filter size of 5, 5\n",
        "filter_size_lay1 = (5, 5) #@param [\"(3, 3)\", \"(5, 5)\", \"(7, 7)\"] {type:\"raw\"}\n",
        "# Final Dense Layer: Set number of output nodes for network (same as number of classes)\n",
        "num_classes = 3 #@param {type:\"integer\"}\n",
        "# Compile model: Choose loss function. Categorical supposed to be better for multiple classes, but binary got better results one run\n",
        "loss_fun = \"categorical_crossentropy\" #@param [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential([\n",
        "    Conv2D(no_filters_lay1, filter_size_lay1, padding='same', activation='relu',\n",
        "        input_shape=(IMAGE_SIZE + (3,))),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(no_filters_lay2, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(no_filters_lay3, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(), # this converts our 3D feature maps to 1D feature vectors\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(num_classes, activation='softmax') # softmax good for multiple class models with exclusive classes\n",
        "])\n",
        "\n",
        "  # Compile model\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1, amsgrad=False,\n",
        "    name='Adam')\n",
        "  model.compile(loss=loss_fun,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Create model instance\n",
        "model = create_model()\n",
        "\n",
        "# Set steps per epoch and testing\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ycnwnKkkfa"
      },
      "source": [
        "### Actual Training \n",
        "* For the first time training each model with specified hyper-parameters, go to **First time training**. If hyper-parameters are changed and a model is retrained, also go to **First time training**.   \n",
        "* If hyper-parameters are kept the same, and the model only needs to be trained for additional epochs, go to **Resume training from a saved checkpoint** below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbToKXYlEo5"
      },
      "source": [
        "# TO DO: Adjust number of epochs to find balance between underfit and overfit for training\n",
        "num_epochs = '80' #@param {type:\"string\"}\n",
        "\n",
        "# Save each new training attempt results in new folder\n",
        "last_attempt = !ls /content/drive/'My Drive'/summer20/classification/flower_fruit/saved_models/ | tail -n 1\n",
        "if not last_attempt:\n",
        "  last_attempt = 0\n",
        "else:\n",
        "  last_attempt = int(last_attempt.n)\n",
        "TRAIN_SESS_NUM = str(last_attempt + 1)\n",
        "CKPT_PATH = '/content/drive/My Drive/summer20/classification/flower_fruit/saved_models/' + TRAIN_SESS_NUM + '/ckpt/cp-{epoch:04d}.ckpt' \n",
        "\n",
        "print(\"Last training attempt number:\", last_attempt)\n",
        "print(\"Training attempt number: {}, for {} epochs\".format(TRAIN_SESS_NUM, num_epochs))\n",
        "\n",
        "# Create a callback that saves the model's weights during training\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CKPT_PATH,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Save weights for 0th epoch\n",
        "model.save_weights(CKPT_PATH.format(epoch=0))\n",
        "\n",
        "# Train the model with the new callback\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=int(num_epochs), steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[ckpt_callback],\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_steps).history\n",
        "\n",
        "# Save trained model \n",
        "saved_model_path = '/content/drive/My Drive/summer20/classification/flower_fruit/saved_models/' + TRAIN_SESS_NUM\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ionhn0UvmE-x"
      },
      "source": [
        "#### Plot loss and accuracy for training session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kfOjJ3llGVY"
      },
      "source": [
        "# Plot loss\n",
        "plt.figure()\n",
        "plt.title(\"Attempt {}:Training and Validation Loss\".format(TRAIN_SESS_NUM))\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"], label='Train')\n",
        "plt.plot(hist[\"val_loss\"], label='Test')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure()\n",
        "plt.title(\"Attempt {}:Training and Validation Accuracy\".format(TRAIN_SESS_NUM))\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"], label='Train')\n",
        "plt.plot(hist[\"val_accuracy\"], label='Test')\n",
        "plt.legend(loc='upper right')\n",
        "path = '/content/drive/My Drive/summer20/classification/flower_fruit/train_graphs/' + TRAIN_SESS_NUM + '.png'\n",
        "plt.savefig(path)\n",
        "\n",
        "# Print final loss and accuracy values\n",
        "final_loss, final_accuracy = model.evaluate(test_generator, steps = test_steps)\n",
        "print('Final loss: {:.2f}'.format(final_loss))\n",
        "print('Final accuracy: {:.2f}%'.format(final_accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpjn2qz-57p_"
      },
      "source": [
        "## Review training results\n",
        "---   \n",
        "Display classification results on images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNxq35BeEi-k"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "# TO DO: Do you want to display classification results for the most recently trained model?\n",
        "answer = \"No\" #@param [\"Yes\", \"No\"]\n",
        "# TO DO: If No, manually input desired training attempt number to the right\n",
        "if answer == \"Yes\":\n",
        "  # Display results from most recent training attempt\n",
        "  last_attempt = !ls /content/drive/'My Drive'/summer20/classification/flower_fruit/saved_models/ | tail -n 1\n",
        "  TRAIN_SESS_NUM = str(last_attempt.n)\n",
        "else:\n",
        "  TRAIN_SESS_NUM = \"11\" #@param [\"09\", \"07\", \"08\", \"03\", \"05\", \"22_retrain\", \"23_retrain\", \"11\"]\n",
        "\n",
        "# Load trained model from path\n",
        "saved_model_path = '/content/drive/My Drive/summer20/classification/flower_fruit/saved_models/' + TRAIN_SESS_NUM\n",
        "flower_model = tf.keras.models.load_model(saved_model_path)\n",
        "\n",
        "# Function for plotting classification results with color-coded label if true or false prediction\n",
        "label_names = ['Flower', 'Fruit', 'Null']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awIMvGFtGlvE"
      },
      "source": [
        "# Run inference\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# TO DO: Choose which image class to inspect results for in true_imclass to right\n",
        "# TO DO: Choose start and end image numbers to inspect (inspect up to 50 images at a time)\n",
        "base = '/content/drive/My Drive/summer20/classification/'\n",
        "classifier = \"flower_fruit/\" #@param [\"flower_fruit/\"]\n",
        "true_imclass = \"03_null\" #@param [\"02_fruit\", \"01_flower\", \"03_null\"]\n",
        "PATH_TO_TEST_IMAGES_DIR = base + classifier + \"images/\" + true_imclass\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "start = 0 #@param {type:\"number\"}\n",
        "end =  50 #@param {type:\"number\"}\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS[start:end], start=1):\n",
        "    # Load in image\n",
        "    img = Image.open(im_path)\n",
        "    image = img.resize((224,224))\n",
        "    image = np.reshape(image,[1,224,224,3])\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection and draw boxes on image\n",
        "    predictions = flower_model.predict(image, batch_size=1)\n",
        "    label_num = np.argmax(predictions)\n",
        "    conf = predictions[0][label_num]\n",
        "    otherconfa = predictions[0][:label_num]\n",
        "    otherconfb = predictions[0][label_num+1:]\n",
        "    imclass = label_names[label_num]\n",
        "    other_class = label_names[:label_num]+label_names[label_num+1:]\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for {} of {} images'.format(im_num, (end-start)))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(img)\n",
        "    plt.title('{}) Prediction: {}, Confidence: {}, Inference time: {}, \\\n",
        "    \\n Other Predictions: {}, Other Conf: {}, {}'.format(im_num, imclass, \\\n",
        "    format(conf, '.2f'), format(end_time-start_time, '.2f'), other_class, \\\n",
        "    otherconfa, otherconfb))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}