{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "image_type_train.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "LdeXMkjv4EJl",
        "5hh-3CQr1yGx"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/image_type_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB"
      },
      "source": [
        "# Training Tensorflow MobileNetSSD v2 and Inception v3 models to classify maps, phylogenies, illustrations, and herbarium sheets from EOL images\n",
        "---\n",
        "*Last Updated 29 October 2020*   \n",
        "Transfer learning to train [MobileNet SSD v2](https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4) and [Inception v3](https://tfhub.dev/google/imagenet/inception_v3/classification/4) to classify maps, phylogenies, illustrations, and herbarium sheets from EOL images. The training dataset consists of image bundles from a variety of sources. Maps and phylogenies are from Wikimedia Commons, herbarium sheets are from NMNH Botany on EOL, botanical illustrations are from Flickr BHL, and zoological images are from EOL. Classifications will be used to generate image tags to improve searchability of EOLv3 images.\n",
        "\n",
        "Training images were downloaded to Google Drive and processed using [image_type_preprocessing.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/image_type_preprocessing.ipynb). Manual inspection of images was used to determine and apply exclusion criteria for each training class.\n",
        "\n",
        "**Best results for each model from 15 total trials:**   \n",
        "* **MobileNet SSD v2 was trained for 3 hours to 30 epochs with Batch Size=16, Lr=0.00001, Dropout=0.3, epsilon=1e-7, Adam optimizer. Final validation accuracy = 0.90**\n",
        "* Inception v3 was trained for 3.5 hours to 30 epochs with Batch Size=16, Lr=0.0001, Dropout=0.2, epsilon=1, Adam optimizer. Final validation accuracy = 0.89. \n",
        "\n",
        "**Notes**\n",
        "* Change filepaths or information using the form fields to the right of code blocks (also noted in code with 'TO DO')\n",
        "* Make sure to set the runtime to GPU Hardware Accelerator with a High RAM Runtime Shape (Runtime -> Change runtime type)\n",
        "\n",
        "**References**   \n",
        "* https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\n",
        "* https://www.tensorflow.org/tutorials/images/classification\n",
        "* https://medium.com/analytics-vidhya/create-tensorflow-image-classification-model-with-your-own-dataset-in-google-colab-63e9d7853a3e\n",
        "* https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb#scrollTo=umB5tswsfTEQ\n",
        "* https://medium.com/analytics-vidhya/how-to-do-image-classification-on-custom-dataset-using-tensorflow-52309666498e\n",
        "* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "* https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz"
      },
      "source": [
        "## Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP"
      },
      "source": [
        "# For working with data and plotting graphs\n",
        "import itertools\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For image classification and training\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, InputLayer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikRQ9LLbVHXQ"
      },
      "source": [
        "## Train Classification Model(s)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvOpxBvKWCht"
      },
      "source": [
        "### Training Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWylHChz4JTh"
      },
      "source": [
        "#### If using pre-trained classifier\n",
        "Use dropdown menu on the right to choose which pre-trained model to use and set the image batch size for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77MDrSHntvWK"
      },
      "source": [
        "# TO DO: Select pre-trained model to use from Tensorflow Hub Model Zoo\n",
        "module_selection = (\"mobilenet_v2_1.0_224\", 224) #@param [\"(\\\"mobilenet_v2_1.0_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base, pixels = module_selection\n",
        "\n",
        "if handle_base == \"inception_v3\":\n",
        "  MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/inception_v3/classification/4\".format(handle_base)\n",
        "  epsilon = 1\n",
        "elif handle_base == \"mobilenet_v2_1.0_224\":\n",
        "  MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\".format(handle_base) \n",
        "  epsilon = 1e-07\n",
        "# TO DO: adjust batch size to make training faster or slower\n",
        "BATCH_SIZE = \"16\" #@param [\"16\", \"32\", \"64\", \"128\"]\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {} and batch size {}\".format(handle_base, IMAGE_SIZE, BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocQ6gDasveZ5"
      },
      "source": [
        "# TO DO: Change directory to wherever images are stored\n",
        "PATH = '/content/drive/My Drive/summer20/classification/image_type/images' #@param {type:\"string\"}\n",
        "TRAINING_DATA_DIR = str(PATH)\n",
        "print(TRAINING_DATA_DIR)\n",
        "\n",
        "# TO DO: Adjust interpolation method and see how training results change\n",
        "interp = \"nearest\" #@param [\"nearest\", \"bilinear\"]\n",
        "\n",
        "# Set data generation and flow parameters\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=int(BATCH_SIZE),\n",
        "                    interpolation = interp, color_mode='rgb') # unsuccessful training with rgba...\n",
        "\n",
        "# Make test dataset\n",
        "test_datagen = ImageDataGenerator(**datagen_kwargs)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"validation\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "# Make train dataset using augmentation parameters below\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40, # randomly rotates image 0-40 degrees\n",
        "    horizontal_flip=True, # random horizontal flip\n",
        "    width_shift_range=0.2, height_shift_range=0.2, # randomly distorts height and width\n",
        "    shear_range=0.2, zoom_range=0.2, # randomly clips and zooms in on images\n",
        "    **datagen_kwargs)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR, subset=\"training\", shuffle=True, **dataflow_kwargs)\n",
        "\n",
        "# Learn more about data batches\n",
        "image_batch_train, label_batch_train = next(iter(train_generator))\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(dataset_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdeXMkjv4EJl"
      },
      "source": [
        "#### If creating model from scratch\n",
        "* Select batch size from dropdown menu on the right  \n",
        "* Type in filepath to image directory using form field on the right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Def61kAU4BXz"
      },
      "source": [
        "# TO DO: adjust batch size to make training faster or slower\n",
        "BATCH_SIZE = 16 #@param [\"16\", \"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "\n",
        "# Set input image size for model\n",
        "IMAGE_SIZE = (150, 150)\n",
        "\n",
        "# TO DO: Change directory to wherever images are stored\n",
        "PATH = '/content/drive/My Drive/summer20/classification/flower_fruit/images' #@param {type:\"string\"}\n",
        "TRAINING_DATA_DIR = str(PATH)\n",
        "print(TRAINING_DATA_DIR)\n",
        "\n",
        "# TO DO: Adjust interpolation method and see how training results change\n",
        "interp = \"nearest\" #@param [\"nearest\", \"bilinear\"]\n",
        "\n",
        "# Set data generation and flow parameters\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=int(BATCH_SIZE),\n",
        "                    interpolation = interp)\n",
        "\n",
        "# Make test dataset\n",
        "test_datagen = ImageDataGenerator(**datagen_kwargs)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"validation\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SIZE\n",
        ")\n",
        "\n",
        "# Make train dataset using augmentation parameters below\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=40, # randomly rotates image 0-40 degrees\n",
        "    horizontal_flip=True, # random horizontal flip\n",
        "    width_shift_range=0.2, height_shift_range=0.2, # randomly distorts height and width\n",
        "    shear_range=0.2, zoom_range=0.2, # randomly clips and zooms in on images\n",
        "    **datagen_kwargs)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR, subset=\"training\", shuffle=True, **dataflow_kwargs)\n",
        "\n",
        "# Learn more about data batches\n",
        "image_batch_train, label_batch_train = next(iter(train_generator))\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(dataset_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72N6UtPiVQNW"
      },
      "source": [
        "### Model Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8AxN-b113c"
      },
      "source": [
        "#### If fine-tuning pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1pGKmlkVS1k",
        "cellView": "both"
      },
      "source": [
        "# Build model\n",
        "print(\"Building model with\", handle_base)\n",
        "# TO DO: If model is overfitting, add/increase dropout rate\n",
        "dropout_rate = 0.3 #@param {type:\"slider\", min:0, max:0.5, step:0.1}\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.Sequential([\n",
        "    InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=False), #False freezes lower layers so only top classifier is retrained\n",
        "    Dropout(rate = dropout_rate), \n",
        "    Dense(train_generator.num_classes,\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "  ])\n",
        "  \n",
        "  # Build model\n",
        "  model.build((None,)+IMAGE_SIZE+(3,))\n",
        "  \n",
        "  # Compile model\n",
        "  model.compile(\n",
        "    # Parameters for Adam optimizer\n",
        "    optimizer=tf.keras.optimizers.Adam(\n",
        "      learning_rate=0.00001, beta_1=0.9, beta_2=0.999, epsilon=epsilon, amsgrad=False,\n",
        "      name='Adam'), \n",
        "      # Categorical cross entropy because 3 exclusive classes\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "    metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Create new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Steps per epoch and testing\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hh-3CQr1yGx"
      },
      "source": [
        "#### Create new model from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMpY5U9k_Tk-"
      },
      "source": [
        "# Build model\n",
        "print(\"Building model from scratch\")\n",
        "# Modified from TF Image Classification Tutorial\n",
        "\n",
        "# To DO: Adjust model perforamance using below hyperparameters\n",
        "# See blog post for explanations https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
        "\n",
        "# If model is overfitting, add/increase dropout rate\n",
        "dropout_rate = 0.2 #@param {type:\"slider\", min:0, max:0.7, step:0.1}\n",
        "# Layer 1: Start with smaller number of filters and increase number if performance too low\n",
        "no_filters_lay1 = 64 #@param [\"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "# Layer 2: Use either the same number of layers as Layer 1, or 2x as many\n",
        "no_filters_lay2 = no_filters_lay1 * 2 #@param [\"no_filters_lay1\", \"no_filters_lay1 * 2\"] {type:\"raw\"}\n",
        "# Layer 3: Use 2x as many layers as Layer 2\n",
        "no_filters_lay3 = no_filters_lay2 * 2\n",
        "# Layer 1: If input image size >128, may need to use initial filter size of 5, 5\n",
        "filter_size_lay1 = (5, 5) #@param [\"(3, 3)\", \"(5, 5)\", \"(7, 7)\"] {type:\"raw\"}\n",
        "# Final Dense Layer: Set number of output nodes for network (same as number of classes)\n",
        "num_classes = 3 #@param {type:\"integer\"}\n",
        "# Compile model: Choose loss function. Categorical supposed to be better for multiple classes, but binary got better results one run\n",
        "loss_fun = \"categorical_crossentropy\" #@param [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential([\n",
        "    Conv2D(no_filters_lay1, filter_size_lay1, padding='same', activation='relu',\n",
        "        input_shape=(IMAGE_SIZE + (3,))),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(no_filters_lay2, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(no_filters_lay3, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(), # this converts our 3D feature maps to 1D feature vectors\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(num_classes, activation='softmax') # softmax good for multiple class models with exclusive classes\n",
        "])\n",
        "\n",
        "  # Compile model\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=1, amsgrad=False,\n",
        "    name='Adam')\n",
        "  model.compile(loss=loss_fun,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Create model instance\n",
        "model = create_model()\n",
        "\n",
        "# Set steps per epoch and testing\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ycnwnKkkfa"
      },
      "source": [
        "### Actual Training \n",
        "* For the first time training each model with specified hyper-parameters, go to **First time training**. If hyper-parameters are changed and a model is retrained, also go to **First time training**.   \n",
        "* If hyper-parameters are kept the same, and the model only needs to be trained for additional epochs, go to **Resume training from a saved checkpoint** below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbToKXYlEo5"
      },
      "source": [
        "# TO DO: Adjust number of epochs to find balance between underfit and overfit for training\n",
        "num_epochs = '30' #@param {type:\"string\"}\n",
        "\n",
        "# Save each new training attempt results in new folder\n",
        "last_attempt = !ls /content/drive/'My Drive'/summer20/classification/image_type/saved_models/ | tail -n 1\n",
        "if not last_attempt:\n",
        "  last_attempt = 0\n",
        "else:\n",
        "  last_attempt = int(last_attempt.n)\n",
        "TRAIN_SESS_NUM = str(last_attempt + 1)\n",
        "CKPT_PATH = '/content/drive/My Drive/summer20/classification/image_type/saved_models/' + TRAIN_SESS_NUM + '/ckpt/cp-{epoch:04d}.ckpt' \n",
        "\n",
        "print(\"Last training attempt number:\", last_attempt)\n",
        "print(\"Training attempt number: {}, for {} epochs\".format(TRAIN_SESS_NUM, num_epochs))\n",
        "\n",
        "# Create a callback that saves the model's weights during training\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CKPT_PATH,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Save weights for 0th epoch\n",
        "model.save_weights(CKPT_PATH.format(epoch=0))\n",
        "\n",
        "# Train the model with the new callback\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=int(num_epochs), steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[ckpt_callback],\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_steps).history\n",
        "\n",
        "# Save trained model \n",
        "saved_model_path = '/content/drive/My Drive/summer20/classification/image_type/saved_models/' + TRAIN_SESS_NUM\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ionhn0UvmE-x"
      },
      "source": [
        "#### Plot loss and accuracy for training session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kfOjJ3llGVY"
      },
      "source": [
        "# Plot loss\n",
        "plt.figure()\n",
        "plt.title(\"Attempt {}:Training and Validation Loss\".format(TRAIN_SESS_NUM))\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"], label='Train')\n",
        "plt.plot(hist[\"val_loss\"], label='Test')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "# Plot accuracy\n",
        "plt.figure()\n",
        "plt.title(\"Attempt {}:Training and Validation Accuracy\".format(TRAIN_SESS_NUM))\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"], label='Train')\n",
        "plt.plot(hist[\"val_accuracy\"], label='Test')\n",
        "plt.legend(loc='upper right')\n",
        "path = '/content/drive/My Drive/summer20/classification/image_type/train_graphs/' + TRAIN_SESS_NUM + '.png'\n",
        "plt.savefig(path)\n",
        "\n",
        "# Print final loss and accuracy values\n",
        "final_loss, final_accuracy = model.evaluate(test_generator, steps = test_steps)\n",
        "print('Final loss: {:.2f}'.format(final_loss))\n",
        "print('Final accuracy: {:.2f}%'.format(final_accuracy * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpjn2qz-57p_"
      },
      "source": [
        "## Review training results\n",
        "---   \n",
        "Display classification results on images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNxq35BeEi-k"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "# TO DO: Do you want to display classification results for the most recently trained model?\n",
        "answer = \"No\" #@param [\"Yes\", \"No\"]\n",
        "# TO DO: If No, manually input desired training attempt number to the right\n",
        "if answer == \"Yes\":\n",
        "  # Display results from most recent training attempt\n",
        "  last_attempt = !ls /content/drive/'My Drive'/summer20/classification/image_type/saved_models/ | tail -n 1\n",
        "  TRAIN_SESS_NUM = str(last_attempt.n)\n",
        "else:\n",
        "  TRAIN_SESS_NUM = \"15\" #@param [\"13\", \"11\", \"15\"] {allow-input: true}\n",
        "\n",
        "# Load trained model from path\n",
        "saved_model_path = '/content/drive/My Drive/summer20/classification/image_type/saved_models/' + TRAIN_SESS_NUM\n",
        "imtype_model = tf.keras.models.load_model(saved_model_path)\n",
        "\n",
        "# TO DO: Select model type\n",
        "module_selection = (\"inception_v3\", 299) #@param [\"(\\\"mobilenet_v2_1.0_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base, pixels = module_selection\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "\n",
        "# Function for plotting classification results with color-coded label if true or false prediction\n",
        "label_names = ['herbarium sheet', 'illustration', 'illustration_grey', 'maps', 'null', 'phylo']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awIMvGFtGlvE"
      },
      "source": [
        "# Run inference\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# TO DO: Choose which image class to inspect results for in true_imclass to right\n",
        "# TO DO: Choose start and end image numbers to inspect (inspect up to 50 images at a time)\n",
        "base = '/content/drive/My Drive/summer20/classification/'\n",
        "classifier = \"image_type/\" #@param [\"flower_fruit/\", \"image_type/\"]\n",
        "true_imclass = \"illus\" #@param [\"maps\", \"phylo\", \"illus\", \"herb\", \"null\"]\n",
        "PATH_TO_TEST_IMAGES_DIR = base + classifier + \"images/\" + true_imclass\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "start = 150 #@param {type:\"number\"}\n",
        "end =   200#@param {type:\"number\"}\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS[start:end], start=1):\n",
        "    # Load in image\n",
        "    imga = Image.open(im_path)\n",
        "    img = imga.convert('RGB')\n",
        "    image = img.resize(IMAGE_SIZE)\n",
        "    image = np.reshape(image,[1,pixels,pixels,3])\n",
        "    image = image*1./255\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection and draw boxes on image\n",
        "    predictions = imtype_model.predict(image, batch_size=1)\n",
        "    label_num = np.argmax(predictions[0], axis=-1)\n",
        "    conf = predictions[0][label_num]\n",
        "    otherconfa = predictions[0][:label_num]\n",
        "    otherconfb = predictions[0][label_num+1:]\n",
        "    imclass = label_names[label_num]\n",
        "    other_class = label_names[:label_num]+label_names[label_num+1:]\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for {} of {} images'.format(im_num, (end-start)))\n",
        "    # Plot and show detection boxes on images\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    if imga.getbands() == ('R', 'G', 'B', 'A'):\n",
        "      ax.imshow(imga)\n",
        "    else:\n",
        "      ax.imshow(img)\n",
        "    plt.title('color mode: {} \\n {}) Prediction: {}, Confidence: {}, Inference time: {}, \\\n",
        "    \\n Other Predictions: {}, Other Conf: {}, {}'.format(imga.getbands(), im_num, imclass, \\\n",
        "    format(conf, '.2f'), format(end_time-start_time, '.2f'), other_class, \\\n",
        "    otherconfa, otherconfb))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}