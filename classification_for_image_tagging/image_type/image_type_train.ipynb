{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/image_type_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB"
      },
      "source": [
        "# Training Tensorflow MobileNetSSD v2 and Inception v3 models to classify maps, phylogenies, illustrations, and herbarium sheets from EOL images\n",
        "---\n",
        "*Last Updated 10 November 2022*   \n",
        "Train [MobileNet SSD v2](https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4) and [Inception v3](https://tfhub.dev/google/imagenet/inception_v3/classification/4) via fine-tuning (unfreezing lower layers) to classify EOL image types as maps, phylogenies, illustrations, or herbarium sheets. The training dataset consists of image bundles from a variety of sources. Maps and phylogenies are from Wikimedia Commons, herbarium sheets are from NMNH Botany on EOL, botanical illustrations are from Flickr BHL, and zoological images are from EOL. Classifications will be used to generate image tags to improve searchability of EOLv3 images.\n",
        "\n",
        "Training images were downloaded to Google Drive and processed using [image_type_preprocessing.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/image_type_preprocessing.ipynb). Manual inspection of images was used to determine and apply exclusion criteria for each training class.\n",
        "\n",
        "***Models were trained in Python 2 and TF 1 in October 2020: MobileNet SSD v2 was trained for 3 hours to 30 epochs with Batch Size=16, Lr=0.00001, Dropout=0.3, epsilon=1e-7, Adam optimizer. Final validation accuracy = 0.90. Inception v3 was trained for 3.5 hours to 30 epochs with Batch Size=16, Lr=0.0001, Dropout=0.2, epsilon=1, Adam optimizer. Final validation accuracy = 0.89.***\n",
        "\n",
        "Notes:\n",
        "* Run code blocks by pressing play button in brackets on left\n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change parameters using form fields on right (find details at corresponding lines of code by searching '#@param') \n",
        "\n",
        "References:  \n",
        "* https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\n",
        "* https://www.tensorflow.org/tutorials/images/classification\n",
        "* https://medium.com/analytics-vidhya/create-tensorflow-image-classification-model-with-your-own-dataset-in-google-colab-63e9d7853a3e\n",
        "* https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb#scrollTo=umB5tswsfTEQ\n",
        "* https://medium.com/analytics-vidhya/how-to-do-image-classification-on-custom-dataset-using-tensorflow-52309666498e\n",
        "* https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
        "* https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz"
      },
      "source": [
        "## Installs & Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu"
      },
      "source": [
        "#@title Choose where to save results & set up directory structure\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "print(\"Saving results \", save)\n",
        "\n",
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Type in the path to your working directory in form field to right\n",
        "import os\n",
        "cwd = \"/content/drive/MyDrive/train/tf2\" #@param [\"/content/drive/MyDrive/train/tf2\"] {allow-input: true}\n",
        "if not os.path.exists(cwd):\n",
        "    os.makedirs(cwd)\n",
        "\n",
        "# Folder where train/test images will be saved\n",
        "train_folder = \"images\" #@param [\"images\"] {allow-input: true}\n",
        "train_wd = cwd + '/pre-processing/' + train_folder\n",
        "if not os.path.exists(train_wd):\n",
        "    os.makedirs(train_wd)\n",
        "print(\"\\nTraining images directory set to: \\n\", train_wd)\n",
        "\n",
        "# Folder where trained models will be saved\n",
        "saved_models_folder = \"saved_models\" #@param [\"saved_models\"] {allow-input: true}\n",
        "saved_models_wd = cwd + '/' + saved_models_folder\n",
        "if not os.path.exists(saved_models_wd):\n",
        "    os.makedirs(saved_models_wd)\n",
        "print(\"\\nTrained models directory set to: \\n\", train_wd)\n",
        "\n",
        "# Folder where model training graphs will be saved\n",
        "train_graphs_folder = \"train_graphs\" #@param [\"train_graphs\"] {allow-input: true}\n",
        "train_graphs_wd = cwd + '/' + train_graphs_folder\n",
        "if not os.path.exists(train_graphs_wd):\n",
        "    os.makedirs(train_graphs_wd)\n",
        "print(\"\\nModel training graphs directory set to: \\n\", train_graphs_wd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP"
      },
      "source": [
        "# For working with data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import itertools\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For downloading and displaying images\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import ImageFile, Image\n",
        "\n",
        "# For image classification and training through TF-Hub\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, InputLayer\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('Tensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikRQ9LLbVHXQ"
      },
      "source": [
        "## Model & Training Dataset Preparation\n",
        "---\n",
        "Run these blocks every time you train to choose model hyperparameters and training dataset pre-processing steps."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define functions\n",
        "\n",
        "# Set checkpoint paths for training\n",
        "def set_ckpt_path(saved_models_wd, resume_train_ckpt, TRAIN_SESS_NUM, num_epochs):\n",
        "    # If resuming from a previous training attempt (= saved checkpoint)\n",
        "    if resume_train_ckpt:\n",
        "        TRAIN_SESS_NUM = TRAIN_SESS_NUM\n",
        "        CKPTS_PATH = saved_models_wd + TRAIN_SESS_NUM + '/ckpt/'\n",
        "        CKPT_PATH = CKPTS_PATH + 'cp-{epoch:04d}.ckpt'\n",
        "        latest = tf.train.latest_checkpoint(CKPTS_PATH)\n",
        "        print(\"Restoring weights from Model {}, Checkpoint {}\".format(TRAIN_SESS_NUM, latest))\n",
        "        model.load_weights(latest)\n",
        "    # If new training attempt\n",
        "    else:\n",
        "        # Save each new training attempt results in new folder\n",
        "        last_attempt = !ls $saved_models_wd | tail -n 1\n",
        "        if not last_attempt:\n",
        "            last_attempt = 0\n",
        "        # Name folder to sort by attempt number (useful if many training runs)\n",
        "        else:\n",
        "            last_attempt = int(last_attempt.n)\n",
        "            if last_attempt < 9:\n",
        "                TRAIN_SESS_NUM = \"0\" + str(last_attempt + 1)\n",
        "            else:\n",
        "                TRAIN_SESS_NUM = str(last_attempt + 1)\n",
        "    # Set checkpoint naming scheme\n",
        "    CKPT_PATH = saved_models_wd + TRAIN_SESS_NUM + '/ckpt/cp-{epoch:04d}.ckpt' \n",
        "    print(\"Last training attempt number:\", last_attempt)\n",
        "    print(\"Training attempt number: {}, for {} epochs\".format(TRAIN_SESS_NUM, num_epochs))\n",
        "\n",
        "    return CKPT_PATH, last_attempt, TRAIN_SESS_NUM\n",
        "\n",
        "# Set checkpoint callbacks for training\n",
        "def set_ckpt_callbacks(CKPT_PATH):\n",
        "    # Save weights for 0th epoch\n",
        "    model.save_weights(CKPT_PATH.format(epoch=0))\n",
        "    # Create a callback that saves the model's weights during training\n",
        "    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=CKPT_PATH,\n",
        "                                                       save_weights_only=True,\n",
        "                                                       verbose=1) # Verbosity mode: 0 = silent, 1 = progress bar, 2 = one line per epoch\n",
        "    \n",
        "    return ckpt_callback\n",
        "\n",
        "# Plot loss and accuracy for training session\n",
        "def plot_train_graph(train_graphs_dir, TRAIN_SESS_NUM, hist, test_generator):\n",
        "    # Plot loss\n",
        "    plt.figure()\n",
        "    plt.title(\"Attempt {}: Training and Validation Loss\".format(TRAIN_SESS_NUM))\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylim([0,2])\n",
        "    plt.plot(hist[\"loss\"], label='Train')\n",
        "    plt.plot(hist[\"val_loss\"], label='Test')\n",
        "    plt.legend(loc='lower right')\n",
        "    # Plot accuracy\n",
        "    plt.figure()\n",
        "    plt.title(\"Attempt {}: Training and Validation Accuracy\".format(TRAIN_SESS_NUM))\n",
        "    plt.xlabel(\"Training Steps\")\n",
        "    plt.ylim([0,1])\n",
        "    plt.plot(hist[\"accuracy\"], label='Train')\n",
        "    plt.plot(hist[\"val_accuracy\"], label='Test')\n",
        "    plt.legend(loc='upper right')\n",
        "    train_graph_path = train_graphs_dir + TRAIN_SESS_NUM + '.png'\n",
        "    plt.savefig(train_graph_path)\n",
        "    # Print final loss and accuracy values\n",
        "    final_loss, final_accuracy = model.evaluate(test_generator, steps = test_steps)\n",
        "    print('Final loss: {:.2f}'.format(final_loss))\n",
        "    print('Final accuracy: {:.2f}%'.format(final_accuracy * 100))"
      ],
      "metadata": {
        "id": "eW3iTeSsuqmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsPw68vevGGy"
      },
      "source": [
        "#@title Choose model type and batch size\n",
        "\n",
        "# Use pre-trained model or build one from scratch?\n",
        "use_pretrained_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Set model info\n",
        "# To use pretrained model\n",
        "if use_pretrained_model: \n",
        "    # Select pre-trained model to use from Tensorflow Hub Model Zoo\n",
        "    module_selection = (\"mobilenet_v2_1.0_224\", 224) #@param [\"(\\\"mobilenet_v2_1.0_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "    handle_base, pixels = module_selection\n",
        "    IMAGE_SIZE = (pixels, pixels)\n",
        "    if handle_base == \"inception_v3\":\n",
        "        MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/inception_v3/classification/4\".format(handle_base)\n",
        "        epsilon = 1\n",
        "    elif handle_base == \"mobilenet_v2_1.0_224\":\n",
        "        MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\".format(handle_base) \n",
        "        epsilon = 1e-07\n",
        "# To build model from scratch\n",
        "else: \n",
        "    # Set input image size for model\n",
        "    module_selection = (\"from_scratch\", 150)\n",
        "    epsilon = 1\n",
        "\n",
        "# Adjust batch size to make training faster or slower\n",
        "BATCH_SIZE = \"16\" #@param [\"16\", \"32\", \"64\", \"128\"]\n",
        "\n",
        "print(\"Using {} with input size {} and batch size {}\".format(handle_base, IMAGE_SIZE, BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocQ6gDasveZ5"
      },
      "source": [
        "#@title Prepare training dataset *(Note: If output says line of output says \"Found 0 images belonging to...\" run again until a non-zero number is given. Some images throw errors in ImageDataGenerator but are fine for training)*\n",
        "\n",
        "# To suppress warnings from pillow about image sizes\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "Image.MAX_IMAGE_PIXELS = 95000000\n",
        "\n",
        "# Set path to folder where training images were stored in rating_preprocessing.ipynb\n",
        "print(\"Loading training images from: \\n\", train_wd)\n",
        "\n",
        "# Adjust interpolation method and see how training results change\n",
        "interpolation = \"nearest\" #@param [\"nearest\", \"bilinear\"]\n",
        "\n",
        "# Set data generation and flow parameters\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=int(BATCH_SIZE),\n",
        "                       interpolation = interpolation, color_mode='rgb')\n",
        "\n",
        "# Make test dataset\n",
        "test_datagen = ImageDataGenerator(**datagen_kwargs)\n",
        "test_generator = test_datagen.flow_from_directory(train_wd, \n",
        "                                                  subset=\"validation\",\n",
        "                                                  shuffle=True, **dataflow_kwargs)\n",
        "\n",
        "# Make train dataset\n",
        "train_datagen = ImageDataGenerator(**datagen_kwargs)\n",
        "train_generator = train_datagen.flow_from_directory(train_wd, \n",
        "                                                    subset=\"training\", shuffle=True, \n",
        "                                                    **dataflow_kwargs)\n",
        "\n",
        "# Learn more about data batches\n",
        "image_batch_train, label_batch_train = next(iter(train_generator))\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(\"Dataset label classes: \\n\", dataset_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72N6UtPiVQNW"
      },
      "source": [
        "### Prepare Model\n",
        "If fine-tuning a pre-trained model, run first block only. If building custom model from scratch, run second block only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1pGKmlkVS1k"
      },
      "source": [
        "#@title Fine-tune pretrained model\n",
        "\n",
        "# If model is overfitting, add/increase dropout rate\n",
        "dropout_rate = 0.1 #@param {type:\"slider\", min:0, max:0.5, step:0.1}\n",
        "lr = 0.001 #@param [\"0.1\", \"0.01\", \"0.001\", \"0.0001\", \"0.00001\"] {type:\"raw\"}\n",
        "\n",
        "# reeze or unfreeze lower layers for transfer learning and fine tuning\n",
        "## False freezes lower layers so only top classifier is retrained\n",
        "trainable = True #@param [\"True\", \"False\"] {type:\"raw\"} \n",
        "\n",
        "# Build model\n",
        "print(\"Building model with\", handle_base)\n",
        "def create_model():\n",
        "    model = tf.keras.Sequential([InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "                                 hub.KerasLayer(MODULE_HANDLE, trainable=trainable),\n",
        "                                 Dropout(rate = dropout_rate),\n",
        "                                 Dense(train_generator.num_classes,\n",
        "                                       kernel_regularizer=tf.keras.regularizers.l2(0.0001))])\n",
        "  \n",
        "    # Build model\n",
        "    model.build((None,)+IMAGE_SIZE+(3,))\n",
        "  \n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        # Parameters for Adam optimizer\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, \n",
        "                                           beta_2=0.999, epsilon=epsilon, \n",
        "                                           amsgrad=False, name='Adam'), \n",
        "                  # Categorical cross entropy because 5 exclusive classes\n",
        "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, \n",
        "                                                               label_smoothing=0),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create new model instance\n",
        "model = create_model()\n",
        "\n",
        "# Steps per epoch and testing\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMpY5U9k_Tk-"
      },
      "source": [
        "#@title Build model from scratch\n",
        "\n",
        "# Adjust model perforamance using below hyperparameters\n",
        "# See blog post for explanations https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
        "\n",
        "# Adjust learning rate\n",
        "lr = 0.001 #@param [\"0.1\", \"0.01\", \"0.001\", \"0.0001\", \"0.00001\"] {type:\"raw\"}\n",
        "\n",
        "# If model is overfitting, add/increase dropout rate\n",
        "dropout_rate = 0.2 #@param {type:\"slider\", min:0, max:0.7, step:0.1}\n",
        "\n",
        "# Layer 1: Start with smaller number of filters and increase number if performance too low\n",
        "no_filters_lay1 = 64 #@param [\"32\", \"64\", \"128\"] {type:\"raw\"}\n",
        "\n",
        "# Layer 2: Use either the same number of layers as Layer 1, or 2x as many\n",
        "no_filters_lay2 = no_filters_lay1 * 2 #@param [\"no_filters_lay1\", \"no_filters_lay1 * 2\"] {type:\"raw\"}\n",
        "\n",
        "# Layer 3: Use 2x as many layers as Layer 2\n",
        "no_filters_lay3 = no_filters_lay2 * 2\n",
        "\n",
        "# Layer 1: If input image size >128, may need to use initial filter size of 5, 5\n",
        "filter_size_lay1 = (5, 5) #@param [\"(3, 3)\", \"(5, 5)\", \"(7, 7)\"] {type:\"raw\"}\n",
        "\n",
        "# Compile model: Choose loss function. Categorical supposed to be better for multiple classes, but binary got better results one run\n",
        "loss_fun = \"categorical_crossentropy\" #@param [\"categorical_crossentropy\", \"binary_crossentropy\"]\n",
        "print(\"Building model from scratch\")\n",
        "def create_model():\n",
        "  model = Sequential([\n",
        "    Conv2D(no_filters_lay1, filter_size_lay1, padding='same', activation='relu',\n",
        "        input_shape=(IMAGE_SIZE + (3,))),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(no_filters_lay2, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(no_filters_lay3, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(), # this converts our 3D feature maps to 1D feature vectors\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(dropout_rate),\n",
        "    Dense(train_generator.num_classes, activation='softmax') # softmax good for multiple class models with exclusive classes\n",
        "])\n",
        "\n",
        "  # Compile model\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, epsilon=epsilon, amsgrad=False,\n",
        "    name='Adam')\n",
        "  model.compile(loss=loss_fun,\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# Create model instance\n",
        "model = create_model()\n",
        "\n",
        "# Set steps per epoch and testing\n",
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7ycnwnKkkfa"
      },
      "source": [
        "## Train\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNbToKXYlEo5"
      },
      "source": [
        "#@title Train the model\n",
        "\n",
        "# Adjust number of epochs to find balance between underfit and overfit for training\n",
        "num_epochs = '10' #@param {type:\"string\"}\n",
        "\n",
        "# Resume training from previous training attempt/checkpoint ?\n",
        "resume_train_ckpt = True #@param {type:\"boolean\"}\n",
        "# (Optional: Only if above is True) \n",
        "# Resume from which saved checkpoint?\n",
        "if resume_train_ckpt:\n",
        "    TRAIN_SESS_NUM = \"13\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Set where trained models will be saved\n",
        "CKPT_PATH, last_attempt, TRAIN_SESS_NUM = set_ckpt_path(saved_models_wd, \n",
        "                                                        resume_train_ckpt, \n",
        "                                                        TRAIN_SESS_NUM,\n",
        "                                                        num_epochs)\n",
        "\n",
        "# Set up checkpoint callbacks for saving during training \n",
        "ckpt_callback = set_ckpt_callbacks(CKPT_PATH)\n",
        "\n",
        "# Train the model with the new callback\n",
        "hist = model.fit(train_generator,\n",
        "                 epochs=int(num_epochs), steps_per_epoch=steps_per_epoch,\n",
        "                 callbacks=[ckpt_callback],\n",
        "                 validation_data=test_generator,\n",
        "                 validation_steps=test_steps).history\n",
        "\n",
        "# Save trained model \n",
        "saved_model_path = saved_models_wd + TRAIN_SESS_NUM\n",
        "tf.keras.models.save_model(model, saved_model_path)\n",
        "\n",
        "# Plot loss and accuracy for training session\n",
        "plot_train_graph(train_graphs_wd, TRAIN_SESS_NUM, hist, test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpjn2qz-57p_"
      },
      "source": [
        "## Review training results\n",
        "---   \n",
        "Display classification results on images for specific label classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNxq35BeEi-k"
      },
      "source": [
        "#@title Define functions & parameters\n",
        "\n",
        "# Display results for the most recently trained model?\n",
        "use_last_attempt = True #@param {type:\"boolean\"}\n",
        "# (Optional: Only if above is False) TO DO: Resume from which saved checkpoint?\n",
        "TRAIN_SESS_NUM = \"20\" #@param [\"13\", \"11\", \"15\"] {allow-input: true}\n",
        "\n",
        "# Only need to set parameters if not running images through last attempt\n",
        "if not use_last_attempt:\n",
        "    # Select model type for train attempt number\n",
        "    module_selection = (\"inception_v3\", 299) #@param [\"(\\\"mobilenet_v2_1.0_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "    # Label names\n",
        "    dataset_labels = [\"map\", \"phylo\", \"herb\", \"illus\"] #@param [\"[\\\"map\\\", \\\"phylo\\\", \\\"herb\\\", \\\"illus\\\"]\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# Choose which image class to inspect results for\n",
        "true_imclass = \"map\" #@param [\"map\", \"phylo\", \"illus\", \"herb\", \"null\"]\n",
        "\n",
        "# Get test image paths\n",
        "PATH_TO_TEST_IMAGES_DIR = train_wd + '/' + true_imclass\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Load saved model from directory\n",
        "def load_saved_model(use_last_attempt, saved_models_wd, TRAIN_SESS_NUM, module_selection):\n",
        "    # If using last training attempt, get number from director\n",
        "    if use_last_attempt:\n",
        "        # Display results from most recent training attempt\n",
        "        last_attempt = !ls $saved_models_wd | tail -n 1\n",
        "        TRAIN_SESS_NUM = str(last_attempt.n)\n",
        "    # Load trained model from path\n",
        "    saved_model_path = saved_models_wd + TRAIN_SESS_NUM\n",
        "    model = tf.keras.models.load_model(saved_model_path)\n",
        "    # Get name and image size for model type\n",
        "    handle_base, pixels = module_selection\n",
        "\n",
        "    return model, pixels\n",
        "\n",
        "# Define start and stop indices in EOL bundle for running inference   \n",
        "def set_start_stop(run):\n",
        "    # To test with a tiny subset, use 5 random bundle images\n",
        "    if \"tiny subset\" in run:\n",
        "        N = len(TEST_IMAGE_PATHS)\n",
        "        start=np.random.choice(a=N, size=1)[0]\n",
        "        stop=start+5\n",
        "    # To run for up to 50 images\n",
        "    else:\n",
        "        start=np.random.choice(a=N, size=1)[0]\n",
        "        stop=start+50\n",
        "    \n",
        "    return start, stop\n",
        "\n",
        "# For reading an image from filename\n",
        "def filename_to_image(fn):\n",
        "    img = Image.open(im_path)\n",
        "    disp_img = img.convert('RGB')\n",
        "    inf_img = disp_img.resize((pixels, pixels))\n",
        "    inf_img = np.reshape(inf_img,[1,pixels,pixels,3])\n",
        "    image = inf_img*1./255\n",
        "\n",
        "    return image, disp_img\n",
        "\n",
        "# Get info from predictions to display on images\n",
        "def get_predict_info(predictions, im_num, stop, start):\n",
        "    # Get info from predictions\n",
        "    label_num = np.argmax(predictions[0], axis=-1)\n",
        "    conf = predictions[0][label_num]\n",
        "    im_class = dataset_labels[label_num]\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for {} of {} images'.format(im_num, (stop-start)))\n",
        "\n",
        "    return label_num, conf, im_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awIMvGFtGlvE"
      },
      "source": [
        "#@title Run inference\n",
        "\n",
        "# Load saved model\n",
        "model, pixels = load_saved_model(use_last_attempt, saved_models_wd, \n",
        "                                 TRAIN_SESS_NUM, module_selection)\n",
        "\n",
        "# Test pipeline with a smaller subset than 5k images?\n",
        "run = \"test with tiny subset\" #@param [\"test with tiny subset\", \"for all images\"]\n",
        "print(\"Run: \", run)\n",
        "\n",
        "# Run EOL images through trained model and display results\n",
        "start, stop = set_start_stop(run)\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS[start:stop], start=1):\n",
        "    # Load in image\n",
        "    image, disp_img = filename_to_image(im_path)\n",
        "    \n",
        "    # Image classification\n",
        "    start_time = time.time() # Record inference time\n",
        "    predictions = model.predict(image, batch_size=1)\n",
        "    label_num, conf, im_class = get_predict_info(predictions, im_num, stop, start)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Display classification results with images\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(disp_img)\n",
        "    plt.title('{}) Prediction: {}, Confidence: {}, Inference time: {}'.format(\n",
        "              im_num, im_class, format(conf, '.2f'), format(end_time-start_time, '.2f')))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}