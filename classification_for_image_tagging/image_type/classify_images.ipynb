{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classify_images.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMs5f3FSaqS2tI/o4C6uWHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/classify_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TcYNLBrWC0C"
      },
      "source": [
        "# Run images through image type classification pipeline\n",
        "---\n",
        "*Last Updated 30 October 2020*  \n",
        "Classify images as map, phylogeny, illustration, herbarium sheet, or none.\n",
        "\n",
        "1) Use \"cartoonization\" approach to add photographic or non-photographic tags to images. (Low accuracy for illustrations, so this method adds coverage for downstream predictions with low confidence values).\n",
        "\n",
        "2) Run images through trained MobileNet SSD v2 model to add tags to images for image types (map, phylogeny, ilustration, herbarium sheet, non) for predictions with confidence > 1.6. (Confidence value chosen in [inspect_train_results.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/inspect_train_results.ipynb)).\n",
        "\n",
        "3) Display tagging results on images to verify behavior is as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYW4W2aqdnTN"
      },
      "source": [
        "### Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k81-h_UV_ny"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AGFM4fSWhbT"
      },
      "source": [
        "# For working with data and plotting graphs\n",
        "import itertools\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import norm\n",
        "from scipy import sum, average\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "# For image classification and training\n",
        "import tensorflow as tf\n",
        "\n",
        "# For working with images\n",
        "!pip install pillow\n",
        "!pip install scipy==1.1.0\n",
        "import cv2\n",
        "import scipy\n",
        "from scipy import misc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0u9Dd5OmWAO"
      },
      "source": [
        "#### Define functions & variables\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7LrA9zfAlq3"
      },
      "source": [
        "# For images to read in from bundle\n",
        "\n",
        "# Load in image from URL\n",
        "# Modified from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb#scrollTo=JhVecdzJTsKE\n",
        "def image_from_url(url, fn):\n",
        "  file = tf.keras.utils.get_file(fn, url) # Filename doesn't matter\n",
        "  disp_img = tf.keras.preprocessing.image.load_img(file)\n",
        "  img = tf.keras.preprocessing.image.load_img(file, target_size=[224, 224])\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
        "    x[tf.newaxis,...])\n",
        "  return x, disp_img\n",
        "\n",
        "# Read in EOL image bundle dataframe\n",
        "# TO DO: Type in image bundle address using form field to right\n",
        "bundle = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Angiosperms_20K_breakdown_000031.txt' #@param {type:\"string\"}\n",
        "df = pd.read_csv(bundle, sep='\\t', header=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL0DthODQw45"
      },
      "source": [
        "# For exporting tagging results\n",
        "import csv\n",
        "\n",
        "# Write header row of output crops file\n",
        "# TO DO: Change file name for each bundle/run abcd if doing 4 batches using dropdown form to right\n",
        "tags_file = \"tags_imtype_20k_a\" #@param [\"tags_imtype_20k_a\", \"tags_imtype_20k_b\", \"tags_imtype_20k_c\", \"tags_imtype_20k_d\"]\n",
        "tags_fpath = \"/content/drive/My Drive/summer20/classification/image_type/results/\" + tags_file + \".tsv\"\n",
        "with open(tags_fpath, 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"eolMediaURL\", \"identifier\", \\\n",
        "                                       \"dataObjectVersionID\", \"ancestry\", \\\n",
        "                                       \"mnorm_pp\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2tO7HH-uLXY"
      },
      "source": [
        "### Cartoonize images to classify as photographic or non-photographic\n",
        "---   \n",
        "Cartoonify image, then compare change in color values. If change above a certain threshold, then image is likely a photograph. If change below a certain threshold, image is likely a cartoon (non-photograph)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05_nxEYupOY"
      },
      "source": [
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "# TO DO: Set start and end rows to run inference for from EOL image bundle using form field to right\n",
        "# If running in 4 batches of 5000 images, use values in dropdown menu\n",
        "a = 0 #@param [\"0\", \"5000\", \"10000\", \"15000\"] {type:\"raw\"}\n",
        "b = 5 #@param [\"5\", \"5000\", \"10000\", \"15000\", \"20000\"] {type:\"raw\"}\n",
        "\n",
        "# Loop through EOL image bundle to classify images and generate tags\n",
        "for i, row in df.iloc[a:b].iterrows():\n",
        "  try:\n",
        "    # Get url from image bundle\n",
        "    start = time.time()\n",
        "    url = df['eolMediaURL'][i]\n",
        "    # Read in image from url\n",
        "    fn = str(i) + '.jpg'\n",
        "    img, disp_img = image_from_url(url, fn)\n",
        "    # Display image\n",
        "    #_, ax = plt.subplots(figsize=(10, 10))\n",
        "    #plt.title(\"Original\")\n",
        "    #ax.imshow(disp_img)\n",
        "    # Make edges\n",
        "    cv_img = np.array(disp_img) \n",
        "    gray = cv2.cvtColor(cv_img, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.medianBlur(gray, 5) \n",
        "    edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,  \n",
        "                                         cv2.THRESH_BINARY, 9, 9)  \n",
        "    edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "    # Cartoonization - bilateral filter and edges \n",
        "    color = cv2.bilateralFilter(cv_img, 9, 250, 250) \n",
        "    img2 = cv2.bitwise_and(color, edges) \n",
        "\n",
        "    # Calculate difference in original and cartoonized image\n",
        "    # Convert both images from RGB to HSV\n",
        "    HSV_img = cv2.cvtColor(cv_img, cv2.COLOR_RGB2HSV)\n",
        "    HSV_img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2HSV)\n",
        "    # Fnd the difference for H of HSV values of the images\n",
        "    diff = HSV_img[:,:,0]-HSV_img2[:,:,0]\n",
        "    mnorm = sum(abs(diff))  # Manhattan norm\n",
        "    mnorm_pp = mnorm/HSV_img.size # per pixel\n",
        "    end = time.time()\n",
        "    print(\"Completed for {}, {} of {} files in {} seconds\".format(url, i, format(b-a, '.0f'), format(end-start, '.2f')))\n",
        "\n",
        "    # Display cartoonized image\n",
        "    #_, ax = plt.subplots(figsize=(10, 10))\n",
        "    #plt.title(\"Cartoonized \\n Manhattan norm: {} / per pixel {} \\\n",
        "    #\".format(mnorm, mnorm_pp))\n",
        "    #ax.imshow(img2)\n",
        "\n",
        "    # Export tagging results to tsv\n",
        "    # Define variables for export\n",
        "    identifier = df['identifier'][i]\n",
        "    dataObjectVersionID = df['dataObjectVersionID'][i]\n",
        "    ancestry = df['ancestry'][i]\n",
        "    with open(tags_fpath, 'a') as out_file:\n",
        "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "        tsv_writer.writerow([url, identifier, dataObjectVersionID, ancestry, \\\n",
        "                               mnorm_pp])\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2PEaR_a_0QH"
      },
      "source": [
        "### Run images through model(s) for image type classification\n",
        "---   \n",
        "Use model selected in inspect_train_results.ipynb (MobileNet SSD v2) to classify images as map, phylogeny, herbarium sheet, illustration, or none."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEgxXYbTmY1P"
      },
      "source": [
        "#### Run 20K image bundle through classification pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0dRSsDAk-GW",
        "cellView": "both"
      },
      "source": [
        "# Load trained model from path\n",
        "TRAIN_SESS_NUM = \"13\"\n",
        "saved_model_path = '/content/drive/My Drive/summer20/classification/image_type/saved_models/' + TRAIN_SESS_NUM\n",
        "imtype_model = tf.keras.models.load_model(saved_model_path)\n",
        "label_names = ['herbarium sheet', 'illustration', 'map', 'null', 'phylogeny']\n",
        "module_selection = (\"mobilenet_v2_1.0_224\", 224)\n",
        "handle_base, pixels = module_selection\n",
        "IMAGE_SIZE = (pixels, pixels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZXo6iVvBF0G"
      },
      "source": [
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "import time\n",
        "\n",
        "# For exporting results\n",
        "confs = []\n",
        "imclasses = []\n",
        "\n",
        "# Loop through EOL image bundle to classify images and generate tags\n",
        "for i, row in df.iloc[a:b].iterrows():\n",
        "  try:\n",
        "    # Get url from image bundle\n",
        "    url = df['eolMediaURL'][i]\n",
        "    # Read in image from url\n",
        "    fn = str(i) + '.jpg'\n",
        "    img, disp_img = image_from_url(url, fn)\n",
        "    ax.imshow(disp_img)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection and draw boxes on image\n",
        "    # For flowers/fruits (reproductive structures)\n",
        "    predictions = imtype_model.predict(img, batch_size=1)\n",
        "    label_num = np.argmax(predictions)\n",
        "    conf = predictions[0][label_num]\n",
        "    confs.append(conf)\n",
        "    imclass = label_names[label_num]\n",
        "    imclasses.append(imclass)\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for {} of {} images in {} sec'.format(i, (b-a), \\\n",
        "                                            format(end_time-start_time, '.2f')))\n",
        "\n",
        "    # Optional: Show classification results for images\n",
        "    # Only use to view predictions on <50 images at a time\n",
        "    #_, ax = plt.subplots(figsize=(10, 10))\n",
        "    #ax.imshow(disp_img)\n",
        "    #plt.axis('off')\n",
        "    #plt.title(\"{}) Prediction: {}, Confidence: {}%, \\\n",
        "    #\\n Inference Time: {}\".format(i, imclass, conf, \\\n",
        "    #format(end_time-start_time, '.2f')))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(url))\n",
        "\n",
        "# Export tagging results to tsv\n",
        "# Define variables for export\n",
        "classif = pd.DataFrame(([imclasses,confs]))\n",
        "classif = classif.transpose()\n",
        "classif.columns = [\"imclass\", \"conf\"]\n",
        "df = pd.read_csv(tags_fpath, sep='\\t')\n",
        "comb = pd.concat([df, classif], axis=1, ignore_index=True)\n",
        "comb.columns = [\"eolMediaURL\", \"identifier\", \"dataObjectVersionID\", \\\n",
        "                  \"ancestry\", \"mnorm_pp\", \"imclass\", \"conf\"]\n",
        "print(comb.head())\n",
        "comb.to_csv(tags_fpath, sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GrMVL_CTZit"
      },
      "source": [
        "### Post-process classification predictions using Manhattan norm and confidence threshold values\n",
        "---\n",
        "Manhattan norm per pixel threshold (=<2) was determined in cartoonify_images.ipynb. MobileNet SSD v2 confidence threshold (>1.6) was chosen in inspect_train_results.ipynb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw8m9XNWTh85"
      },
      "source": [
        "# Combine exported model predictions and confidence values from above to one dataframe\n",
        "base = '/content/drive/My Drive/summer20/classification/image_type/results/tags_imtype_20k_' \n",
        "exts = ['a.tsv', 'b.tsv', 'c.tsv', 'd.tsv']\n",
        "all_filenames = [base + e for e in exts]\n",
        "df = pd.concat([pd.read_csv(f, sep='\\t', header=0, na_filter = False) for f in all_filenames], ignore_index=True)\n",
        "\n",
        "# Filter predictions using determined confidence value thresholds\n",
        "# Make column for \"reproductive structures present?\" tag\n",
        "df['tag_cartoon'] = np.nan\n",
        "df['tag_imtype'] = np.nan\n",
        "df['problematic'] = np.nan\n",
        "# Adjust final tag based on Model 7 and 11 predictions and confidence values\n",
        "for i, row in df.iterrows():\n",
        "  # If Manhattan norm per pixel from cartoonization =< 2\n",
        "  if df['mnorm_pp'][i]<=2: \n",
        "    df['tag_cartoon'][i] = 'non-photo'\n",
        "  elif df['mnorm_pp'][i]>2: \n",
        "    df['tag_cartoon'][i] = 'photo'\n",
        "for i, row in df.iterrows():\n",
        "  if df['conf'][i]>1.6: \n",
        "    df['tag_imtype'][i] = df['imclass'][i]  \n",
        "  elif df['conf'][i]<=1.6: \n",
        "    df['tag_imtype'][i] = 'none' \n",
        "for i, row in df.iterrows():\n",
        "  if df['imclass'][i]=='illustration': \n",
        "    df['problematic'][i] = 'maybe' \n",
        "for i, row in df.iterrows():\n",
        "  if 'Chloroplastida' in df['ancestry'][i]: \n",
        "    df['problematic'][i] = 'maybe'\n",
        "\n",
        "# Write results to tsv\n",
        "df.to_csv(\"/content/drive/My Drive/summer20/classification/image_type/results/tags_imtype_20k_finaltags.tsv\", sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDnCXzDGVa6t"
      },
      "source": [
        "### Display final classification results on images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxurlbjZJd9q"
      },
      "source": [
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# TO DO: Update file path to finaltags.tsv file\n",
        "path = \"/content/drive/My Drive/summer20/classification/image_type/results/\"\n",
        "f = \"tags_imtype_20k_finaltags.tsv\" #@param\n",
        "fpath = path + f\n",
        "df = pd.read_csv(fpath, sep='\\t', header=0, na_filter = False)\n",
        "\n",
        "# Function to load in image from URL\n",
        "# Modified from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb#scrollTo=JhVecdzJTsKE\n",
        "def image_from_url(url, fn):\n",
        "  file = tf.keras.utils.get_file(fn, url) # Filename doesn't matter\n",
        "  disp_img = tf.keras.preprocessing.image.load_img(file)\n",
        "  img = tf.keras.preprocessing.image.load_img(file, target_size=[224, 224])\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  x = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
        "    x[tf.newaxis,...])\n",
        "  return x, disp_img\n",
        "\n",
        "# TO DO: Set start and end rows to run inference for from EOL image bundle using form field to right\n",
        "# If running in 4 batches of 5000 images, use values in dropdown menu\n",
        "start =  0#@param {type:\"raw\"}\n",
        "end = 50 #@param {type:\"raw\"}\n",
        "\n",
        "# Loop through EOL image bundle to classify images and generate tags\n",
        "for i, row in df.iloc[start:end].iterrows():\n",
        "  try:\n",
        "    # Get url from image bundle\n",
        "    url = df['eolMediaURL'][i]\n",
        "    # Read in image from url\n",
        "    fn = str(i) + '.jpg'\n",
        "    img, disp_img = image_from_url(url, fn)\n",
        "    # Record inference time\n",
        "    tag1 = df['tag_cartoon'][i]\n",
        "    tag2 = df['tag_imtype'][i]\n",
        "    # Display progress message after each image is loaded\n",
        "    print('Successfully loaded {} of {} images'.format(i+1, (end-start)))\n",
        "\n",
        "    # Show classification results for images\n",
        "    # Only use to view predictions on <50 images at a time\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(disp_img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"{}) Photo or not: {}, Image type: {} \".format(i+1, tag1, tag2))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(url))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}