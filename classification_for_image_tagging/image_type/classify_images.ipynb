{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classify_images.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOZw4G1dFo6e/Eo9WYBPpWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/classify_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TcYNLBrWC0C"
      },
      "source": [
        "# Run images through image type classification pipeline\n",
        "---\n",
        "*Last Updated 31 July 2022*  \n",
        "-Runs in Python 3 with Tensorflow 2.0-   \n",
        "\n",
        "Use trained image classification model to add tags for image type (map, phylogeny, illustration, herbarium sheet) to EOL images.\n",
        "\n",
        "Models were trained in [image_type_train.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/image_type_train.ipynb). Confidence threshold for the best trained model was selected in [inspect_train_results.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/inspect_train_results.ipynb). \n",
        "\n",
        "In post-processing, an additional tag is used to filter images because the model did not learn to predict phylogenies or illustrations very well. Using \"cartoonization\" in  [cartoonize_images.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/image_type/cartoonify_images.ipynb), image properties (Manhattan norm per pixel) are leveraged to classify images as photographic or non-photographic. Tags with matching cartoonized and image type tags above chosen thresholds are kept. \n",
        "\n",
        "Finally, display tagging results on images to verify behavior is as expected.\n",
        "\n",
        "***Models were trained in Python 2 and TF 1 in October 2020: MobileNet SSD v2 was trained for 3 hours to 30 epochs with Batch Size=16, Lr=0.00001, Dropout=0.3, epsilon=1e-7, Adam optimizer. Final validation accuracy = 0.90.***\n",
        "\n",
        "Notes:     \n",
        "* Run code blocks by pressing play button in brackets on left\n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change parameters using form fields on right (find details at corresponding lines of code by searching '#@param')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYW4W2aqdnTN"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k81-h_UV_ny"
      },
      "source": [
        "#@title Choose where to save results\n",
        "import os\n",
        "\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image tagging file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "basewd = \"/content/drive/MyDrive/train\" #@param [\"/content/drive/MyDrive/train\"] {allow-input: true}\n",
        "\n",
        "# Make folder for image tags within base wd\n",
        "cwd = basewd + '/results/'\n",
        "if not os.path.exists(cwd):\n",
        "    os.makedirs(cwd)\n",
        "\n",
        "print(\"Saving results \", save)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AGFM4fSWhbT"
      },
      "source": [
        "#@title Import libraries\n",
        "\n",
        "# For downloading and displaying images\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "# For working with data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "import csv\n",
        "import itertools\n",
        "from scipy.linalg import norm\n",
        "from scipy import sum, average\n",
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For image classification\n",
        "import tensorflow as tf\n",
        "print('\\nTensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0u9Dd5OmWAO"
      },
      "source": [
        "### Prepare classification functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define functions\n",
        "\n",
        "# To read in EOL formatted data files\n",
        "def read_datafile(fpath, sep=\"\\t\", header=0, disp_head=True, lineterminator='\\n', encoding='latin1'):\n",
        "    try:\n",
        "        df = pd.read_csv(fpath, sep=sep, header=header, lineterminator=lineterminator, encoding=encoding)\n",
        "        if disp_head:\n",
        "          print(\"Data header: \\n\", df.head())\n",
        "    except FileNotFoundError as e:\n",
        "        raise Exception(\"File not found: Enter the path to your file in form field and re-run\").with_traceback(e.__traceback__)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Define start and stop indices in EOL bundle for running inference   \n",
        "def set_start_stop(df):\n",
        "    # To test with a tiny subset, use 5 random bundle images\n",
        "    if \"tiny subset\" in run:\n",
        "        start=np.random.choice(a=len(df), size=1)[0]\n",
        "        stop=start+5\n",
        "    # To run inference on 4 batches of 5k images each\n",
        "    elif \"_a.\" in outfpath: # batch a is from 0-5000\n",
        "        start=0\n",
        "        stop=5000\n",
        "    elif \"_b.\" in outfpath: # batch b is from 5000-1000\n",
        "        start=5000\n",
        "        stop=10000\n",
        "    elif \"_c.\" in outfpath: # batch c is from 10000-15000\n",
        "        start=10000\n",
        "        stop=15000\n",
        "    elif \"_d.\" in outfpath: # batch d is from 15000-20000\n",
        "        start=15000\n",
        "        stop=20000\n",
        "\n",
        "    return start, stop\n",
        "\n",
        "# Load in image from URL\n",
        "# Modified from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb#scrollTo=JhVecdzJTsKE\n",
        "def image_from_url(url, fn):\n",
        "    # Formatted for classification\n",
        "    f = tf.keras.utils.get_file(fn, url) # Filename doesn't matter\n",
        "    disp_img = tf.keras.preprocessing.image.load_img(f) # For display\n",
        "    img_cv = np.array(disp_img) # For working with cv2 lib\n",
        "    image = tf.keras.preprocessing.image.load_img(f, target_size=[pixels, pixels])\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
        "        image[tf.newaxis,...])\n",
        "    \n",
        "    return image, disp_img, img_cv\n",
        "\n",
        "# To cartoonize an image\n",
        "def cartoonize(img_cv):\n",
        "    # Add edges\n",
        "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY) \n",
        "    gray = cv2.medianBlur(gray, 5) \n",
        "    edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,  \n",
        "                                         cv2.THRESH_BINARY, 9, 9)  \n",
        "    edges = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
        "    # Bilateral filter \n",
        "    color = cv2.bilateralFilter(img_cv, 9, 250, 250) \n",
        "    img2 = cv2.bitwise_and(color, edges)\n",
        "\n",
        "    return img2\n",
        "\n",
        "# Calculate differences between original and cartoonized image\n",
        "def calc_img_diffs(img, img2):\n",
        "    # Convert both images from RGB to HSV\n",
        "    HSV_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
        "    HSV_img2 = cv2.cvtColor(img2, cv2.COLOR_RGB2HSV)\n",
        "    # Fnd the difference for H of HSV values of the images\n",
        "    diff = HSV_img[:,:,0]-HSV_img2[:,:,0]\n",
        "    mnorm = sum(abs(diff))  # Manhattan norm\n",
        "    mnorm_pp = mnorm/HSV_img.size # per pixel\n",
        "    znorm = norm(diff.ravel(), 0)  # Zero norm\n",
        "    znorm_pp = znorm*1.0/HSV_img2.size # per pixel\n",
        "\n",
        "    return mnorm_pp, mnorm, znorm_pp, znorm\n",
        "\n",
        "\n",
        "# Load saved model from directory\n",
        "def load_saved_model(saved_models_dir, TRAIN_SESS_NUM, module_selection):\n",
        "    # Load trained model from path\n",
        "    saved_model_path = saved_models_dir + TRAIN_SESS_NUM\n",
        "    model = tf.keras.models.load_model(saved_model_path)\n",
        "    # Get name and image size for model type\n",
        "    handle_base, pixels = module_selection\n",
        "\n",
        "    return model, pixels, handle_base\n",
        "\n",
        "# Get info from predictions to display on images\n",
        "def get_predict_info(predictions, url, i, stop, start):\n",
        "    # Get info from predictions\n",
        "    label_num = np.argmax(predictions[0], axis=-1)\n",
        "    conf = predictions[0][label_num]\n",
        "    im_class = dataset_labels[label_num]\n",
        "    # Display progress message after each image\n",
        "    print(\"Completed for {}, {} of {} files\".format(url, i+1, format(stop-start, '.0f')))\n",
        "    \n",
        "    return label_num, conf, im_class\n",
        "\n",
        "# Set filename for saving classification results\n",
        "def set_outpath(tags_file):\n",
        "    outpath = basewd + '/results/' + tags_file + '.tsv'\n",
        "    print(\"\\nSaving results to: \\n\", outpath)\n",
        "\n",
        "    return outpath\n",
        "\n",
        "# Export results\n",
        "def export_results(df, url, mnorm_pp, det_imclass, conf):\n",
        "    # Define variables for export\n",
        "    if 'ancestry' in df.columns:\n",
        "        ancestry = df['ancestry'][i]\n",
        "    else:\n",
        "        ancestry = \"NA\"\n",
        "    identifier = df['identifier'][i]\n",
        "    dataObjectVersionID = df['dataObjectVersionID'][i] \n",
        "    # Write row with results for each image\n",
        "    results = [url, identifier, dataObjectVersionID, ancestry, mnorm_pp, \n",
        "               det_imclass, conf]\n",
        "    with open(outfpath, 'a') as out_file:\n",
        "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "        tsv_writer.writerow(results)"
      ],
      "metadata": {
        "id": "nbFSjm5qWjp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7LrA9zfAlq3"
      },
      "source": [
        "#@title Choose saved model parameters (if using EOL model, defaults are already selected)\n",
        "\n",
        "# Use EOL pre-trained model for object detection?\n",
        "use_EOL_model = True #@param {type: \"boolean\"}\n",
        "\n",
        "# Get info about trained classification model\n",
        "def get_model_info(use_EOL_model):\n",
        "    # Use EOL pre-trained model\n",
        "    if use_EOL_model:\n",
        "        # Model metadata\n",
        "        module_selection =('mobilenet_v2_1.0_224', 224)\n",
        "        dataset_labels = [\"map\", \"phylo\", \"illus\", \"herb\", \"null\"]\n",
        "        TRAIN_SESS_NUM = '13'\n",
        "        saved_models_dir = basewd + '/saved_models/'\n",
        "        # If running for the first time, download model\n",
        "        if not os.path.exists(saved_models_dir):\n",
        "            # Make folder for trained model\n",
        "            os.makedirs(saved_models_dir)\n",
        "            %cd $saved_models_dir\n",
        "            os.makedirs(TRAIN_SESS_NUM)\n",
        "            # Download saved model files for Run 13 - MobileNet SSD v2\n",
        "            !gdown --id 1Fr1x5ZLXdd-DBZ7yRWx691orbtXh9lW1\n",
        "            !unzip 13.zip -d .\n",
        "            !mv -v content/drive/'My Drive'/summer20/classification/image_type/saved_models/13/* 13\n",
        "            !rm -r content\n",
        "            !rm -r 13.zip\n",
        "            %cd ../\n",
        "            print(\"\\nSuccessfully downloaded pre-trained EOL model to: \\n\", (saved_models_dir + TRAIN_SESS_NUM))\n",
        "    \n",
        "    # Use your own trained model\n",
        "    elif not use_EOL_model:\n",
        "        # Change values to match your trained model\n",
        "        module_selection = (\"mobilenet_v2_1.0_224\", 224) #@param [\"(\\\"mobilenet_v2_1.0_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "        dataset_labels = [\"map\", \"phylo\", \"illus\", \"herb\", \"null\"] #@param [\"[\\\"map\\\", \\\"phylo\\\", \\\"illus\\\", \\\"herb\\\", \\\"null\\\"]\"] {type:\"raw\", allow-input: true}\n",
        "        saved_models_dir = \"train/saved_models/\" #@param [\"train/saved_models/\"] {allow-input: true}\n",
        "TRAIN_SESS_NUM = \"13\" #@param [\"13\"] {allow-input: true}\n",
        "\n",
        "    return module_selection, dataset_labels, saved_models_dir, TRAIN_SESS_NUM\n",
        "\n",
        "# Load saved model\n",
        "module_selection, dataset_labels, saved_models_dir, TRAIN_SESS_NUM = get_model_info(use_EOL_model)\n",
        "model, pixels, handle_base = load_saved_model(saved_models_dir, TRAIN_SESS_NUM, module_selection)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEWlhuLnV-lh"
      },
      "source": [
        "### Generate tags: Run inference on EOL images & save results for tagging - Run 4X for batches A-D\n",
        "Use 20K EOL image bundle to classify image type as map, phylogeny, illustration, or herbarium sheet. Results are saved to [tags_file].tsv. Run this section 4 times (to make batches A-D) of 5K images each to incrementally save in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O60NDALk_8LW"
      },
      "source": [
        "#@title Enter EOL image bundle and choose inference settings. Change **tags_file** for each batch A-D\n",
        "%cd $cwd\n",
        "\n",
        "# Load in EOL image bundle\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_000001.txt\" #@param [\"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Coleoptera_20K_breakdown_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Anura_20K_breakdown_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Carnivora_20K_breakdown_000001.txt\"] {allow-input: true}\n",
        "df = read_datafile(bundle, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# Test pipeline with a smaller subset than 5k images?\n",
        "run = \"test with tiny subset\" #@param [\"test with tiny subset\", \"for all images\"]\n",
        "\n",
        "# Take 5k subset of bundle for running inference\n",
        "# Change filename for each batch\n",
        "tags_file = \"image_type_tags_tf2_d\" #@param [\"image_type_tags_tf2_a\", \"image_type_tags_tf2_b\", \"image_type_tags_tf2_c\", \"image_type_tags_tf2_d\"] {allow-input: true}\n",
        "outfpath = set_outpath(tags_file)\n",
        "\n",
        "# Write header row of tagging file\n",
        "if not os.path.isfile(outfpath): \n",
        "    with open(outfpath, 'a') as out_file:\n",
        "              tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "              tsv_writer.writerow([\"eolMediaURL\", \"identifier\", \n",
        "                                   \"dataObjectVersionID\", \"ancestry\", \\\n",
        "                                   \"mnorm_pp\", \"imclass\", \"conf\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_05_nxEYupOY"
      },
      "source": [
        "#@title Run inference and cartoonify images as photographic/non-photographic\n",
        "start, stop = set_start_stop(df)\n",
        "for i, row in enumerate(df.iloc[start:stop].iterrows()):\n",
        "    try:       \n",
        "        # Read in image from url\n",
        "        url = df['eolMediaURL'][i]\n",
        "        fn = str(i) + '.jpg'\n",
        "        img, disp_img, img_cv = image_from_url(url, fn)\n",
        "\n",
        "        # Cartoonization\n",
        "        img2 = cartoonize(img_cv)\n",
        "        # Calculate differences between original and cartoonized image\n",
        "        mnorm_pp, _, _, _ = calc_img_diffs(img_cv, img2)\n",
        "\n",
        "        # Image classification\n",
        "        start_time = time.time() # Record inference time\n",
        "        predictions = model.predict(img, batch_size=1)\n",
        "        label_num, conf, det_imclass = get_predict_info(predictions, url, i, stop, start)\n",
        "        end_time = time.time()\n",
        "        print(\"Inference time: {} sec\".format(format(end_time-start_time, '.2f')))\n",
        "\n",
        "        # Export cartoonization results to tsv\n",
        "        export_results(df, url, mnorm_pp, det_imclass, conf)\n",
        "\n",
        "    except:\n",
        "        print('Check if URL from {} is valid'.format(url))\n",
        "\n",
        "print(\"\\n\\n~~~\\nInference complete! Run these steps for remaining batches A-D before proceeding.\\n~~~\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GrMVL_CTZit"
      },
      "source": [
        "## Post-process classification predictions using threshold values\n",
        "---\n",
        "Use Manhattan norm values for classifying images as photographic or non-photographic (Manhattan norm per pixel threshold < 2). Then, use MobileNet SSD v2 confidence values for classifying image type (confidence threshold > 1.6). When tag types do not match or are outside of thresholds, they are discarded.\n",
        "\n",
        "*Example of tag types matching: Phylogeny, map, and illustration are all non-photographic (cartoons). If an image is classified as photographic and any of those classes, the tags will be discarded because it is likely a misidentification.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtrzAD675GXx"
      },
      "source": [
        "### Prepare post-processing functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLcphqOl45li"
      },
      "source": [
        "#@title Adjust Manhattan norm and confidence threshold parameters (or use EOL defaults)\n",
        "\n",
        "# Are images botanical? \n",
        "botanical = False #@param {type:\"boolean\"}\n",
        "\n",
        "# More strict primary cartoonization and confidence thresholds\n",
        "mnorm_thresh = 2 #@param [\"2\"] {type:\"raw\", allow-input: true}\n",
        "conf_thresh = 1.6 #@param [\"1.6\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# Less strict secondary cartoonization and confidence thresholds\n",
        "mnorm_thresh2 = 15 #@param [\"15\"] {type:\"raw\", allow-input: true}\n",
        "conf_thresh2 = 0.05 #@param [\"0.05\"] {type:\"raw\", allow-input: true}\n",
        " \n",
        "# Intermediate cartoonization threshold for herbarium sheets (less colors than photographic image, more than illustration)\n",
        "mnorm_herb = 20 #@param [\"20\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# Combine tagging files for batches A-D\n",
        "def combine_tag_files(tags_fpath):\n",
        "    # Combine tag files for batches A-D\n",
        "    fpath =  os.path.splitext(tags_fpath)[0]\n",
        "    base = fpath.rsplit('_',1)[0] + '_'\n",
        "    exts = ['a.tsv', 'b.tsv', 'c.tsv', 'd.tsv'] \n",
        "    all_filenames = [base + e for e in exts]\n",
        "    df = pd.concat([pd.read_csv(f, sep='\\t', header=0, na_filter = False) for f in all_filenames], ignore_index=True)\n",
        "    # Choose desired columns for tagging\n",
        "    df[['mnorm_pp', 'conf']] = df[['mnorm_pp', 'conf']].apply(pd.to_numeric)\n",
        "    df.rename(columns={'conf': 'confidence', 'imclass': 'tag'}, inplace=True)\n",
        "    # Make empty columns for final tag types\n",
        "    df['tag_cartoon'] = np.nan\n",
        "    df['tag_imtype'] = np.nan\n",
        "    df['problematic'] = np.nan\n",
        "\n",
        "    return df, base\n",
        "\n",
        "# Filter by thresholds\n",
        "def filter_by_thresholds(df, cartoonization, confidence):\n",
        "    # Filter by cartoonization threshold for photo or non-photo\n",
        "    idx_tokeep = df.index[df.mnorm_pp <= mnorm_thresh]\n",
        "    idx_todiscard = df.index.difference(idx_tokeep)\n",
        "    df.loc[idx_tokeep, 'tag_cartoon'] = 'non-photo'\n",
        "    df.loc[idx_todiscard, 'tag_cartoon'] = 'photo'\n",
        "\n",
        "    # Filter by classification confidence threshold\n",
        "    idx_tokeep = df.index[df.confidence > conf_thresh]\n",
        "    idx_todiscard = df.index.difference(idx_tokeep)\n",
        "    df.loc[idx_tokeep, 'tag_imtype'] = df.loc[idx_tokeep, 'tag']\n",
        "    df.loc[idx_todiscard, 'tag_imtype'] = 'null'\n",
        "\n",
        "    return df\n",
        "\n",
        "# Add 'problematic' tags to classes model didn't learn well\n",
        "def add_problematic_tags(df):\n",
        "    # Tagged as illustration\n",
        "    idx_tokeep = df.index[df.tag == 'illus']\n",
        "    df.loc[idx_tokeep, 'problematic'] = 'maybe'\n",
        "    # Any image of grasses\n",
        "    idx_todiscard = df.index[df.ancestry.str.contains('Poaceae')]\n",
        "    df.loc[idx_todiscard, 'problematic'] = 'maybe'\n",
        "\n",
        "    return df\n",
        "\n",
        "# Remove conflicting cartoonization and classification tags\n",
        "def remove_conflicting_tags(df):\n",
        "    # Photographic and non-photographic image class names\n",
        "    photos = 'herb|null' \n",
        "    nonphotos = 'phylo|map|illus' \n",
        "    \n",
        "    # Images tagged as non-photographic by cartoonization\n",
        "    idx_todiscard = df.index[df.tag_cartoon == 'non-photo']\n",
        "    # Make any non-photos tagged as photo by classification 'null'\n",
        "    idx_todiscard = df.index[df.loc[idx_todiscard, 'tag_imtype'].str.contains(photos)]\n",
        "    df.loc[idx_todiscard, 'tag_imtype'] = 'null'\n",
        "    \n",
        "    # Images tagged as photographic by cartoonization\n",
        "    idx_todiscard = df.index[df.tag_cartoon == 'photo']\n",
        "    # If any photos tagged as non-photo by classification, make 'null'\n",
        "    idx_todiscard = df.index[df.loc[idx_todiscard, 'tag_imtype'].str.contains(nonphotos)]\n",
        "    df.loc[idx_todiscard, 'tag_imtype'] = 'null'\n",
        "\n",
        "    return df\n",
        "\n",
        "# Refine botanical image classifications (plants are rule-breakers)\n",
        "def refine_botanical_images(df):\n",
        "    # If images are zoological, remove any botanical classifications\n",
        "    if not botanical:\n",
        "        idx_todiscard = df.index[df.tag == 'herb']\n",
        "        df.loc[idx_todiscard, 'tag_imtype'] = 'null'\n",
        "    else:\n",
        "        # Filter by less strict cartoonization & confidence thresholds\n",
        "        # (Herbarium sheet mnorms fall between 'photo' and 'non-photo' thresholds\n",
        "        # and had fewer false classif dets)\n",
        "        idx_tokeep = df.index[(df.mnorm_pp <= mnorm_thresh2) & (df.confidence > conf_thresh2)]\n",
        "        df.loc[idx_tokeep, 'tag_cartoon'] = 'non-photo'\n",
        "        df.loc[idx_tokeep, 'tag_imtype'] = 'herb'\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpyJZTh4mJ_e"
      },
      "source": [
        "#@title Post-process image classifications\n",
        "\n",
        "# Combine tagging files for batches A-D\n",
        "df, base = combine_tag_files(tags_file)\n",
        "print(\"Model predictions for Training Attempt {}, {}:\".format(TRAIN_SESS_NUM, handle_base))\n",
        "print(\"Total Images: {}\\n{}\".format(len(df), df[['eolMediaURL', 'mnorm_pp', 'tag', 'confidence']].head()))\n",
        "\n",
        "# Filter predictions using determined confidence value thresholds\n",
        "\n",
        "# Filter by cartoonization and confidence thresholds \n",
        "df = filter_by_thresholds(df, mnorm_thresh, conf_thresh)\n",
        "\n",
        "# Add 'problematic' tags to classes model didn't learn well\n",
        "df = add_problematic_tags(df) \n",
        "\n",
        "# Remove conflicting cartoonization and classification tags\n",
        "df = remove_conflicting_tags(df)\n",
        "\n",
        "# Refine botanical image classifications (plants are rule-breakers)\n",
        "df = refine_botanical_images(df)\n",
        "\n",
        "# Write results to tsv\n",
        "print(\"\\nFinal tagging dataset after filtering predictions: \\n\", df[['eolMediaURL', 'tag_imtype', 'tag_cartoon']].head())\n",
        "outfpath = cwd + base + 'final.tsv'\n",
        "print(\"\\nSaving results to: \\n\", outfpath)\n",
        "df.to_csv(outfpath, sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDnCXzDGVa6t"
      },
      "source": [
        "## Display classification results on images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxurlbjZJd9q"
      },
      "source": [
        "#@title Adjust start index and display up to 50 images with their tags\n",
        "\n",
        "# Adjust start index using slider\n",
        "start = 0 #@param {type:\"slider\", min:0, max:5000, step:50}\n",
        "stop = min((start+50), len(df))\n",
        "\n",
        "# Loop through EOL image bundle to classify images and generate tags\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "    try:\n",
        "        # Read in image from url\n",
        "        url = df['eolMediaURL'][i]\n",
        "        fn = str(i) + '.jpg'\n",
        "        img, disp_img, img_cv = image_from_url(url, fn)\n",
        "    \n",
        "        # Get image type tag\n",
        "        tag = df['tag_imtype'][i]\n",
        "    \n",
        "        # Display progress message after each image is loaded\n",
        "        print('Successfully loaded {} of {} images'.format(i+1, (stop-start)))\n",
        "\n",
        "        # Show classification results for images\n",
        "        # Only use to view predictions on <50 images at a time\n",
        "        _, ax = plt.subplots(figsize=(10, 10))\n",
        "        ax.imshow(disp_img)\n",
        "        plt.axis('off')\n",
        "        plt.title(\"{}) Image type: {} \".format(i+1, tag))\n",
        "\n",
        "    except:\n",
        "        print('Check if URL from {} is valid'.format(url))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}