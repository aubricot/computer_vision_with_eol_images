{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rating_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOE/dYpad7I/I0QwjL7n5A0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/rating/rating_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGFNOrAs0pMf"
      },
      "source": [
        "# Pre-process Image Rating Classifier Training Images\n",
        "---\n",
        "*Last Updated 20 Oct 2021*   \n",
        "Follow steps below to make training and testing datasets using the [EOL user generated image ratings file](https://editors.eol.org/other_files/EOL_v2_files/image_ratings.txt.zip). 7K images per rating class (1 - 5) are downloaded to Google Drive for use training models in [rating_train.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/rating/rating_train.ipynb).     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F53iiacTFVz7"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeNoVQDN0I1q"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxIUuGkDa__r",
        "cellView": "code"
      },
      "source": [
        "# Install libraries for working with datasets\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# For downloading images\n",
        "!apt-get install aria2\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# To read in EOL formatted data files\n",
        "def read_datafile(fpath, sep=\"\\t\", header=0, disp_head=True, lineterminator='\\n', encoding='latin1'):\n",
        "    \"\"\"\n",
        "    Defaults to tab-separated data files with header in row 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(fpath, sep=sep, header=header, lineterminator=lineterminator, encoding=encoding)\n",
        "        if disp_head:\n",
        "          print(\"Data header: \\n\", df.head())\n",
        "    except FileNotFoundError as e:\n",
        "        raise Exception(\"File not found: Enter the path to your file in form field and re-run\").with_traceback(e.__traceback__)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Suppress pandas warning about writing over a copy of data\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Filter by rating of interest\n",
        "def filter_by_rating(df, filter=filter, disp_head=False):\n",
        "    rating = df.loc[round(df[\"overall_rating\"])==int(filter)]\n",
        "    rating = rating[\"obj_url\"].copy()\n",
        "    \n",
        "    if disp_head:\n",
        "          print(\"Rating = {}}:\\n {}\".format(filter, rating.head()))\n",
        "    print(\"\\n Number of available ratings for training/testing class {}: \\n {}\".format(filter, len(rating)))\n",
        "\n",
        "    return rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZlJ7Rjaub3O"
      },
      "source": [
        "### 1) Inspect EOL User Generated Image Ratings File\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoSSBABRk6tS"
      },
      "source": [
        "# Download EOL user generated rating data\n",
        "\n",
        "# TO DO: Type in the path to your working directory in form field to right\n",
        "wd = \"/content/drive/MyDrive/train\" #@param {type:\"string\"}\n",
        "\n",
        "# Download EOL user generated rating file to temporary runtime location\n",
        "!wget --user-agent=\"Mozilla\" https://editors.eol.org/other_files/EOL_v2_files/image_ratings.txt.zip\n",
        "\n",
        "# Unzip cropping file to your working directory\n",
        "!unzip /content/image_ratings.txt.zip -d $wd\n",
        "\n",
        "# Change to your training directory within Google Drive and move to pre-processing/\n",
        "%cd $wd\n",
        "!mkdir pre-processing\n",
        "!mv image_ratings.txt pre-processing/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E9mmcA-Dekh"
      },
      "source": [
        "# Filter EOL rating coordinates by class (1-bad to 5-good)\n",
        "\n",
        "# Optional = Un-comment out if want to view individual URLs, increase output length so full URL visible\n",
        "pd.set_option('display.max_colwidth', 1000) # Print full urls for inspection\n",
        "\n",
        "# Read in user-generated image rating file\n",
        "fpath = 'pre-processing/' + 'image_ratings.txt'\n",
        "df = read_datafile(fpath)\n",
        "\n",
        "# Make train and test datasets for each rating class\n",
        "# List of rating classes to filter by\n",
        "filters = ['1', '2', '3', '4', '5']\n",
        "\n",
        "for filter in filters:\n",
        "    # Filter by rating of interest \n",
        "    filtered = filter_by_rating(df, filter, disp_head=False)\n",
        "\n",
        "    # Make folder for rating class\n",
        "    dir = 'pre-processing/images/' + filter\n",
        "    os.makedirs(dir)\n",
        "\n",
        "    # Export filtered dataset as txt to folder of interest\n",
        "    outfpath = dir + '/' + filter + '_download.txt'\n",
        "    filtered.to_csv(outfpath, sep='\\n', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUzJS0BsYh0f"
      },
      "source": [
        "### 2) Build 7k image bundles for rating classes 1-5\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt7zEGcKZF97"
      },
      "source": [
        "# Make 7k image bundles for rating classes 1-4 \n",
        "# Rating class 5 only has 1200 images, so it is built differently in next code block\n",
        "%cd pre-processing/images\n",
        "\n",
        "# Future image directories for training classifier\n",
        "ratings = filters[:4]\n",
        "all_filenames = [rating + '/' + rating + '_download.txt' for rating in ratings] # Image rating filenames\n",
        "filenames_7k = [rating + '/' + rating + '_download_7k.txt' for rating in ratings] # Future 7K image bundle filenames\n",
        "\n",
        "# Randomly pick 7,000 images from each rating class and write to csv\n",
        "for num, filename in enumerate(all_filenames):\n",
        "    df = pd.read_table(filename, sep='\\n')\n",
        "    bundle = df.sample(7000)\n",
        "    fn = str(filenames_7k[num])\n",
        "    print(\"7k image bundle filename for rating {}: {}\\n{}\\n\".format((num+1), fn, bundle.head()))\n",
        "    bundle.to_csv(fn, sep='\\n', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsoet2PW6G-5"
      },
      "source": [
        "# Make 7k bundle for Rating = 5 dataset\n",
        "# Different because only 1200 images total\n",
        "\n",
        "# Add images to Rating = 5 dataset from EOL User Exemplar File\n",
        "# Read in Exemplar File\n",
        "# Download EOL user generated rating file to temporary runtime location\n",
        "!wget --user-agent=\"Mozilla\" https://editors.eol.org/other_files/EOL_v2_files/images_selected_as_exemplar.txt.zip\n",
        "\n",
        "# Unzip cropping file to your working directory\n",
        "!unzip images_selected_as_exemplar.txt.zip -d ./5\n",
        "\n",
        "# Read in user-generated image exemplars file\n",
        "fpath = '5/' + 'images_selected_as_exemplar.txt'\n",
        "df = read_datafile(fpath)\n",
        "\n",
        "# Include all duplicates from exemplar file \n",
        "# (these ones may be better or more controversial, see email from JH 28 Oct 2020)\n",
        "idx = df.index[df.duplicated(['object_url'])].tolist()\n",
        "dups = df.loc[idx]\n",
        "dups = pd.DataFrame(dups[\"object_url\"])\n",
        "# Add 4k random images from exemplar file\n",
        "unq = df.drop(idx, errors='ignore')\n",
        "unq = unq.sample(4000)\n",
        "unq = pd.DataFrame(unq[\"object_url\"])\n",
        "# Read in Rating = 5 images\n",
        "df1 = pd.read_table('5/5_download.txt', sep='\\n')\n",
        "df1.columns = unq.columns\n",
        "# Make combined 7k bundle from Exemplar duplicates & random images, + Rating = 5 images\n",
        "comb = pd.concat([df1,unq,dups], ignore_index=True)\n",
        "print(\"Rating = 5:\\n {}\".format(comb.head()))\n",
        "print(\"\\n Number of available ratings for training/testing class 5: \\n {}\".format(len(comb)))\n",
        "comb.to_csv('5/5_download_7k.txt', sep='\\n', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOJEcY_BYYjl"
      },
      "source": [
        "### 3) Download images to Google Drive\n",
        "---\n",
        "Run all steps once per rating class 1-5. Where you see 'TO DO' (3 places), change number to match rating class each time you run "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQBcC0zzdrVy"
      },
      "source": [
        "# Optional: Test downloads with a small subset first?\n",
        "\n",
        "# TO DO: If yes, check test_with_tiny_subset box\n",
        "test_with_tiny_subset = True #@param {type: \"boolean\"}\n",
        "\n",
        "cwd = wd + 'pre-processing/images'\n",
        "%cd $cwd\n",
        "\n",
        "# Test downloads with tiny subset\n",
        "if test_with_tiny_subset:\n",
        "    filenames_tiny = []\n",
        "    # Make tiny subsets with only 5 images per class\n",
        "    for fn in filenames_7k:\n",
        "        df = pd.read_table(fn, sep='\\n')\n",
        "        df1 = df.head().copy()\n",
        "        fn1 = os.path.splitext(fn)[0] + '_tinysubset.txt'\n",
        "        filenames_tiny.append(fn1)\n",
        "        df1.to_csv(fn1, sep='\\n', index=False, header=False)\n",
        "\n",
        "    # Download images\n",
        "    # Loop through image data files to download images into their respective folders\n",
        "    for num, fn in enumerate(filenames_tiny, start=1):\n",
        "        # Download images\n",
        "        cwd = wd + '/pre-processing/images/' + str(num)\n",
        "        %cd $cwd\n",
        "        fn = os.path.basename(fn)\n",
        "        !aria2c -x 16 -s 1 -i $fn\n",
        "\n",
        "    # Move text file to image_data/bundles\n",
        "    %cd ../..\n",
        "    fpath = 'images/' + str(num) + '/*.txt'\n",
        "    !mv $fpath image_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3VBccvEjEj1"
      },
      "source": [
        "# Run for all images\n",
        "# Make a folder to store image data files after downloading images\n",
        "cwd = wd + '/pre-processing'\n",
        "%cd $cwd\n",
        "!mkdir image_data\n",
        "\n",
        "# Loop through image data files for each rating class to download images into their respective folders\n",
        "for num, fn in enumerate(filenames_7k, start=1):\n",
        "    # Download images (this will take ~2 hours / class)\n",
        "    cwd = wd + '/pre-processing/images/' + str(num)\n",
        "    %cd $cwd\n",
        "    fn = os.path.basename(fn)\n",
        "    !aria2c -x 16 -s 1 -i $fn\n",
        "    \n",
        "    # Check how many images downloaded\n",
        "    print(\"Number of images downloaded to Google Drive: \")\n",
        "    !ls . | wc -l\n",
        "\n",
        "    # Move text file to image_data/bundles\n",
        "    %cd ../..\n",
        "    fpath = 'images/' + str(num) + '/*.txt'\n",
        "    !mv $fpath image_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uELne7tJrEDZ"
      },
      "source": [
        "### 4) Delete all downloaded non-image files\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTPAxR5V-wTm"
      },
      "source": [
        "# Find and delete all downloaded non-image files\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "Image.MAX_IMAGE_PIXELS = 95000000 # To suppress errors from Pillow about decompression bombs\n",
        "import io\n",
        "\n",
        "# Loop through image rating class folders 1 - 5\n",
        "for num in range(1,6):\n",
        "    cwd = wd + '/pre-processing/images/' + str(num)\n",
        "    %cd $cwd\n",
        "    # Inspect each file in folder\n",
        "    for path in listdir('./'):\n",
        "        with open(path, 'rb') as f:\n",
        "            # Verify that file is an image \n",
        "            try:\n",
        "                if '.html' not in path: # hacky fix to catch htmls\n",
        "                    img = Image.open(io.BytesIO(f.read()))\n",
        "                    img.verify() \n",
        "                else:\n",
        "                    raise NameError\n",
        "            # If file isn't an image, delete it\n",
        "            except (IOError, SyntaxError, NameError) as e:\n",
        "                print('Bad file:', path)\n",
        "                if '(' in path: # rm doesn't work for files with parenthesis in name, need to manually remove\n",
        "                    print(\"Manually remove from Google Drive: {}\".format(path)) \n",
        "                else:\n",
        "                    !rm $path "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG12l9JVnpmU"
      },
      "source": [
        "### 5) Aggregate classes into good (4 & 5) and bad (1 & 2) because models did not learn classes 1-5 with any hyperparameter combinations\n",
        "---\n",
        "*Afternote: Users were more conflicted on what makes an image \"good\" than what makes it \"bad.\" Because models learn patterns from the training data, this resulted in high accuracy for predicting \"bad\" images (classes 1 & 2), but mixed accuracy for predicting \"ok\" or \"good\" (classes 4 & 5)*   \n",
        "Models were retrained using aggregated \"bad\" and \"good\" classes with improved success."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ag4nN14noK2"
      },
      "source": [
        "# Move text file to image_data/bundles\n",
        "cwd = wd + '/pre-processing/images/'\n",
        "%cd $cwd\n",
        "\n",
        "# Make aggregated 'bad' images folder (combined classes 1 and 2)\n",
        "!mkdir -p agg/bad\n",
        "!cp 1/* agg/bad/\n",
        "!cp 2/* agg/bad/\n",
        "print(\"Number of images in new aggregated 'bad' folder: \")\n",
        "!ls agg/bad | wc -l\n",
        "\n",
        "# Make aggregated 'good' images folder (combined classes 4 and 5)\n",
        "!mkdir -p agg/good \n",
        "!cp 4/* agg/good/\n",
        "!cp 5/* agg/good/\n",
        "print(\"Number of images in new aggregated 'good' folder: \")\n",
        "!ls agg/good | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}