{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkopeMkGBNOqnp1XhjIhXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/rating/rating_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGFNOrAs0pMf"
      },
      "source": [
        "# Pre-process Image Rating Classifier Training Images\n",
        "---\n",
        "*Last Updated 4 Oct 2022*   \n",
        "Follow steps below to make training and testing datasets using the [EOL user generated image ratings file](https://editors.eol.org/other_files/EOL_v2_files/image_ratings.txt.zip). 7K images per rating class (1 - 5) are downloaded to Google Drive for use training models in [rating_train.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/rating/rating_train.ipynb).     \n",
        "\n",
        "Notes:   \n",
        "* Run code blocks by pressing play button in brackets on left\n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change parameters using form fields on right (find details at corresponding lines of code by searching '#@param')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F53iiacTFVz7"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeNoVQDN0I1q"
      },
      "source": [
        "#@title Choose where to save results & set up directory structure\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "print(\"Saving results \", save)\n",
        "\n",
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Type in the path to your working directory in form field to right\n",
        "import os\n",
        "basewd = \"/content/drive/MyDrive/train/tf2\" #@param [\"/content/drive/MyDrive/train/tf2\"] {allow-input: true}\n",
        "if not os.path.exists(basewd):\n",
        "    os.makedirs(basewd)\n",
        "\n",
        "# Enter image classes of interest in form field\n",
        "filters = [\"1\", \"2\", \"3\", \"4\", \"5\"] #@param [\"[\\\"1\\\", \\\"2\\\", \\\"3\\\", \\\"4\\\", \\\"5\\\"]\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# Folder where pre-processing results will be saved\n",
        "preprocessing_folder = \"pre-processing\" #@param [\"pre-processing\"] {allow-input: true}\n",
        "cwd = basewd + '/' + preprocessing_folder\n",
        "print(\"\\nWorking directory set to: \\n\", cwd)\n",
        "\n",
        "# Folder where image metadata will be saved\n",
        "data_folder = \"image_data\" #@param [\"image_data\"] {allow-input: true}\n",
        "data_wd = cwd + '/' + data_folder\n",
        "if not os.path.exists(data_wd):\n",
        "    os.makedirs(data_wd)\n",
        "print(\"\\nImage metadata directory set to: \\n\", data_wd)\n",
        "\n",
        "# Folder where train/test images will be saved\n",
        "train_folder = \"images\" #@param [\"images\"] {allow-input: true}\n",
        "train_wd = cwd + '/' + train_folder\n",
        "if not os.path.exists(train_wd):\n",
        "    os.makedirs(train_wd)\n",
        "print(\"\\nTraining images directory set to: \\n\", train_wd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxIUuGkDa__r"
      },
      "source": [
        "# For importing/exporting files, working with arrays, etc\n",
        "import os\n",
        "from os import listdir\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For working with images\n",
        "from PIL import Image\n",
        "Image.MAX_IMAGE_PIXELS = 95000000 # To suppress errors from Pillow about decompression bombs\n",
        "import io\n",
        "\n",
        "# For downloading images\n",
        "!apt-get install aria2\n",
        "\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# EOL image data bundle\n",
        "bundle = \"https://editors.eol.org/other_files/EOL_v2_files/image_ratings.txt.zip\" #@param [\"https://editors.eol.org/other_files/EOL_v2_files/image_ratings.txt.zip\"] {allow-input: true}\n",
        "\n",
        "# Read in data file exported from \"Combine output files A-D\" block above\n",
        "def read_datafile(fpath, sep=\"\\t\", header=0, disp_head=True):\n",
        "    hdr = {\n",
        "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "        'Accept-Encoding': 'none',\n",
        "        'Accept-Language': 'en-US,en;q=0.8',\n",
        "        'Connection': 'keep-alive'\n",
        "        }\n",
        "    try:\n",
        "        df = pd.read_csv(fpath, sep=sep, header=header, storage_options=hdr)\n",
        "        if disp_head:\n",
        "          print(\"Data header: \\n\", df.head())\n",
        "    except FileNotFoundError as e:\n",
        "        raise Exception(\"File not found: Enter the path to your file in form field and re-run\").with_traceback(e.__traceback__)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Suppress pandas warning about writing over a copy of data\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Optional = Un-comment out if want to view individual URLs, increase output length so full URL visible\n",
        "pd.set_option('display.max_colwidth', 1000) # Print full urls for inspection\n",
        "\n",
        "# Filter by rating of interest\n",
        "def filter_by_rating(df, filter=filter, disp_head=False):\n",
        "    rating = df.loc[round(df[\"overall_rating\"])==int(filter)]\n",
        "    rating = rating[\"obj_url\"].copy()\n",
        "    \n",
        "    if disp_head:\n",
        "          print(\"Rating = {}}:\\n {}\".format(filter, rating.head()))\n",
        "    print(\"\\n Number of available ratings for training/testing class {}: \\n {}\".format(filter, len(rating)))\n",
        "\n",
        "    return rating\n",
        "\n",
        "# Define start and stop indices in EOL bundle for running inference   \n",
        "def set_start_stop(run):\n",
        "    # To test with a tiny subset, use 5 random bundle images\n",
        "    if \"tiny subset\" in run:\n",
        "        start=np.random.choice(a=1000, size=1)[0]\n",
        "        stop=start+5\n",
        "    # To run for all images\n",
        "    else:\n",
        "        start=None\n",
        "        stop=None\n",
        "    \n",
        "    return start, stop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZlJ7Rjaub3O"
      },
      "source": [
        "## Inspect EOL User Generated Image Ratings File\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E9mmcA-Dekh"
      },
      "source": [
        "#@title Filter EOL image quality ratings by class (1-bad to 5-good)\n",
        "\n",
        "# Download EOL user generated rating file to temporary runtime location\n",
        "!wget --user-agent=\"Mozilla\" $bundle\n",
        "\n",
        "# Unzip cropping file to your working directory\n",
        "!unzip /content/image_ratings.txt.zip -d $cwd\n",
        "\n",
        "# Read in user-generated image rating file\n",
        "%cd $cwd\n",
        "fpath = os.path.splitext(os.path.basename(bundle))[0]\n",
        "df = pd.read_csv(fpath, sep='\\t', header=0, lineterminator='\\n', encoding='latin1')\n",
        "\n",
        "# Make train and test datasets for each rating class 1-5\n",
        "for filter in filters:\n",
        "    # Filter by rating of interest \n",
        "    filtered = filter_by_rating(df, filter, disp_head=False)\n",
        "\n",
        "    # Make folder for rating class\n",
        "    dir = 'images/' + filter\n",
        "    os.makedirs(dir)\n",
        "\n",
        "    # Export filtered dataset as txt to folder of interest\n",
        "    outfpath = dir + '/' + filter + '_download.txt'\n",
        "    filtered.to_csv(outfpath, sep='\\n', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUzJS0BsYh0f"
      },
      "source": [
        "## Build 7k image bundles for rating classes 1-5\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt7zEGcKZF97"
      },
      "source": [
        "# Make 7k image bundles for rating classes 1-4 \n",
        "# Rating class 5 only has 1200 images, so it is built differently in next code block\n",
        "%cd $train_wd\n",
        "\n",
        "# Future image directories for training classifier\n",
        "ratings = filters[:4]\n",
        "all_filenames = [rating + '/' + rating + '_download.txt' for rating in ratings] # Image rating filenames\n",
        "filenames_7k = [rating + '/' + rating + '_download_7k.txt' for rating in ratings] # Future 7K image bundle filenames\n",
        "\n",
        "# Randomly pick 7,000 images from each rating class and write to csv\n",
        "for num, filename in enumerate(all_filenames):\n",
        "    df = pd.read_table(filename, sep='\\n')\n",
        "    bundle = df.sample(7000)\n",
        "    fn = str(filenames_7k[num])\n",
        "    print(\"7k image bundle filename for rating {}: {}\\n{}\\n\".format((num+1), fn, bundle.head()))\n",
        "    bundle.to_csv(fn, sep='\\n', index=False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsoet2PW6G-5"
      },
      "source": [
        "# Make 7k bundle for Rating = 5 dataset\n",
        "# Different because Rating=5 had only 1200 images total\n",
        "\n",
        "# Add images to Rating = 5 dataset from EOL User Exemplar File\n",
        "# Download EOL user generated exemplar file to temporary runtime location\n",
        "!wget --user-agent=\"Mozilla\" https://editors.eol.org/other_files/EOL_v2_files/images_selected_as_exemplar.txt.zip\n",
        "\n",
        "# Unzip cropping file to your working directory\n",
        "!unzip images_selected_as_exemplar.txt.zip -d ./5\n",
        "\n",
        "# Read in user-generated image exemplars file\n",
        "fpath = '5/' + 'images_selected_as_exemplar.txt'\n",
        "df = pd.read_csv(fpath, sep='\\t', header=0, lineterminator='\\n', encoding='latin1')\n",
        "\n",
        "# Include all duplicates from exemplar file \n",
        "# (these ones may be better or more controversial, see email from JH 28 Oct 2020)\n",
        "idx = df.index[df.duplicated(['object_url'])].tolist()\n",
        "dups = df.loc[idx]\n",
        "dups = pd.DataFrame(dups[\"object_url\"])\n",
        "\n",
        "# Add 4k random images from exemplar file\n",
        "unq = df.drop(idx, errors='ignore')\n",
        "unq = unq.sample(4000)\n",
        "unq = pd.DataFrame(unq[\"object_url\"])\n",
        "\n",
        "# Read in Rating = 5 images\n",
        "df1 = pd.read_table('5/5_download.txt', sep='\\n')\n",
        "df1.columns = unq.columns\n",
        "\n",
        "# Make combined 7k bundle from Exemplar duplicates & random images, + Rating = 5 images\n",
        "comb = pd.concat([df1,unq,dups], ignore_index=True)\n",
        "print(\"Rating = 5:\\n {}\".format(comb.head()))\n",
        "print(\"\\n Number of available ratings for training/testing class 5: \\n {}\".format(len(comb)))\n",
        "comb.to_csv('5/5_download_7k.txt', sep='\\n', index=False, header=False)\n",
        "filenames_7k.append('5/5_download_7k.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOJEcY_BYYjl"
      },
      "source": [
        "## Download images to Google Drive\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQBcC0zzdrVy",
        "cellView": "code"
      },
      "source": [
        "#@title Download images for each class\n",
        "%cd $train_wd\n",
        "\n",
        "# Test pipeline with a smaller subset than 5k images?\n",
        "run = \"test with tiny subset\" #@param [\"test with tiny subset\", \"for all images\"]\n",
        "print(\"Run: \", run)\n",
        "\n",
        "# Download images, augment them, and save to Google Drive\n",
        "print(\"\\nDownloading training images for each class\")\n",
        "start, stop = set_start_stop(run)\n",
        "\n",
        "# Loop through image data files for each rating class to download images into their respective folders\n",
        "for imclass, fn in enumerate(filenames_7k, start=1):\n",
        "\n",
        "    # CWD to image class folder\n",
        "    impath = train_wd + '/' + str(imclass) + '/'\n",
        "    %cd $impath\n",
        "\n",
        "    # Take tiny subset or all images from bundle\n",
        "    fn = os.path.basename(fn)\n",
        "    df = pd.read_csv(fn, sep='\\n', header=None)\n",
        "    df = df.iloc[start:stop]\n",
        "    df.to_csv(fn, sep='\\n', header=False, index=False)\n",
        "\n",
        "    # Download images\n",
        "    !aria2c -x 16 -s 1 -i $fn\n",
        "\n",
        "    # Check how many images downloaded\n",
        "    print(\"Number of images downloaded to Google Drive: \")\n",
        "    !ls . | wc -l\n",
        "\n",
        "    # Move text file to image_data/bundles\n",
        "    %cd $cwd\n",
        "    impath = impath + \"*.txt\"\n",
        "    !mv $impath image_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uELne7tJrEDZ"
      },
      "source": [
        "## Delete all downloaded non-image files\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTPAxR5V-wTm"
      },
      "source": [
        "# Loop through image rating class folders 1 - 5\n",
        "for imclass in range(1,6):\n",
        "    # CWD to image class folder\n",
        "    impath = train_wd + '/' + str(imclass) + '/'\n",
        "    %cd $impath\n",
        "    # Inspect each file in folder\n",
        "    for path in listdir('./'):\n",
        "        with open(path, 'rb') as f:\n",
        "            # Verify that file is an image \n",
        "            try:\n",
        "                if '.html' not in path: # hacky fix to catch htmls\n",
        "                    img = Image.open(io.BytesIO(f.read()))\n",
        "                    img.verify() \n",
        "                else:\n",
        "                    raise NameError\n",
        "            # If file isn't an image, delete it\n",
        "            except (IOError, SyntaxError, NameError) as e:\n",
        "                print('Bad file:', path)\n",
        "                if '(' in path: # rm doesn't work for files with parenthesis in name, need to manually remove\n",
        "                    print(\"Manually remove from Google Drive: {}\".format(path)) \n",
        "                else:\n",
        "                    !rm $path "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG12l9JVnpmU"
      },
      "source": [
        "## Aggregate classes into good (4 & 5) and bad (1 & 2) because models did not learn classes 1-5 with any hyperparameter combinations\n",
        "---\n",
        "*Afternote: Users were more conflicted on what makes an image \"good\" than what makes it \"bad.\" Because models learn patterns from the training data, this resulted in high accuracy for predicting \"bad\" images (classes 1 & 2), but mixed accuracy for predicting \"ok\" or \"good\" (classes 4 & 5)*   \n",
        "Models were retrained using aggregated \"bad\" and \"good\" classes with improved success."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ag4nN14noK2"
      },
      "source": [
        "# Move text file to image_data/bundles\n",
        "%cd $train_wd\n",
        "\n",
        "# Make aggregated 'bad' images folder (combined classes 1 and 2)\n",
        "!mkdir -p agg/bad\n",
        "!cp 1/* agg/bad/\n",
        "!cp 2/* agg/bad/\n",
        "print(\"Number of images in new aggregated 'bad' folder: \")\n",
        "!ls agg/bad | wc -l\n",
        "\n",
        "# Make aggregated 'good' images folder (combined classes 4 and 5)\n",
        "!mkdir -p agg/good \n",
        "!cp 4/* agg/good/\n",
        "!cp 5/* agg/good/\n",
        "print(\"Number of images in new aggregated 'good' folder: \")\n",
        "!ls agg/good | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}