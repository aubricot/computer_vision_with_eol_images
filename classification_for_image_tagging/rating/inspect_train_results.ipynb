{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inspect_train_results.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMDJxDvGIPH6sNkQEsF42Y/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/rating/inspect_train_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TcYNLBrWC0C"
      },
      "source": [
        "# Determine confidence threshold for Image Rating Classification Models \n",
        "---\n",
        "*Last Updated 26 October 2021*   \n",
        "Choose which trained model and confidence threshold values to use for classifying EOL image ratings. Threshold values should be chosen that maximize coverage and minimize error.\n",
        "\n",
        "First, choose the best models trained in [rating_train.ipynb](https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/classification_for_image_tagging/rating/rating_train.ipynb). Then, run this notebook. \n",
        "\n",
        "Run 500 images per class (Image ratings 1-5) through the best models chosen in rating_train.ipynb for validation of model performance. Plot histograms of true and false predictions per class at binned confidence intervals to find the best performance by class and confidence threshold. (This is helpful because all models may not learn classes equally well).\n",
        "\n",
        "***Models were trained in Python 2 and TF 1 in December 2020: MobileNet SSD v2 (Run 18, trained on 'good' and 'bad' classes) was trained for 12 hours to 10 epochs with Batch Size=16, Lr=0.001, Dropout=0.2. Inception v3 was trained for 12 hours to 10 epochs with Batch Size=32 Lr=0.001, Dropout=0 (Run 20, trained on 'good' and 'bad' classes). Inception v3 was trained for 4 hours to 15 epochs with Batch Size=64, Lr=0.1, Dropout=0 (Run 6, trained on numerical rating classes 1-5).***\n",
        "\n",
        "Notes:   \n",
        "* Change parameters using form fields on right (/where you see 'TO DO' in code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYW4W2aqdnTN"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k81-h_UV_ny"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AGFM4fSWhbT"
      },
      "source": [
        "# For working with data\n",
        "import itertools\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For downloading and displaying images\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "%matplotlib inline\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For image classification and training\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# To read in EOL formatted data files\n",
        "def read_datafile(fpath, sep=\"\\t\", header=0, disp_head=True, lineterminator='\\n', encoding='latin1'):\n",
        "    \"\"\"\n",
        "    Defaults to tab-separated data files with header in row 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(fpath, sep=sep, header=header, lineterminator=lineterminator, encoding=encoding)\n",
        "        if disp_head:\n",
        "          print(\"Data header: \\n\", df.head())\n",
        "    except FileNotFoundError as e:\n",
        "        raise Exception(\"File not found: Enter the path to your file in form field and re-run\").with_traceback(e.__traceback__)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# List filenames of all images used for training/testing models\n",
        "def list_train_images(imclasses):\n",
        "    # Get image class bundle filenames\n",
        "    all_filenames = ['image_data/' + imclass + '_download_7k.txt' for imclass in imclasses] \n",
        "    print('Image class bundles used for training/testing models: \\n', all_filenames)\n",
        "    # Make combined list all image ratings from bundles\n",
        "    used_images = []\n",
        "    for fn in all_filenames: \n",
        "        df = pd.read_csv(fn, index_col=None, header=1, sep='\\n')\n",
        "        df.columns = ['link']\n",
        "        used_images.append(df)\n",
        "    used_images = pd.concat(used_images, axis=0, ignore_index=True)\n",
        "    print('No. image ratings used for training/testing: {}'.format(len(used_images),\n",
        "                                                                   used_images.head()))\n",
        "\n",
        "    return used_images\n",
        "\n",
        "# Remove all images used for training/testing from EOL bundle\n",
        "def remove_used_images(df, used_images, dataset):\n",
        "    print(\"Total image ratings available for {}: {}\".format(dataset, len(df)))\n",
        "    if 'object_url' in df:\n",
        "        df.rename(columns={'object_url':'obj_url'}, inplace=True)\n",
        "    condition = df['obj_url'].isin(used_images['link'])\n",
        "    df.drop(df[condition].index, inplace = True)\n",
        "    unused_images = df.copy()\n",
        "    print(\"Total un-used image ratings available for {}: {}\".format(dataset, len(unused_images)))\n",
        "\n",
        "    return unused_images\n",
        "\n",
        "# Make master unused image dataset for ratings and exemplars\n",
        "def make_master_unused_df(ratings, exemplars):\n",
        "    # Reformat image ratings to match exemplars\n",
        "    df1 = unused_ratings[[\"obj_with_overall_rating\", \"obj_url\", \"overall_rating\", \"ancestry\"]].copy()\n",
        "    df1.rename(columns={\"obj_with_overall_rating\": \"obj_id\"}, inplace=True)\n",
        "    print(df1.head())\n",
        "    # Reformat image exemplars to match ratings\n",
        "    df2 = unused_exemplars[[\"target_id\", \"obj_url\", \"ancestry\"]].copy()\n",
        "    df2.rename(columns={\"object_url\":\"obj_url\", \"target_id\": \"obj_id\"}, inplace=True)\n",
        "    df2[\"overall_rating\"] = 5\n",
        "    # Merge ratings and exemplars\n",
        "    unused_images = pd.concat([df1, df2])\n",
        "    print(\"Master un-used image ratings for validation (ratings + exemplars): {}\\n{}\".format(len(unused_images), unused_images))\n",
        "\n",
        "    return unused_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfFgkKi3nILR"
      },
      "source": [
        "## Build validation dataset (Only run once)\n",
        "---\n",
        "Build dataset of image ratings for images not previously seen by models.  \n",
        "Removes image ratings found in EOL user generated rating and exemplar files that were used in 7k training/testing datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpIZ_UE6RCo-"
      },
      "source": [
        "# Find images with ratings that were not used for training or testing models \n",
        "\n",
        "# TO DO: Type in the path to your working directory in form field to right\n",
        "wd = \"/content/drive/MyDrive/train/\" #@param {type:\"string\"}\n",
        "\n",
        "# Set current working directory\n",
        "cwd = wd + 'pre-processing/'\n",
        "%cd $cwd\n",
        "\n",
        "# Get list of images used for 7k training/testing datasets\n",
        "# TO DO: Make list of image classes used for training\n",
        "imclasses = ['1', '2', '3', '4', '5'] #@param\n",
        "used_images = list_train_images(imclasses)\n",
        "\n",
        "# Remove images already used for training/testing from EOL rating dataset\n",
        "df = read_datafile(\"image_data/image_ratings.txt\", disp_head=False)\n",
        "unused_ratings = remove_used_images(df, used_images, \"Ratings\")\n",
        "unused_ratings.to_csv('image_data/unused_image_ratings_foreval.txt', sep=\"\\t\", index=False, header=True)\n",
        "\n",
        "# Remove images already used for training/testing from EOL exemplar dataset (used to supplment rating=5)\n",
        "df = read_datafile(\"image_data/images_selected_as_exemplar.txt\", disp_head=False)\n",
        "unused_exemplars = remove_used_images(df, used_images, \"Exemplars\")\n",
        "unused_exemplars.to_csv('image_data/unused_image_exemplars_foreval.txt', sep=\"\\t\", index=False, header=True)\n",
        "\n",
        "# Make master unused images dataset for ratings and exemplars\n",
        "unused_images = make_master_unused_df(unused_ratings, unused_exemplars)\n",
        "unused_images.to_csv('image_data/unused_images_foreval_master.txt', sep=\"\\t\", index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_uxbhWyHag"
      },
      "source": [
        "## Run images through for classification and validating predictions (Run 1x for each trained model)   \n",
        "---\n",
        "Selected models from rating_train.ipynb   \n",
        "* Run 20: Inception v3 (trained on 'good' and 'bad' classes)\n",
        "* Run 18: Mobilenet SSD v2 (trained on 'good' and 'bad' classes)\n",
        "* Run 06: Inception v3 (trained on numerical rating classes 1-5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efIHQCtAAmYg"
      },
      "source": [
        "# Set parameters \n",
        "\n",
        "# TO DO: Choose training attempt number to inspect results for\n",
        "TRAIN_SESS_NUM = \"18\" #@param [\"20\", \"18\", \"06\"] {allow-input: true}\n",
        "\n",
        "# Directory to saved models\n",
        "saved_models_dir = wd + 'saved_models/'\n",
        "\n",
        "# Set current working directory\n",
        "cwd = wd + 'inspect_resul/'\n",
        "%cd $cwd\n",
        "\n",
        "# Suppress pandas setting with copy warning\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# Define start and stop indices in EOL bundle for running inference   \n",
        "def set_start_stop(df):\n",
        "    # To test with a tiny subset, use 5 random bundle images\n",
        "    N = len(df)\n",
        "    if test_with_tiny_subset:\n",
        "        start=np.random.choice(a=N, size=1)[0]\n",
        "        stop=start+5\n",
        "    # To run for larger set, use 500 random images\n",
        "    else : \n",
        "        start=np.random.choice(a=N, size=1)[0]\n",
        "        stop=start+500\n",
        "    print(\"Running inference on images\")\n",
        "    \n",
        "    return start, stop\n",
        "\n",
        "# Load saved model from directory\n",
        "def load_saved_model(saved_models_dir, TRAIN_SESS_NUM, module_selection):\n",
        "    # Load trained model from path\n",
        "    saved_model_path = saved_models_dir + TRAIN_SESS_NUM\n",
        "    model = tf.keras.models.load_model(saved_model_path)\n",
        "    # Get name and image size for model type\n",
        "    handle_base, pixels = module_selection\n",
        "\n",
        "    return model, pixels, handle_base\n",
        "\n",
        "# Get info about model based on training attempt number\n",
        "def get_model_info(TRAIN_SESS_NUM):\n",
        "    # Session 18\n",
        "    if int(TRAIN_SESS_NUM) == 18:\n",
        "        module_selection =(\"mobilenet_v2_1.0_224\", 224)\n",
        "        dataset_labels = ['bad', 'good'] # Classes aggregated after attempt 7: 1/2 -> bad, 4/5 -> good\n",
        "    # Session 20\n",
        "    elif int(TRAIN_SESS_NUM) == 20:\n",
        "        module_selection = (\"inception_v3\", 299)\n",
        "        dataset_labels = ['bad', 'good'] # Classes aggregated after attempt 7: 1/2 -> bad, 4/5 -> good\n",
        "    # Session 6\n",
        "    elif int(TRAIN_SESS_NUM) == 6:\n",
        "        module_selection = (\"inception_v3\", 299)\n",
        "        dataset_labels = ['1', '2', '3', '4', '5'] # Before aggregating classes\n",
        "\n",
        "    return module_selection, dataset_labels\n",
        "\n",
        "# Set filename for saving classification results\n",
        "def set_outpath(true_imclass):\n",
        "    outpath = wd + 'inspect_resul/ratings_' + TRAIN_SESS_NUM + '_' + true_imclass + '.csv'\n",
        "    print(\"Saving results to: \\n\", outpath)\n",
        "\n",
        "    return outpath\n",
        "\n",
        "# Load in image from URL\n",
        "# Modified from https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/saved_model.ipynb#scrollTo=JhVecdzJTsKE\n",
        "def image_from_url(url, fn):\n",
        "    file = tf.keras.utils.get_file(fn, url) # Filename doesn't matter\n",
        "    disp_img = tf.keras.preprocessing.image.load_img(file)\n",
        "    image = tf.keras.preprocessing.image.load_img(file, target_size=[pixels, pixels])\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    image = tf.keras.applications.mobilenet_v2.preprocess_input(\n",
        "        image[tf.newaxis,...])\n",
        "\n",
        "    return image, disp_img\n",
        "\n",
        "# Get info from predictions to display on images\n",
        "def get_predict_info(predictions, url, i, stop, start):\n",
        "    # Get info from predictions\n",
        "    label_num = np.argmax(predictions[0], axis=-1)\n",
        "    conf = predictions[0][label_num]\n",
        "    im_class = dataset_labels[label_num]\n",
        "    # Display progress message after each image\n",
        "    print(\"Completed for {}, {} of {} files\".format(url, i, format(stop-start, '.0f')))\n",
        "    \n",
        "    return label_num, conf, im_class\n",
        "\n",
        "# Record results for confidence thresholds\n",
        "# Make placeholder lists to fill for each class\n",
        "def make_placeholders():\n",
        "    filenames = []\n",
        "    confidences = []\n",
        "    true_imclasses = []\n",
        "    det_imclasses = []\n",
        "    ancestries = []\n",
        "\n",
        "    return filenames, confidences, true_imclasses, det_imclasses, ancestries\n",
        "    \n",
        "# Add values for each image to placeholder list\n",
        "def record_results(fn, conf, true_imclass, det_imclass, ancestry):\n",
        "    filenames.append(fn)\n",
        "    confidences.append(conf)\n",
        "    true_imclasses.append(true_imclass)\n",
        "    det_imclasses.append(str(det_imclass))\n",
        "    ancestries.append(ancestry)\n",
        "    results = [filenames, confidences, true_imclasses, det_imclasses, ancestries]\n",
        "\n",
        "    return results\n",
        "\n",
        "# Export results\n",
        "def export_results(results):\n",
        "    results = pd.DataFrame(results)\n",
        "    results = results.transpose()\n",
        "    results.to_csv(outpath, index=False, header=(\"filename\", \"confidence\", \n",
        "                                                     \"true_id\", \"det_id\", \"ancestry\"))\n",
        "    print(\"Classification predictions for image class {}: {}\".format(\n",
        "          true_imclass, results.head()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS95n46BnbJ5"
      },
      "source": [
        "# Run inference\n",
        "\n",
        "# Test with tiny subset (5 images)?\n",
        "# TO DO: If yes, check test_with_tiny_subset box\n",
        "test_with_tiny_subset = True #@param {type: \"boolean\"}\n",
        "\n",
        "# Load saved model\n",
        "module_selection, dataset_labels = get_model_info(TRAIN_SESS_NUM)\n",
        "model, pixels, handle_base = load_saved_model(saved_models_dir, TRAIN_SESS_NUM, module_selection)\n",
        "\n",
        "# Run inference for each image class to compare known versus predicted ratings\n",
        "true_imclasses = ['1', '2', '3', '4', '5']\n",
        "for true_imclass in true_imclasses:\n",
        "    # Set filename for saving classification results\n",
        "    outpath = set_outpath(true_imclass)\n",
        "\n",
        "    # Make placeholder lists to record values for each image\n",
        "    filenames, confidences, true_imclasses, det_imclasses, ancestries = make_placeholders()\n",
        "\n",
        "    # Load subset of in validation images df for each image class\n",
        "    df = unused_images.copy()\n",
        "    df = df[df.overall_rating==int(true_imclass)]\n",
        "\n",
        "    # Run 500 random EOL bundle images through trained model\n",
        "    start, stop = set_start_stop(df)\n",
        "    for i, row in df.iloc[start:stop].iterrows():\n",
        "        try:\n",
        "            # Read in image from url\n",
        "            url = df['obj_url'][i]\n",
        "            fn = str(i) + '.jpg'\n",
        "            img, disp_img = image_from_url(url, fn)\n",
        "            ancestry = df['ancestry'][i]\n",
        "        \n",
        "            # Image classification\n",
        "            start_time = time.time() # Record inference time\n",
        "            predictions = model.predict(img, batch_size=1)\n",
        "            label_num, conf, det_imclass = get_predict_info(predictions, url, i, stop, start)\n",
        "            end_time = time.time()\n",
        "            print(\"Inference time: {} sec\".format(format(end_time-start_time, '.2f')))\n",
        "\n",
        "            # Record results in placeholder lists to inspect results in next step\n",
        "            results = record_results(fn, conf, true_imclass, str(det_imclass), ancestry)\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Combine to df and export results\n",
        "    export_results(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnUCpn8sWVzi"
      },
      "source": [
        "# Combine model outputs for image rating classes 1-5\n",
        "\n",
        "# Combine prediction files created in codeblock above\n",
        "true_imclasses = ['1', '2', '3', '4', '5']\n",
        "base = 'ratings_' + TRAIN_SESS_NUM + '_'\n",
        "all_filenames = [base + true_imclass + '.csv' for true_imclass in true_imclasses]\n",
        "all_predictions = pd.concat([pd.read_csv(f, sep=',', header=0, na_filter = False) for f in all_filenames])\n",
        "print(\"Model predictions for Training Attempt {}, {}:\".format(TRAIN_SESS_NUM, handle_base))\n",
        "print(\"No. Images: {}\\n{}\".format(len(all_predictions), all_predictions[['filename', 'true_id', 'det_id']].head()))\n",
        "\n",
        "# Aggregate numerical \"true_id\" classes into 'bad' and 'good'\n",
        "# TO DO: Enter class names\n",
        "c0 = \"bad\" #@param {type:\"string\"}\n",
        "c1 = \"good\" #@param {type:\"string\"}\n",
        "imclasses = [c0, c1]\n",
        "\n",
        "# All predictions of 1 or 2 become 'bad'\n",
        "all_predictions.true_id[(all_predictions.true_id==1) | (all_predictions.true_id==2)] = c0\n",
        "all_predictions.det_id[(all_predictions.det_id==1) | (all_predictions.det_id==2)] = c0\n",
        "\n",
        "# All predictions of 4 or 5 become 'good'\n",
        "all_predictions.true_id[(all_predictions.true_id==4) | (all_predictions.true_id==5)] = c1\n",
        "all_predictions.det_id[(all_predictions.det_id==4) | (all_predictions.det_id==5)] = c1\n",
        "\n",
        "# Remove all predictions of 3\n",
        "all_predictions = all_predictions[all_predictions.det_id!=3]\n",
        "all_predictions = all_predictions[all_predictions.true_id!=3]\n",
        "\n",
        "print(\"Numeric image ratings successfully aggregated into {} (1-2) and {} (4-5):\\n{}\".format(c0, c1, all_predictions[['filename', 'true_id', 'det_id']].head()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL_56etnd0Rd"
      },
      "source": [
        "## Plot prediction error and confidence for each class (Run 1x for each trained model)\n",
        "---   \n",
        "Use these histograms to find a confidence threshold value to optimize dataset coverage and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81N24FjAJqQV"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "# Valide predictions by image class (and optionally, by: taxon)\n",
        "def validate_predictions(df, inspect_by_taxon):\n",
        "    # If inspecting for taxon-specific images only\n",
        "    taxon = None\n",
        "    if inspect_by_taxon:\n",
        "        # TO DO: Type in the taxon you'd like to inspect results for using form field to right\n",
        "        taxon = \"\" #@param {type:\"string\"}\n",
        "        df = df.loc[df.ancestry.str.contains(taxon, case=False, na=False)]\n",
        "        print(\"Inspecting results for {}:\\n{}\".format(taxon, df.head()))\n",
        "    \n",
        "    # Validate predictions\n",
        "    # Check where true ratings and model-determined classes match\n",
        "    df['det'] = (df['true_id'] == df['det_id'])\n",
        "    tru = df.loc[df.det, :] # True ID\n",
        "    fal = df.loc[~df.det, :] # False ID\n",
        "    \n",
        "    # Inspect by image class and confidence values\n",
        "    # Check how many true/false predictions are at each confidence value\n",
        "    # Class 0 - 'Bad'\n",
        "    c0t = tru.loc[tru['true_id'] == c0, :] # True dets\n",
        "    c0f = fal.loc[fal['true_id'] == c0, :] # False dets\n",
        "    # Class 1 - 'Good'\n",
        "    c1t = tru.loc[tru['true_id'] == c1, :] # True dets\n",
        "    c1f = fal.loc[fal['true_id'] == c1, :] # False dets\n",
        "\n",
        "    return tru, fal, c0t, c0f, c1t, c1f, taxon\n",
        "\n",
        "# Plot results by image class\n",
        "def plot_predict_x_conf(tru, fal, c0t, c0f, c1t, c1f, imclasses=imclasses, thresh=thresh):\n",
        "    # Plot parameters to make 1 subplot per image class\n",
        "    kwargs = dict(alpha=0.5, bins=15)\n",
        "    fig, axes = plt.subplots(len(imclasses), figsize=(10, 10), constrained_layout=True)\n",
        "    fig.suptitle('Prediction Confidence by Class\\n Overall Accuracy: {}'.format(\n",
        "                  format((len(tru)/(len(tru)+len(fal))),'.2f')))\n",
        "    \n",
        "    # Make subplots\n",
        "\n",
        "    # Class 0 - 'Bad'\n",
        "    c0 = df.iloc[:, :N] # Pull N items for c0\n",
        "    # True predictions\n",
        "    axes[0].hist(c0t['confidence'], color='y', label='True Det', **kwargs)\n",
        "    # False predictions\n",
        "    axes[0].hist(c0f['confidence'], color='r', label='False Det', **kwargs)\n",
        "    axes[0].set_title(\"{} (n={} images)\\n Accuracy: {}\".format(imclasses[0], \n",
        "                      len(c0t+c0f), format((len(c0t)/(len(c0t)+len(c0f))),'.2f')))\n",
        "    axes[0].legend();\n",
        "\n",
        "    # Class 1 - 'Good'\n",
        "    c1 = df.iloc[:, N:2*N] # Pull N items for c1\n",
        "    # True predictions\n",
        "    axes[1].hist(c1t['confidence'], color='y', label='True Det', **kwargs)\n",
        "    # False predictions\n",
        "    axes[1].hist(c1f['confidence'], color='r', label='False Det', **kwargs)\n",
        "    axes[1].set_title(\"{} (n={} images)\\n Accuracy: {}\".format(imclasses[1], \n",
        "                      len(c1t+c1f), format((len(c1t)/(len(c1t)+len(c1f))),'.2f')))\n",
        "    axes[1].legend();\n",
        "\n",
        "    # Add Y-axis labels\n",
        "    for ax in fig.get_axes():\n",
        "        ax.set(ylabel='Freq (# imgs)')\n",
        "        if thresh:\n",
        "            ax.axvline(thresh, color='k', linestyle='dashed', linewidth=1)\n",
        "\n",
        "    return fig\n",
        "\n",
        "# To save the figure\n",
        "def save_figure(fig, TRAIN_SESS_NUM=TRAIN_SESS_NUM, taxon=taxon, handle_base=handle_base):\n",
        "    # Make filename\n",
        "    if taxon: # If for a specific taxon\n",
        "        if 'plant' in taxon:\n",
        "            handle_base = handle_base + '_plantae'\n",
        "        elif 'anim' in taxon:\n",
        "            handle_base = handle_base + '_animalia'\n",
        "\n",
        "    figname = TRAIN_SESS_NUM + '_' + handle_base + '.png'\n",
        "    fig.savefig(figname)\n",
        "    print(\"Histograms saved to \", figname)\n",
        "\n",
        "    return figname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10GhFabiCj3c"
      },
      "source": [
        "# Load combined prediction results from above\n",
        "df = all_predictions.copy()\n",
        "\n",
        "# Optional: Inspect predictions for taxon-specific images only?\n",
        "# TO DO: If \"yes,\" check box\n",
        "inspect_by_taxon = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Optional: Draw threshold value to help choose optimal balance b/w maximizing useful data and minimizing error\n",
        "# TO DO: Set threshold value\n",
        "thresh = 0 #@param {type:\"number\"}\n",
        "\n",
        "# Valide predictions by image class (and optionally, by: taxon)\n",
        "tru, fal, c0t, c0f, c1t, c1f, taxon = validate_predictions(df, inspect_by_taxon)\n",
        "\n",
        "# Plot results by image class\n",
        "fig = plot_predict_x_conf(tru, fal, c0t, c0f, c1t, c1f)\n",
        "\n",
        "# Export histograms\n",
        "figname = save_figure(fig)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}