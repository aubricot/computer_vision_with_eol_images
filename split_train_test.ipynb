{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "split_train_test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2jasjSAHf3ffo2JXCPZ2D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/object_detection_for_image_cropping/blob/master/split_train_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nmOIIRYi55F",
        "colab_type": "text"
      },
      "source": [
        "# Split EOL user crops dataset into train and test\n",
        "---\n",
        "*Last Updated 24 January 2020*  \n",
        "Instead of creating image annotations from scratch, EOL user-generated cropping coordinates are used to create training and testing data to teach object detection models and evaluate model accuracy for YOLO via darkflow, SSD and Faster-RCNN object detection models, respectively. \n",
        "\n",
        "Following the [Pareto principle](https://en.wikipedia.org/wiki/Pareto_principle), 80% of the original EOL crops dataset are randomly selected to be training data and the remaining 20% will be used to test model accuracy. \n",
        "\n",
        "Resulting train and test datasets are exported for further pre-processing in [preprocessing.ipynb](https://github.com/aubricot/object_detection_for_image_cropping/blob/master/preprocessing.ipynb), before they are ready to use with the object detection models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVLY-E_tsqFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xmqBM3Esyua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read in EOL user-generated cropping data\n",
        "crops = pd.read_csv('drive/My Drive/fall19_smithsonian_informatics/train/chiroptera_crops.tsv', sep=\"\\t\", header=0)\n",
        "print(crops.head())\n",
        "\n",
        "# Randomly select 80% of data to use for training\n",
        "# set seed=2 for reproducible results\n",
        "idx = crops.sample(frac = 0.8, random_state=2).index\n",
        "train = crops.iloc[idx]\n",
        "print(train.head())\n",
        "\n",
        "# Select the remaining 20% of data for testing using the inverse index from above\n",
        "test = crops.iloc[crops.index.difference(idx)]\n",
        "print(test.head())\n",
        "\n",
        "# Write test and train to tsvs \n",
        "train.to_csv('drive/My Drive/fall19_smithsonian_informatics/train/chiroptera_crops_train.tsv', sep='\\t', header=True, index=False)\n",
        "train.to_csv('drive/My Drive/fall19_smithsonian_informatics/train/chiroptera_crops_test.tsv', sep='\\t', header=True, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}