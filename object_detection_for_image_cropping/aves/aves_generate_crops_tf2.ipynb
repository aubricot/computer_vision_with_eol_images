{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aves_generate_crops_tf2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/aves/aves_generate_crops_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrXhn1qKWm_"
      },
      "source": [
        "# Using Faster-RCNN and SSD in Tensorflow to detect birds from images   \n",
        "---   \n",
        "*Last Updated 7 May 2021*.  \n",
        "-Runs in Python 3 with Tensorflow 2.0-   \n",
        "Using Faster-RCNN and SSD as methods to do customized, large-scale image processing with Tensorflow. Using the location and dimensions of the detected birds, images will be cropped to square dimensions that are centered and padded around the detection box. Pre-trained models are used for \"out of the box\" inference on images of birds of varying dimensions and resolutions.\n",
        "\n",
        "It is modified from [here](https://medium.com/@nickbortolotti/tensorflow-object-detection-api-in-5-clicks-from-colaboratory-843b19a1edf1). The [Tensorflow Object Detection API Tutorial](https://github.com/tensorflow/models/tree/master/research/object_detection) was also used as a reference. Tensorflow Object Detection API is meant for building models for custom object detection, see more information here: [Tensorflow Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tensorflow-models-installation). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQWTwI7k4Bf"
      },
      "source": [
        "# Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvpJJUDTkct3"
      },
      "source": [
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnBVJiIzYune"
      },
      "source": [
        "# For running inference on the TF-Hub module.\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# For downloading the image.\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "\n",
        "# For drawing onto the image.\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# For measuring the inference time.\n",
        "import time\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('Tensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices.\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGx_08UcmtOF"
      },
      "source": [
        "### Model Preparation\n",
        "--- \n",
        "Define functions needed for running inference using chosen pre-trained model (SSD MobileNet v2 or Faster RCNN ResNet 50)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5bi08_cTpYm"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Load Pre-trained model from Tensorflow Hub\n",
        "# TO DO: Choose model to run inference with (both trained on MS COCO 2017)\n",
        "model = \"SSD MobileNet v2\" #@param [\"SSD MobileNet v2\", \"Faster RCNN Resnet 50\"]\n",
        "if 'SSD' in model: \n",
        "  module_handle = \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\"\n",
        "elif 'RCNN' in model:\n",
        "  module_handle = \"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1\"\n",
        "else:\n",
        "  print('Error: Model name does not correspond to module_handle')\n",
        "print('Loading {} from TF-Hub...'.format(model))\n",
        "detector = hub.load(module_handle)\n",
        "\n",
        "# For handling images\n",
        "def display_image(image):\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image)\n",
        "\n",
        "def load_img(path): #From file\n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  return img\n",
        "\n",
        "def download_and_resize_image(url, new_width=256, new_height=256, #From URL\n",
        "                              display=False):\n",
        "  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "  response = urlopen(url)\n",
        "  image_data = response.read()\n",
        "  image_data = BytesIO(image_data)\n",
        "  pil_image = Image.open(image_data)\n",
        "  im_h, im_w = pil_image.size\n",
        "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "  #print(\"Image downloaded to %s.\" % filename)\n",
        "  if display:\n",
        "    display_image(pil_image)\n",
        "  return filename, im_h, im_w\n",
        "\n",
        "# MS COCO 2017 Label Map (# output by models corresp. to text lables)\n",
        "# Note: You can modify \"filter\" to choose detection results for any of these categories\n",
        "filter = \"bird\" #@param {type:\"string\"}\n",
        "label_map = {1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane',\n",
        "              6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light',\n",
        "              11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench',\n",
        "              16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow',\n",
        "              22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack',\n",
        "              28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee',\n",
        "              35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite',\n",
        "              39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard',\n",
        "              43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork',\n",
        "              49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple',\n",
        "              54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog',\n",
        "              59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch',\n",
        "              64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet',\n",
        "              72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard',\n",
        "              77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink',\n",
        "              82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors',\n",
        "              88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n",
        "\n",
        "# For handling bounding boxes\n",
        "def draw_bounding_box_on_image(image,\n",
        "                               ymin,\n",
        "                               xmin,\n",
        "                               ymax,\n",
        "                               xmax,\n",
        "                               color,\n",
        "                               font,\n",
        "                               thickness=4,\n",
        "                               display_str_list=()):\n",
        "  \"\"\"Adds a bounding box to an image.\"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)],\n",
        "            width=thickness,\n",
        "            fill=color)\n",
        "\n",
        "  # If the total height of the display strings added to the top of the bounding\n",
        "  # box exceeds the top of the image, stack the strings below the bounding box\n",
        "  # instead of above.\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  # Each display_str has a top and bottom margin of 0.05x.\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = top + total_display_str_height\n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "    draw.text((left + margin, text_bottom - text_height - margin),\n",
        "              display_str,\n",
        "              fill=\"black\",\n",
        "              font=font)\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "# TO DO: Set the maximum number of detections to keep per image\n",
        "max_boxes = 10 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
        "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "  if max_boxes:\n",
        "    max_boxes = max_boxes\n",
        "  colors = list(ImageColor.colormap.values())\n",
        "\n",
        "  try:\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "  except IOError:\n",
        "    print(\"Font not found, using default font.\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "  for i in range(0, max_boxes):\n",
        "    if scores[0][i] >= min_score:\n",
        "      ymin, xmin, ymax, xmax = tuple(boxes[0][i])\n",
        "      display_str = \"{}: {}%\".format(label_map[class_names[0][i]],\n",
        "                                     int(100 * scores[0][i]))\n",
        "      color = colors[hash(class_names[0][i]) % len(colors)]\n",
        "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "      if filter in display_str: # Only the filtered class is shown on images\n",
        "        draw_bounding_box_on_image(\n",
        "          image_pil,\n",
        "          ymin,\n",
        "          xmin,\n",
        "          ymax,\n",
        "          xmax,\n",
        "          color,\n",
        "          font,\n",
        "          display_str_list=[display_str])\n",
        "      np.copyto(image, np.array(image_pil))\n",
        "  return image\n",
        "  \n",
        "# For running inference\n",
        "def run_detector(image_url):\n",
        "  image_path, im_h, im_w = download_and_resize_image(image_url, 640, 480)\n",
        "  img = load_img(image_path)\n",
        "\n",
        "  converted_img  = tf.image.convert_image_dtype(img, tf.uint8)[tf.newaxis, ...]\n",
        "  start_time = time.time()\n",
        "  result = detector(converted_img)\n",
        "  end_time = time.time()\n",
        "\n",
        "  result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "  print(\"Found %d objects.\" % result[\"num_detections\"])\n",
        "  print(\"Inference time: %s\" % format(end_time-start_time, '.2f'))\n",
        "\n",
        "  image_with_boxes = draw_boxes(img.numpy(), result[\"detection_boxes\"],\n",
        "      result[\"detection_classes\"], result[\"detection_scores\"])\n",
        "  \n",
        "  # Export bounding boxes to file in Google Drive\n",
        "  with open(outfpath, 'a') as out_file:\n",
        "              tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "              img_id = os.path.splitext((os.path.basename(image_url)))[0]\n",
        "              # Write one row per detected object with bounding box coordinates\n",
        "              num_detections = min(int(result[\"num_detections\"][0]), max_boxes)\n",
        "              for i in range(0, num_detections):\n",
        "                class_id = str(label_map[result[\"detection_classes\"][0][i]])\n",
        "                if filter in class_id: # Only writes rows for filtered class\n",
        "                    ymin = result[\"detection_boxes\"][0][i][0]\n",
        "                    xmin = result[\"detection_boxes\"][0][i][1]\n",
        "                    ymax = result[\"detection_boxes\"][0][i][2]\n",
        "                    xmax = result[\"detection_boxes\"][0][i][3]\n",
        "                    tsv_writer.writerow([img_id, class_id, \n",
        "                        xmin, ymin, xmax, ymax, image_url])\n",
        "  return image_with_boxes\n",
        "print('Model loaded and functions defined! \\nGo to next steps for running inference on images.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjE-Gh5ann4E"
      },
      "source": [
        "## Test running inference on a couple images from URLs\n",
        "--- \n",
        "Try running inference on any images from URL to get bounding boxes of detected birds. Results are displayed on images. No detected object (bird) coordinates are saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQgwjTjXWdLh"
      },
      "source": [
        "# TO DO: Type in image URLs 1-3 using form fields to right\n",
        "url_1 = \"https://content.eol.org/data/media/7e/9c/7a/542.15445377044.jpg\" #@param {type:\"string\"}\n",
        "url_2 = \"https://content.eol.org/data/media/81/1c/0d/542.7816025222.jpg\" #@param {type:\"string\"}\n",
        "url_3 = \"https://content.eol.org/data/media/7e/3c/0b/542.10578857864.jpg\" #@param {type:\"string\"}\n",
        "image_urls = [url_1, url_2, url_3]\n",
        "\n",
        "# Display detection results on images\n",
        "display_results = True\n",
        "\n",
        "# Set temporary outfile for tagging results\n",
        "outfpath = \"temp_outfile.tsv\"\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\")\n",
        "for im_num, image_url in enumerate(image_urls, start=1):\n",
        "  try:\n",
        "    image_wboxes = run_detector(image_url)\n",
        "    if display_results:\n",
        "      display_image(image_wboxes)\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for image {} of {}'.format(im_num, len(image_urls)))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))\n",
        "  \n",
        "  os.remove(outfpath) # Delete temporary outfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpMaZ3on72u"
      },
      "source": [
        "## Run inference on EOL 20k image bundles & save results for cropping\n",
        "---\n",
        "Use 20K EOL Aves image bundle to get bounding boxes of detected birds. Results are saved to [crops_file].tsv. Running in 4 batches of 5K images is recommended in case of unexpected Colab timeouts.\n",
        "\n",
        "Optional: Display detection boxes on up to 50 images. Image with bounding box around detected objects (birds), class type, and confidence score are shown with inference times for each image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1jj3B-teGJ7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "# Read in EOL image bundle dataframe\n",
        "# TO DO: Type in image bundle address using form field to right\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_20K_breakdown_download_000001.txt\" #@param {type:\"string\"}\n",
        "df = pd.read_csv(bundle, sep='\\n', header=None)\n",
        "df.columns = ['url']\n",
        "print('EOL image bundle head:\\n{}'.format(df.head()))\n",
        "\n",
        "# Write header row of output tagging file\n",
        "# TO DO: Change file name for each bundle/run\n",
        "# Note: If running in 4 batches of 5k images per 20k image bundle (reccomended), use a/b/c/d for each batch\n",
        "base = '/content/drive/My Drive/train/results/'\n",
        "crops_file = \"aves_cropcoords_tf2_a\" #@param [\"aves_cropcoords_tf2_a\", \"aves_cropcoords_tf2_b\", \"aves_cropcoords_tf2_c\", \"aves_cropcoords_tf2_d\"] {allow-input: true}\n",
        "if 'SSD' in model:\n",
        "  mod_abbv = '_ssd'\n",
        "elif 'RCNN' in model:\n",
        "  mod_abbv = '_rcnn'\n",
        "else:\n",
        "  print('Error: Model name does not correspond to module_handle')\n",
        "outfpath = base + crops_file.rsplit('_',1)[0] + mod_abbv + '_' + crops_file.rsplit('_',1)[1] + '.tsv'\n",
        "print('Cropping file will be saved to:\\n{}'.format(outfpath))\n",
        "\n",
        "# Run in 4 batches of 5k images each (batch a is from 0-5000, b from 5000 to 10000, etc)\n",
        "if \"_a.\" in crops_file:\n",
        "  a=0\n",
        "  b=5000\n",
        "elif \"_b.\" in crops_file:\n",
        "  a=5000\n",
        "  b=10000\n",
        "elif \"_c.\" in crops_file:\n",
        "  a=10000\n",
        "  b=15000\n",
        "elif \"_d.\" in crops_file:\n",
        "  a=15000\n",
        "  b=20000\n",
        "\n",
        "# Write header row of output tag file\n",
        "with open(outfpath, 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"img_id\", \"class_id\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"url\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pERgrECWgNRy"
      },
      "source": [
        "# Test with a smaller subset than 5k images?\n",
        "# TO DO: Check test_with_tiny_subset box if \"Yes\"\n",
        "# Then choose start_index and num_test_images values using sliders\n",
        "test_with_tiny_subset = True #@param {type: \"boolean\"}\n",
        "if test_with_tiny_subset:\n",
        "  start_index = 0 #@param {type:\"slider\", min:0, max:50, step:5}\n",
        "  a = start_index\n",
        "  num_test_imgs = 5 #@param {type:\"slider\", min:0, max:50, step:5}\n",
        "  b = start_index + num_test_imgs\n",
        "\n",
        "# Display detection results on images?\n",
        "# TO DO: Check display_results box if \"Yes\"\n",
        "# Note: Only run for <50 images at a time\n",
        "display_results = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\")\n",
        "for i, row in df.iloc[a:b].iterrows():\n",
        "  try:\n",
        "    image_wboxes = run_detector(df['url'][i])\n",
        "    if display_results:\n",
        "      display_image(image_wboxes)\n",
        "    \n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for image {} of {}'.format(i+1, (b-a)))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(row))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}