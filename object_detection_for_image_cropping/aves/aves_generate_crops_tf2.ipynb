{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aves_generate_crops_tf2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/aves/aves_generate_crops_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrXhn1qKWm_"
      },
      "source": [
        "# Using Faster-RCNN and SSD in Tensorflow to automatically crop images of birds \n",
        "---   \n",
        "*Last Updated 7 May 2021*.  \n",
        "-Runs in Python 3 with Tensorflow 2.0-   \n",
        "Using [Faster-RCNN](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1) and [SSD](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2) models pretrained on [MS COCO 2017](https://cocodataset.org/#explore) as methods to do customized, large-scale image processing with Tensorflow. Using the location and dimensions of the detected birds, images will be cropped to square dimensions that are centered and padded around the object(s) of interest (ie birds). Pre-trained models are used for \"out of the box\" inference on images of birds of varying dimensions and resolutions.\n",
        "\n",
        "Code is modified from [here](https://medium.com/@nickbortolotti/tensorflow-object-detection-api-in-5-clicks-from-colaboratory-843b19a1edf1). The [Tensorflow Object Detection API Tutorial](https://github.com/tensorflow/models/tree/master/research/object_detection) was also used as a reference. The [Tensorflow Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tensorflow-models-installation) is used for building custom models for object detection. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQWTwI7k4Bf"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvpJJUDTkct3"
      },
      "source": [
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnBVJiIzYune"
      },
      "source": [
        "# For running inference on the TF-Hub module\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# For downloading image\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "\n",
        "# For drawing onto images\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# For measuring the inference time\n",
        "import time\n",
        "\n",
        "# For working with data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "import urllib\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('Tensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# Read in data file exported from \"Combine output files A-D\" block above\n",
        "def read_datafile(fpath, sep=\"\\t\", header=0, disp_head=True):\n",
        "    \"\"\"\n",
        "    Defaults to tab-separated data files with header in row 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(fpath, sep=sep, header=header)\n",
        "        if disp_head:\n",
        "          print(\"Data header: \\n\", df.head())\n",
        "    except FileNotFoundError as e:\n",
        "        raise Exception(\"File not found: Enter the path to your file in form field and re-run\").with_traceback(e.__traceback__)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# To load image in and do something with it\n",
        "def load_img(path): \n",
        "  img = tf.io.read_file(path)\n",
        "  img = tf.image.decode_jpeg(img, channels=3)\n",
        "  return img\n",
        "\n",
        "# To display loaded image\n",
        "def display_image(image):\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  plt.grid(False)\n",
        "  plt.imshow(image)\n",
        "\n",
        "# For reading in images from URL and passing through TF models for inference\n",
        "def download_and_resize_image(url, new_width=256, new_height=256, #From URL\n",
        "                              display=False):\n",
        "  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n",
        "  response = urlopen(url)\n",
        "  image_data = response.read()\n",
        "  image_data = BytesIO(image_data)\n",
        "  pil_image = Image.open(image_data)\n",
        "  im_h, im_w = pil_image.size\n",
        "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
        "  pil_image_rgb = pil_image.convert(\"RGB\")\n",
        "  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n",
        "  #print(\"Image downloaded to %s.\" % filename)\n",
        "  if display:\n",
        "    display_image(pil_image)\n",
        "  return filename, im_h, im_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcJET5z5CkBr"
      },
      "source": [
        "## Generate cropping coordinates for images\n",
        "---\n",
        "Run EOL 20k image bundles through pre-trained object detection models and save results in 4 batches (A-D). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGx_08UcmtOF"
      },
      "source": [
        "#### Set up model parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5bi08_cTpYm"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "# Load Pre-trained model from Tensorflow Hub\n",
        "# TO DO: Choose model to run inference with (both trained on MS COCO 2017)\n",
        "model = \"SSD MobileNet v2\" #@param [\"SSD MobileNet v2\", \"Faster RCNN Resnet 50\"]\n",
        "if 'SSD' in model: \n",
        "  module_handle = \"https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2\"\n",
        "  mod_abbv = '_ssd'\n",
        "elif 'RCNN' in model:\n",
        "  module_handle = \"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1\"\n",
        "  mod_abbv = '_rcnn'\n",
        "else:\n",
        "  print('Error: Model name does not correspond to module_handle')\n",
        "print('Loading {} from TF-Hub...'.format(model))\n",
        "detector = hub.load(module_handle)\n",
        "\n",
        "# MS COCO 2017 Label Map (# output by models corresp. to text lables)\n",
        "# Note: You can modify \"filter\" to choose detection results for any of these categories\n",
        "filter = \"bird\" #@param {type:\"string\"}\n",
        "label_map = {1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane',\n",
        "              6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light',\n",
        "              11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench',\n",
        "              16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep', 21: 'cow',\n",
        "              22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe', 27: 'backpack',\n",
        "              28: 'umbrella', 31: 'handbag', 32: 'tie', 33: 'suitcase', 34: 'frisbee',\n",
        "              35: 'skis', 36: 'snowboard', 37: 'sports ball', 38: 'kite',\n",
        "              39: 'baseball bat', 40: 'baseball glove', 41: 'skateboard', 42: 'surfboard',\n",
        "              43: 'tennis racket', 44: 'bottle', 46: 'wine glass', 47: 'cup', 48: 'fork',\n",
        "              49: 'knife', 50: 'spoon', 51: 'bowl', 52: 'banana', 53: 'apple',\n",
        "              54: 'sandwich', 55: 'orange', 56: 'broccoli', 57: 'carrot', 58: 'hot dog',\n",
        "              59: 'pizza', 60: 'donut', 61: 'cake', 62: 'chair', 63: 'couch',\n",
        "              64: 'potted plant', 65: 'bed', 67: 'dining table', 70: 'toilet',\n",
        "              72: 'tv', 73: 'laptop', 74: 'mouse', 75: 'remote', 76: 'keyboard',\n",
        "              77: 'cell phone', 78: 'microwave', 79: 'oven', 80: 'toaster', 81: 'sink',\n",
        "              82: 'refrigerator', 84: 'book', 85: 'clock', 86: 'vase', 87: 'scissors',\n",
        "              88: 'teddy bear', 89: 'hair drier', 90: 'toothbrush'}\n",
        "\n",
        "# For handling bounding boxes\n",
        "def draw_bounding_box_on_image(image,\n",
        "                               ymin,\n",
        "                               xmin,\n",
        "                               ymax,\n",
        "                               xmax,\n",
        "                               color,\n",
        "                               font,\n",
        "                               thickness=4,\n",
        "                               display_str_list=()):\n",
        "  \"\"\"Adds a bounding box to an image.\"\"\"\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  im_width, im_height = image.size\n",
        "  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)],\n",
        "            width=thickness,\n",
        "            fill=color)\n",
        "\n",
        "  # Adjust display string placement if out of bounds\n",
        "  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "  # Each display_str has a top and bottom margin of 0.05x.\n",
        "  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "  if top > total_display_str_height:\n",
        "    text_bottom = top\n",
        "  else:\n",
        "    text_bottom = top + total_display_str_height\n",
        "  # Reverse list and print from bottom to top.\n",
        "  for display_str in display_str_list[::-1]:\n",
        "    text_width, text_height = font.getsize(display_str)\n",
        "    margin = np.ceil(0.05 * text_height)\n",
        "    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "    draw.text((left + margin, text_bottom - text_height - margin),\n",
        "              display_str,\n",
        "              fill=\"black\",\n",
        "              font=font)\n",
        "    text_bottom -= text_height - 2 * margin\n",
        "\n",
        "# TO DO: Set the maximum number of detections to keep per image\n",
        "max_boxes = 10 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
        "  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "  if max_boxes:\n",
        "    max_boxes = max_boxes\n",
        "  colors = list(ImageColor.colormap.values())\n",
        "\n",
        "  try:\n",
        "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "  except IOError:\n",
        "    print(\"Font not found, using default font.\")\n",
        "    font = ImageFont.load_default()\n",
        "\n",
        "  for i in range(0, max_boxes):\n",
        "    if scores[0][i] >= min_score:\n",
        "      ymin, xmin, ymax, xmax = tuple(boxes[0][i])\n",
        "      display_str = \"{}: {}%\".format(label_map[class_names[0][i]],\n",
        "                                     int(100 * scores[0][i]))\n",
        "      color = colors[hash(class_names[0][i]) % len(colors)]\n",
        "      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
        "      if filter in display_str: # Only the filtered class is shown on images\n",
        "        draw_bounding_box_on_image(\n",
        "          image_pil,\n",
        "          ymin,\n",
        "          xmin,\n",
        "          ymax,\n",
        "          xmax,\n",
        "          color,\n",
        "          font,\n",
        "          display_str_list=[display_str])\n",
        "      np.copyto(image, np.array(image_pil))\n",
        "  return image\n",
        "\n",
        "# Define start and stop indices in EOL bundle for running inference   \n",
        "def set_start_stop():\n",
        "    # To test with a tiny subset, use first 5 bundle images\n",
        "    if test_with_tiny_subset:\n",
        "        start=0\n",
        "        stop=5\n",
        "    # To run inference on 4 batches of 5k images each\n",
        "    elif \"_a.\" in outfpath: # batch a is from 0-5000\n",
        "        start=0\n",
        "        stop=5000\n",
        "    elif \"_b.\" in outfpath: # batch b is from 5000-1000\n",
        "        start=5000\n",
        "        stop=10000\n",
        "    elif \"_c.\" in outfpath: # batch c is from 10000-15000\n",
        "        start=10000\n",
        "        stop=15000\n",
        "    elif \"_d.\" in outfpath: # batch d is from 15000-20000\n",
        "        start=15000\n",
        "        stop=20000\n",
        "    \n",
        "    return start, stop\n",
        "\n",
        "# For running inference\n",
        "def run_detector(image_url):\n",
        "  image_path, im_h, im_w = download_and_resize_image(image_url, 640, 480)\n",
        "  img = load_img(image_path)\n",
        "\n",
        "  converted_img  = tf.image.convert_image_dtype(img, tf.uint8)[tf.newaxis, ...]\n",
        "  start_time = time.time()\n",
        "  result = detector(converted_img)\n",
        "  end_time = time.time()\n",
        "\n",
        "  result = {key:value.numpy() for key,value in result.items()}\n",
        "\n",
        "  print(\"Found %d objects.\" % result[\"num_detections\"])\n",
        "  print(\"Inference time: %s\" % format(end_time-start_time, '.2f'))\n",
        "\n",
        "  image_with_boxes = draw_boxes(img.numpy(), result[\"detection_boxes\"],\n",
        "      result[\"detection_classes\"], result[\"detection_scores\"])\n",
        "\n",
        "  # Export bounding boxes to file in Google Drive\n",
        "  with open(outfpath, 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            img_id = os.path.splitext((os.path.basename(image_url)))[0]\n",
        "            # Write one row per detected object with bounding box coordinates\n",
        "            num_detections = min(int(result[\"num_detections\"][0]), max_boxes)\n",
        "            for i in range(0, num_detections):\n",
        "                class_id = str(label_map[result[\"detection_classes\"][0][i]])\n",
        "                if filter in class_id: # Only writes rows for filtered class\n",
        "                      ymin = result[\"detection_boxes\"][0][i][0]\n",
        "                      xmin = result[\"detection_boxes\"][0][i][1]\n",
        "                      ymax = result[\"detection_boxes\"][0][i][2]\n",
        "                      xmax = result[\"detection_boxes\"][0][i][3]\n",
        "                      tsv_writer.writerow([img_id, class_id, \n",
        "                          xmin, ymin, xmax, ymax, im_w, im_h, image_url])\n",
        "  return image_with_boxes\n",
        "\n",
        "print('Model loaded and functions defined! \\nGo to next steps for running inference on images.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjE-Gh5ann4E"
      },
      "source": [
        "#### Test: Run inference on a couple images from URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQgwjTjXWdLh"
      },
      "source": [
        "# TO DO: Type in image URLs 1-3 using form fields to right\n",
        "url_1 = \"https://content.eol.org/data/media/7e/9c/7a/542.15445377044.jpg\" #@param {type:\"string\"}\n",
        "url_2 = \"https://content.eol.org/data/media/81/1c/0d/542.7816025222.jpg\" #@param {type:\"string\"}\n",
        "url_3 = \"https://content.eol.org/data/media/7e/3c/0b/542.10578857864.jpg\" #@param {type:\"string\"}\n",
        "image_urls = [url_1, url_2, url_3]\n",
        "\n",
        "# Display detection results on images\n",
        "display_results = True\n",
        "\n",
        "# Set temporary outfile for tagging results\n",
        "outfpath = \"temp_outfile.tsv\"\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\")\n",
        "for im_num, image_url in enumerate(image_urls, start=1):\n",
        "  try:\n",
        "    image_wboxes = run_detector(image_url)\n",
        "    if display_results:\n",
        "      display_image(image_wboxes)\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for image {} of {}'.format(im_num, len(image_urls)))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))\n",
        "  \n",
        "  os.remove(outfpath) # Delete temporary outfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpMaZ3on72u"
      },
      "source": [
        "### Generate crops: Run inference on EOL images & save results for cropping\n",
        "Use 20K EOL Aves image bundle to get bounding boxes of detected birds. Results are saved to [crops_file].tsv. Running in 4 batches of 5K images is recommended in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1jj3B-teGJ7"
      },
      "source": [
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "# Read in EOL image bundle dataframe\n",
        "# TO DO: Type in image bundle address using form field to right\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_20K_breakdown_download_000001.txt\" #@param {type:\"string\"}\n",
        "df = read_datafile(bundle, sep='\\n', header=None)\n",
        "df.columns = ['url']\n",
        "print('\\n EOL image bundle head:\\n{}'.format(df.head()))\n",
        "\n",
        "# Write header row of output tagging file\n",
        "# TO DO: Change file name for each bundle/run\n",
        "# Note: If running in 4 batches of 5k images per 20k image bundle (reccomended), use a/b/c/d for each batch\n",
        "base = '/content/drive/My Drive/train/results/'\n",
        "crops_file = \"aves_cropcoords_tf2_a\" #@param [\"aves_cropcoords_tf2_a\", \"aves_cropcoords_tf2_b\", \"aves_cropcoords_tf2_c\", \"aves_cropcoords_tf2_d\"] {allow-input: true}\n",
        "outfpath = base + crops_file.rsplit('_',1)[0] + mod_abbv + '_' + crops_file.rsplit('_',1)[1] + '.tsv'\n",
        "print('\\n Cropping file will be saved to:\\n{}'.format(outfpath))\n",
        "\n",
        "# Write header row of output tag file\n",
        "with open(outfpath, 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"img_id\", \"class_id\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"im_width\", \"im_height\", \"url\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pERgrECWgNRy"
      },
      "source": [
        "# Run inference on images\n",
        "\n",
        "# Test with a smaller subset than 5k images?\n",
        "# TO DO: If yes, check test_with_tiny_subset box\n",
        "test_with_tiny_subset = True #@param {type: \"boolean\"}\n",
        "\n",
        "# Display detection results on images?\n",
        "# TO DO: Check display_results box if \"Yes\"\n",
        "# Note: Only run for <50 images at a time\n",
        "display_results = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\")\n",
        "start, stop = set_start_stop()\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "  try:\n",
        "    image_wboxes = run_detector(df['url'][i])\n",
        "    if display_results:\n",
        "      display_image(image_wboxes)\n",
        "    \n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for image {} of {}'.format(i+1, (stop-start)))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(df['url'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VSLfSiGbW_S"
      },
      "source": [
        "## Post-process detection results\n",
        "--- \n",
        "Combine output files for batches A-D. Then, convert detection boxes into square, centered thumbnail cropping coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsdrzerAc1PU"
      },
      "source": [
        "#### Merge batch output files A-D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqSnb1SabbAd"
      },
      "source": [
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# Get name of ONE output file (any of A-D)\n",
        "# TO DO: If you just ran \"Generate crops\" above, you do not need to enter anything\n",
        "# TO DO: If you ran \"Generate crops\" during a previous session, enter the path for ONE output file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "  outfpath = \"/content/drive/My Drive/train/results/aves_cropcoords_tf2_ssd_d.tsv\" #@param {type:\"string\"}\n",
        "\n",
        "# Combine 4 batches of detection box coordinates to one dataframe\n",
        "base =  os.path.splitext(outfpath)[0].rsplit('_',1)[0] + '_'\n",
        "exts = ['a.tsv', 'b.tsv', 'c.tsv', 'd.tsv']\n",
        "all_filenames = [base + e for e in exts]\n",
        "df = pd.concat([pd.read_csv(f, sep='\\t', header=0, na_filter = False) for f in all_filenames], ignore_index=True)\n",
        "\n",
        "# Write results to tsv\n",
        "print(\"New concatenated dataframe with all 4 batches: \\n\", df.head())\n",
        "concat_outfpath = base + 'concat.tsv'\n",
        "df.to_csv(concat_outfpath, sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f4NUfucV5IE"
      },
      "source": [
        "#### Combine individual detection boxes into one \"superbox\" per image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URd75ay5zCOc"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "from functools import reduce\n",
        "from urllib.error import HTTPError\n",
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# Convert normalized detection coordinates (scaled to 0,1) to pixel values\n",
        "def denormalize_coords(crops):\n",
        "    crops.xmin = crops.xmin * crops.im_width\n",
        "    crops.ymin = crops.ymin * crops.im_height\n",
        "    crops.xmax = crops.xmax * crops.im_width\n",
        "    crops.ymax = crops.ymax * crops.im_height\n",
        "    # Round results to 2 decimal places\n",
        "    crops.round(2)\n",
        "    #print(\"De-normalized cropping coordinates: \\n\", crops.head())\n",
        "\n",
        "    return crops\n",
        "\n",
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "def make_superboxes(crops):\n",
        "    # Get superbox coordinates that contain all detection boxes per image\n",
        "    xmin = pd.DataFrame(crops.groupby(['url'])['xmin'].min()) # smallest xmin\n",
        "    ymin = pd.DataFrame(crops.groupby(['url'])['ymin'].min()) # smallest ymin\n",
        "    xmax = pd.DataFrame(crops.groupby(['url'])['xmax'].max()) # largest xmax\n",
        "    ymax = pd.DataFrame(crops.groupby(['url'])['ymax'].max()) # largest ymax\n",
        "\n",
        "    # Workaround to get im_height, im_width and class in same format as 'super box' coords\n",
        "    # There is only one value for im_height and im_width, so taking max will return unchanged values\n",
        "    im_h = pd.DataFrame(crops.groupby(['url'])['im_height'].max())\n",
        "    im_w = pd.DataFrame(crops.groupby(['url'])['im_width'].max())\n",
        "    im_class = pd.DataFrame(crops.groupby(['url'])['class_id'].max())\n",
        "  \n",
        "    # Make list of superboxes\n",
        "    superbox_list = [im_h, im_w, xmin, ymin, xmax, ymax, im_class]\n",
        "\n",
        "    # Make a new dataframe with 1 superbox per image\n",
        "    superbox_df = reduce(lambda  left, right: pd.merge(left, right, on=['url'],\n",
        "                                            how='outer'), superbox_list)\n",
        "    #print(\"Cropping dataframe, 1 superbox per image: \\n\", crops_unq.head())\n",
        "\n",
        "    return superbox_df\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "def add_identifiers(*, bundle_info, crops):\n",
        "    # Get dataObjectVersionIDs, identifiers, and eolMediaURLS from indexed cols\n",
        "    ids = bundle_info.iloc[:, np.r_[0:2,-2]]\n",
        "    ids.set_index('eolMediaURL', inplace=True, drop=True)\n",
        "    #print(\"Bundle identifying info head: \\n\", ids.head())\n",
        "\n",
        "    # Set up superboxes df for mapping to bundle_info\n",
        "    superboxes.reset_index(inplace=True)\n",
        "    superboxes.rename(columns={'url': 'eolMediaURL'}, inplace=True)\n",
        "    superboxes.set_index('eolMediaURL', inplace=True, drop=True)\n",
        "\n",
        "    # Map dataObjectVersionIDs to crops_unq using eolMediaURL as the index\n",
        "    crops_w_identifiers = pd.DataFrame(superboxes.merge(ids, left_index=True, right_index=True))\n",
        "    crops_w_identifiers.reset_index(inplace=True)\n",
        "    print(\"\\n Crops with added EOL identifiers: \\n\", crops_w_identifiers.head())\n",
        "  \n",
        "    return crops_w_identifiers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "b6C68mM_c7Uz"
      },
      "source": [
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "\n",
        "# Read in crop file exported from \"Combine output files A-D\" block above\n",
        "concat_outfpath = \"/content/drive/My Drive/train/results/aves_cropcoords_tf2_ssd_concat.tsv\" #@param {type:\"string\"}\n",
        "crops = read_datafile(concat_outfpath, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# De-normalize cropping coordinates to pixel values\n",
        "crops = denormalize_coords(crops)\n",
        "\n",
        "# Make 1 superbox per image [coordinates: bottom left (smallest xmin, ymin) and top right (largest xmax, ymax)]\n",
        "superboxes = make_superboxes(crops)\n",
        "\n",
        "# Read in EOL image \"breakdown\" bundle dataframe from \"breakdown_download\" bundle used for cropping\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_20K_breakdown_download_000001.txt\" #@param {type:\"string\"}\n",
        "breakdown = bundle.replace(\"download_\", \"\") # Get EOL breakdown bundle url from \"breakdown_download\" address\n",
        "bundle_info = read_datafile(breakdown, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "crops_w_identifiers = add_identifiers(bundle_info=bundle_info, crops=superboxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Vx-B8oc785"
      },
      "source": [
        "#### Make superbox dimensions square (Optional: Add padding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RCQuE-HGRsu"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "# Suppress pandas warning about writing over a copy of data\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Check if dimensions are out of bounds\n",
        "def are_dims_oob(dim):\n",
        "    # Check if square dimensions are out of image bounds (OOB)\n",
        "    if dim > min(im_h, im_w):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Center padded, square coordinates around object midpoint\n",
        "def center_coords(coord_a, coord_b, crop_w, crop_h, im_dim_a, im_dim_b, pad):\n",
        "    # Centered, padded top-right coordinates\n",
        "    tr_coord_a = coord_a + 0.5*(abs(crop_h - crop_w)) + pad\n",
        "    tr_coord_b = coord_a + pad\n",
        "    # Adjust coordinate positions if OOB (out of bounds)\n",
        "    if crop_h != crop_w: # for cond 1 and 2\n",
        "        # Both coords not OOB\n",
        "        if (tr_coord_a <= im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - 0.5*(abs(crop_h - crop_w)) - pad\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_a OOB (+), shift cropping box down/left a-axis \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = 0.5*(abs(im_dim_a - crop_w))\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_b OOB (+), shift cropping box down/left b-axis    \n",
        "        elif (tr_coord_a <= im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - 0.5*(abs(crop_h - crop_w)) - pad\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "        # Both coords OOB (+), shift cropping box down/left both axes     \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = 0.5*(abs(im_dim_a - crop_w))\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "    else: # for cond 3\n",
        "        # Both coords not OOB\n",
        "        if (tr_coord_a <= im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - pad\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_a OOB (+), shift cropping box down/left a-axis \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - (tr_coord_a - im_dim_a + pad)\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_b OOB (+), shift cropping box down/left b-axis    \n",
        "        elif (tr_coord_a <= im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - pad\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "        # Both coords OOB (+), shift cropping box down/left both axes     \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - (tr_coord_a - im_dim_a + pad)\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "    \n",
        "    return bl_coord_a, bl_coord_b\n",
        "\n",
        "# Set square dimensions = larger bounding box side\n",
        "def make_large_square(dim):\n",
        "    # Set new square crop dims = original larger crop dim\n",
        "    lg_square = crop_w1 = crop_h1 = dim\n",
        "    return lg_square\n",
        "\n",
        "# Set square dimensions = smaller bounding box side\n",
        "def make_small_square(dim):\n",
        "    # Set new square crop dims = original smaller crop dim\n",
        "    sm_square = crop_w1 = crop_h1 = dim\n",
        "    return sm_square\n",
        "\n",
        "# Add x% padding to bounding box dimensions\n",
        "def add_padding(dim):\n",
        "    # Add padding on all sides of square\n",
        "    padded_dim = dim + 2*percent_pad*dim\n",
        "    return padded_dim\n",
        "\n",
        "# Make square crops that are within image bounds for different scenarios\n",
        "def make_square_crops(df):\n",
        "    print(\"Before making square: \\n\", df.head())\n",
        "    start_time = time.time()\n",
        "    df['crop_height'] = round(df['ymax'] - df['ymin'], 1)\n",
        "    df['crop_width'] = round(df['xmax'] - df['xmin'], 1)\n",
        "    for i, row in df.iterrows():\n",
        "        # Define variables for use filtering data through loops below\n",
        "        crop_h0 = df['crop_height'][i]\n",
        "        crop_w0 = df['crop_width'][i]\n",
        "        #print(\"crop_h0: {}, crop_w0: {}\".format(crop_h0, crop_w0))\n",
        "        pad = percent_pad * max(crop_h0, crop_w0)  \n",
        "        global im_h, im_w\n",
        "        im_h = df.im_height[i]\n",
        "        im_w = df.im_width[i]\n",
        "        xmin0 = df.xmin[i]\n",
        "        ymin0 = df.ymin[i]\n",
        "        xmax0 = df.xmax[i]\n",
        "        ymax0 = df.ymax[i]\n",
        "        \n",
        "        # Conditions determine how rectangle bounding boxes are made square\n",
        "        cond1 = crop_h0 > crop_w0 # crop height > width\n",
        "        cond2 = crop_h0 < crop_w0 # crop width > height\n",
        "        cond3 = crop_h0 == crop_w0 # crop height = width (already square)\n",
        "\n",
        "        # Crop Height > Crop Width\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 1\n",
        "        if cond1:\n",
        "            lg_sq = make_large_square(crop_h0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_w0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "\n",
        "            # Where padded crop height is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = lg_padded_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i], df.ymin[i] = center_coords(xmin0, ymin0, crop_w0, crop_h1, im_w, im_h, pad)\n",
        "\n",
        "            # Where unpadded crop height is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is False) and (are_dims_oob(lg_sq) is True):\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = lg_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i] = xmin0 - 0.5*(min(im_h, im_w) - crop_w0)\n",
        "                df.ymin[i] = 0\n",
        "\n",
        "            # Where padded crop width is within image dimensions\n",
        "            elif (are_dims_oob(lg_sq) is False) and (are_dims_oob(sm_padded_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = sm_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i] = xmin0 - 0.5*pad\n",
        "                df.ymin[i] = ymin0 + 0.5*(crop_h0 - crop_w0) - pad   \n",
        "\n",
        "            # Where unpadded crop width is within image dimensions\n",
        "            elif (are_dims_oob(sm_padded_sq) is False) and (are_dims_oob(sm_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = sm_sq\n",
        "\n",
        "            # Where crop width and height are both OOB\n",
        "            elif are_dims_oob(sm_sq) is False:\n",
        "                # Do not crop, set values equal to image dimensions\n",
        "                df.crop_height[i] = crop_h1 = im_h \n",
        "                df.ymin[i] = 0\n",
        "                df.xmin[i] = 0 \n",
        "    \n",
        "        # Crop Width > Crop Height\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 2\n",
        "        elif cond2:\n",
        "            lg_sq = make_large_square(crop_w0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_h0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "\n",
        "            # Where padded crop width is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = lg_padded_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i], df.xmin[i] = center_coords(ymin0, xmin0, crop_w1, crop_h0, im_w, im_h, pad)\n",
        "\n",
        "            # Where unpadded crop width is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is False) and (are_dims_oob(lg_sq) is True):\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = lg_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i] = ymin0 - 0.5*(min(im_h, im_w) - crop_h0)\n",
        "                df.xmin[i] = 0\n",
        "\n",
        "            # Where padded crop height is within image dimensions\n",
        "            elif (are_dims_oob(lg_sq) is False) and (are_dims_oob(sm_padded_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = sm_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i] = ymin0 - pad\n",
        "                df.xmin[i] = xmin0 + 0.5*(crop_w0 - crop_h0) - pad   \n",
        "\n",
        "            # Where unpadded crop height is within image dimensions\n",
        "            elif (are_dims_oob(sm_padded_sq) is False) and (are_dims_oob(sm_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = sm_sq\n",
        "\n",
        "            # Where crop width and height are both OOB\n",
        "            elif are_dims_oob(sm_sq) is False:\n",
        "                # Do not crop, set values equal to image dimensions\n",
        "                df.crop_width[i] = crop_w1 = im_w\n",
        "                df.crop_height[i] = crop_h1 = im_h \n",
        "                df.ymin[i] = 0\n",
        "                df.xmin[i] = 0 \n",
        "\n",
        "        # Crop Width == Crop Height\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 3\n",
        "        elif cond3: \n",
        "            lg_sq = make_large_square(crop_w0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_h0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "        \n",
        "            # Where padded crop width/height is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:            \n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = crop_h1 = lg_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i], df.ymin[i] = center_coords(xmin0, ymin0, crop_w0, crop_w1, im_w, im_h, pad)\n",
        "                \n",
        "            # Where unpadded crop width/height is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is True) and (are_dims_oob(lg_sq) is False):\n",
        "                # Both coords not OOB, no changes needed\n",
        "                if (ymax0 <= im_h) and (xmax0 <= im_w):\n",
        "                    pass\n",
        "                \n",
        "                # Topright X coord OOB (+), shift cropping box left\n",
        "                elif (ymax0 <= im_h) and (xmax0 > im_w):  \n",
        "                    df.xmin[i] = xmin0 - (xmax0 - im_w)\n",
        "                # Topright Y coord OOB (+), shift cropping box down\n",
        "                elif (ymax0 > im_h) and (xmax0 <= im_w):\n",
        "                    df.ymin[i] = ymin0 - (ymax0 - im_h)\n",
        "                # X and Y coords OOB (+), shift cropping box down and left   \n",
        "                elif (ymax0 > im_h) and (xmax0 > im_w):\n",
        "                    df.ymin[i] = ymin0 - (ymax0 - im_h)\n",
        "                    df.xmin[i] = xmin0 - (xmax0 - im_w)\n",
        "\n",
        "    # Image coordinates should be positive, set negative xmin and ymin values to 0\n",
        "    df.xmin[df.xmin < 0] = 0\n",
        "    df.ymin[df.ymin < 0] = 0\n",
        "    print(\"Cropping coordinates, made square and with {}% padding: \\n{}\".format(percent_pad, df.head()))\n",
        "\n",
        "    # Print time to run script\n",
        "    print ('Run time: {} seconds'.format(format(time.time()- start_time, '.2f')))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Format cropping dimensions to EOL standards\n",
        "def format_crops_for_eol(df):\n",
        "# {\"height\":\"423\",\"width\":\"640\",\"crop_x\":123.712,\"crop_y\":53.4249,\"crop_width\":352,\"crop_height\":0}\n",
        "    df['crop_dimensions'] = np.nan\n",
        "    for i, row in df.iterrows():\n",
        "        df.crop_dimensions[i] = ('{{\"height\":\"{}\",\"width\":\"{}\",\"crop_x\":{},\"crop_y\":{},\"crop_width\":{},\"crop_height\":{}}}'\n",
        "        .format(df.im_height[i], df.im_width[i], df.xmin[i], df.ymin[i], df.crop_width[i], df.crop_height[i]))\n",
        "    #print(\"\\n EOL formatted cropping dimensions: \\n\", df.head())\n",
        "\n",
        "    # Add other dataframe elements from cols: identifier, dataobjectversionid, eolmediaurl, im_class, crop_dimensions\n",
        "    eol_crops = pd.DataFrame(df.iloc[:,np.r_[-5,-4,-6,0,-1]])\n",
        "    print(\"\\n EOL formatted cropping dimensions: \\n\", eol_crops.head())\n",
        "\n",
        "    return eol_crops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "MW0pjR3_HeYg"
      },
      "source": [
        "# Make crops square and within image bounds\n",
        "\n",
        "# Optional TO DO: Pad by xx% larger crop dimension\n",
        "percent_pad = 0 #@param {type:\"slider\", min:0, max:10, step:2}\n",
        "\n",
        "# Make crops square and within bounds\n",
        "df = make_square_crops(crops_w_identifiers)\n",
        "\n",
        "# Export crop coordinates to display_test.tsv to visualize results in next code block and confirm crop transformations\n",
        "display_test_fpath = os.path.splitext(concat_outfpath)[0] + '_displaytest' + '.tsv'\n",
        "print(\"\\n File for displaying square crops on images will be saved to: \\n\", display_test_fpath)\n",
        "df.to_csv(display_test_fpath, sep='\\t', index=False)\n",
        "\n",
        "# Format image and cropping dimensions for EOL standards\n",
        "eol_crops = format_crops_for_eol(df)\n",
        "\n",
        "# Write results to tsv\n",
        "eol_crops_fpath = os.path.splitext(display_test_fpath)[0].rsplit('_',2)[0] + '_20k_final' + '.tsv'\n",
        "eol_crops.to_csv(eol_crops_fpath, columns = eol_crops.iloc[:,:-1], sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsFGmd2PbiCg"
      },
      "source": [
        "## Display cropping results on images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h28VTCzsboqE"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Read in cropping file for displaying results\n",
        "# Note: If you just ran \"Post-process results\" above, you do not need to enter anything\n",
        "# TO DO: If you ran \"Generate crops\" during a previous session, enter the path for desired cropping file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    outfpath = \"/content/drive/My Drive/train/results/aves_cropcoords_tf2_ssd_concat_displaytest.tsv\" #@param {type:\"string\"}\n",
        "df = pd.read_csv(outfpath, sep=\"\\t\", header=0)\n",
        "print(df.head())\n",
        "\n",
        "# For uploading an image from url\n",
        "# Modified from https://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/\n",
        "def url_to_image(url):\n",
        "    resp = urllib.request.urlopen(url)\n",
        "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    im_h, im_w = image.shape[:2]\n",
        " \n",
        "    return image\n",
        "\n",
        "# Draw cropping box on image\n",
        "def draw_box_on_image(df, img):\n",
        "    # Get box coordinates\n",
        "    xmin = df['xmin'][i].astype(int)\n",
        "    ymin = df['ymin'][i].astype(int)\n",
        "    xmax = df['xmin'][i].astype(int) + df['crop_width'][i].astype(int)\n",
        "    ymax = df['ymin'][i].astype(int) + df['crop_height'][i].astype(int)\n",
        "    boxcoords = [xmin, ymin, xmax, ymax]\n",
        "\n",
        "    # Set box/font color and size\n",
        "    maxdim = max(df['im_height'][i],df['im_width'][i])\n",
        "    fontScale = maxdim/600\n",
        "    box_col = (255, 0, 157)\n",
        "  \n",
        "    # Add label to image\n",
        "    tag = df['class_id'][i]\n",
        "    image_wbox = cv2.putText(img, tag, (xmin+7, ymax-12), cv2.FONT_HERSHEY_SIMPLEX, fontScale, box_col, 2, cv2.LINE_AA)  \n",
        "  \n",
        "    # Draw box label on image\n",
        "    image_wbox = cv2.rectangle(img, (xmin, ymax), (xmax, ymin), box_col, 5)\n",
        "\n",
        "    return image_wbox, boxcoords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwoIgqtgbr6U",
        "cellView": "code"
      },
      "source": [
        "# Display crop dimensions on images\n",
        "\n",
        "# TO DO: Adjust line below to see up to 50 images displayed at a time\n",
        "start = 0 #@param {type:\"slider\", min:0, max:5000, step:50}\n",
        "stop = start+50\n",
        "\n",
        "# Loop through images\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "  # Read in image \n",
        "  url = df['eolMediaURL'][i]\n",
        "  img = url_to_image(url)\n",
        "  \n",
        "  # Draw bounding box on image\n",
        "  image_wbox, boxcoords = draw_box_on_image(df, img)\n",
        "  \n",
        "  # Plot cropping box on image\n",
        "  _, ax = plt.subplots(figsize=(10, 10))\n",
        "  ax.imshow(image_wbox)\n",
        "\n",
        "  # Display image URL and coordinatesabove image\n",
        "  # Helps with fine-tuning data transforms in post-processing steps above\n",
        "  plt.title('{} \\n xmin: {}, ymin: {}, xmax: {}, ymax: {}'.format(url, boxcoords[0], boxcoords[1], boxcoords[2], boxcoords[3]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}