{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/aves/aves_generate_crops_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrXhn1qKWm_"
      },
      "source": [
        "# Using Faster-RCNN and SSD in Tensorflow to automatically crop images of birds\n",
        "---   \n",
        "*Last Updated 17 January 2025*  \n",
        "-Runs in Python 3 with Tensorflow 2.0-   \n",
        "Using [Faster-RCNN](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1) and [SSD](https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2) models pretrained on [MS COCO 2017](https://cocodataset.org/#explore) as methods to do customized, large-scale image processing with Tensorflow. Using the location and dimensions of the detected birds, images will be cropped to square dimensions that are centered and padded around the object(s) of interest (ie birds). Pre-trained models are used for \"out of the box\" inference on images of birds of varying dimensions and resolutions.\n",
        "\n",
        "Code is modified from [here](https://medium.com/@nickbortolotti/tensorflow-object-detection-api-in-5-clicks-from-colaboratory-843b19a1edf1). The [Tensorflow Object Detection API Tutorial](https://github.com/tensorflow/models/tree/master/research/object_detection) was also used as a reference. The [Tensorflow Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tensorflow-models-installation) is used for building custom models for object detection.\n",
        "\n",
        "Notes:\n",
        "* Run code blocks by pressing play button in brackets on left\n",
        "* Change parameters using form fields on right (find details at corresponding lines of code by searching '#@param')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQWTwI7k4Bf"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvpJJUDTkct3"
      },
      "outputs": [],
      "source": [
        "#@title Choose where to save results\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Note: You can modify \"filter\" to choose detection results for any class of interest the model is trained on\n",
        "filter = \"bird\" #@param [\"bird\"] {allow-input: true}\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "basewd = \"/content/drive/MyDrive/train\" #@param [\"/content/drive/MyDrive/train\"] {allow-input: true}\n",
        "# Type in the folder that you want to contain TF2 files\n",
        "folder = \"tf2\" #@param [\"tf2\"] {allow-input: true}\n",
        "# Define current working directory using form field inputs\n",
        "cwd = basewd + '/' + folder\n",
        "\n",
        "# Install dependencies\n",
        "!pip3 install --upgrade gdown\n",
        "!gdown 1fIEf387CNrWk0ziPY-ltvwN9VrRXrRkY # Download helper_funcs folder\n",
        "!tar -xzvf helper_funcs.tar.gz -C .\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11jqYAq1yj7E"
      },
      "outputs": [],
      "source": [
        "#@title Choose saved model parameters\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "from setup import *\n",
        "\n",
        "# Set up directory structure\n",
        "setup_dirs(cwd)\n",
        "%cd $cwd\n",
        "\n",
        "# Load Pre-trained model from Tensorflow Hub (both trained on MS COCO 2017)\n",
        "model = \"SSD MobileNet v2\" #@param [\"SSD MobileNet v2\", \"Faster RCNN Resnet 50\"] {allow-input: true}\n",
        "detector, module_handle, mod_abbv = load_tfhub_detector(model)\n",
        "\n",
        "# Load corresponding label map for MS COCO 2017\n",
        "!gdown 1mWmTvaBWKZ2GBbRllDoxPkecgJl5jnFK # Download labelmap.json\n",
        "label_map = convert_labelmap('labelmap.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnBVJiIzYune"
      },
      "outputs": [],
      "source": [
        "#@title Import libraries\n",
        "\n",
        "# For running inference on the TF-Hub module\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# For downloading and displaying images\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "\n",
        "# For drawing onto images\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For working with data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "pd.options.display.max_columns = None\n",
        "import os\n",
        "import csv\n",
        "import urllib\n",
        "import sys\n",
        "import json\n",
        "\n",
        "# Define EOL CV custom functions\n",
        "from wrangle_data import *\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('Tensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcJET5z5CkBr"
      },
      "source": [
        "## Generate crops: Run inference on EOL images & save resulting coordinates for cropping - Run 4X for batches A-D\n",
        "---\n",
        "Use 20K EOL image bundle to generate bounding boxes around each object with pre-trained object detection models. Results are saved to [crops_file].tsv. Run this section 4 times (to make batches A-D) of 5K images each to incrementally save in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5bi08_cTpYm"
      },
      "outputs": [],
      "source": [
        "#@title Define functions\n",
        "\n",
        "# Set the maximum number of detections to keep per image\n",
        "max_boxes = 10 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "# Set the minimum confidence score for detections to keep per image\n",
        "min_score = 0.6 #@param {type:\"slider\", min:0, max:0.9, step:0.1}\n",
        "\n",
        "# Set filename for saving classification results\n",
        "def set_outpath(crops_file, cwd):\n",
        "    outpath = cwd + '/' + 'results/' + crops_file.rsplit('_',1)[0] + mod_abbv + '_' + crops_file.rsplit('_',1)[1] + '.tsv'\n",
        "    print(\"\\nSaving results to: \\n\", outpath)\n",
        "\n",
        "    return outpath\n",
        "\n",
        "# Export object detection results\n",
        "def export_results(image_url, result, outfpath, im_h, im_w, filter=filter):\n",
        "    with open(outfpath, 'a') as out_file:\n",
        "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "        img_id = os.path.splitext((os.path.basename(image_url)))[0]\n",
        "        # Write one row per detected object with bounding box coordinates\n",
        "        num_detections = min(int(result[\"num_detections\"][0]), max_boxes)\n",
        "        for i in range(0, num_detections):\n",
        "            class_name = str(label_map[result[\"detection_classes\"][0][i]])\n",
        "            if filter in class_name: # Only writes rows for filtered class\n",
        "                ymin = result[\"detection_boxes\"][0][i][0]\n",
        "                xmin = result[\"detection_boxes\"][0][i][1]\n",
        "                ymax = result[\"detection_boxes\"][0][i][2]\n",
        "                xmax = result[\"detection_boxes\"][0][i][3]\n",
        "                tsv_writer.writerow([img_id, class_name,\n",
        "                          xmin, ymin, xmax, ymax, im_h, im_w, image_url])\n",
        "        print(\"\\nObject detection results for Image {} saved to: {}\".format(image_url, outfpath))\n",
        "\n",
        "    return img_id\n",
        "\n",
        "# Format cropping dimensions to EOL standards\n",
        "def format_crops_for_eol(df):\n",
        "# {\"height\":\"423\",\"width\":\"640\",\"crop_x\":123.712,\"crop_y\":53.4249,\"crop_width\":352,\"crop_height\":0}\n",
        "    df['crop_dimensions'] = np.nan\n",
        "    for i, row in df.iterrows():\n",
        "        df.loc[i, 'crop_dimensions'] = ('{{\"height\":\"{}\",\"width\":\"{}\",\"crop_x\":{},\"crop_y\":{},\"crop_width\":{},\"crop_height\":{}}}'\n",
        "        .format(df.im_height[i], df.im_width[i], df.xmin[i], df.ymin[i], df.crop_width[i], df.crop_height[i]))\n",
        "\n",
        "    # Add other dataframe elements from cols: identifier, dataobjectversionid, eolmediaurl, im_class, crop_dimensions\n",
        "    eol_crops = pd.DataFrame(df.iloc[:,np.r_[-5,-4,-6,0,-1]])\n",
        "    print(\"\\n EOL formatted cropping dimensions: \\n\", eol_crops.head())\n",
        "\n",
        "    return eol_crops\n",
        "\n",
        "print('Model loaded and functions defined! \\nGo to next steps to run inference on images.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpMaZ3on72u"
      },
      "source": [
        "### Generate crops: Run inference on EOL images & save results for cropping - Run 4X for batches A-D\n",
        "Use 20K EOL Aves image bundle to get bounding boxes of detected birds. Results are saved to [crops_file].tsv. Run this section 4 times (to make batches A-D) of 5K images each to incrementally save in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHauEgaTVIEO"
      },
      "outputs": [],
      "source": [
        "#@title Enter EOL image bundle and choose inference settings (change **crops_file** for each batch A-D)\n",
        "\n",
        "# Load in EOL image bundle\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_20K_breakdown_download_000001.txt\" #@param [\"https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_20K_breakdown_download_000001.txt\"] {allow-input: true}\n",
        "df = read_datafile(bundle, sep='\\t', header=None, disp_head=False)\n",
        "df.columns = ['url']\n",
        "print('\\n EOL image bundle head:\\n{}'.format(df.head()))\n",
        "\n",
        "# Test pipeline with a smaller subset than 5k images?\n",
        "run = \"test with tiny subset\" #@param [\"test with tiny subset\", \"for all images\"]\n",
        "\n",
        "# Display detection results on images?\n",
        "if 'tiny subset' in run:\n",
        "    display_results = True\n",
        "else:\n",
        "    display_results = False\n",
        "\n",
        "# Take 5k subset of bundle for running inference\n",
        "# Change filename for each batch\n",
        "crops_file = \"aves_cropcoords_tf2_c\" #@param [\"aves_cropcoords_tf2_a\", \"aves_cropcoords_tf2_b\", \"aves_cropcoords_tf2_c\", \"aves_cropcoords_tf2_d\"] {allow-input: true}\n",
        "outfpath = set_outpath(crops_file, cwd)\n",
        "\n",
        "# Write header row of output tag file\n",
        "if not os.path.isfile(outfpath):\n",
        "    with open(outfpath, 'a') as out_file:\n",
        "              tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "              tsv_writer.writerow([\"img_id\", \"class_name\", \"xmin\", \\\n",
        "                                   \"ymin\", \"xmax\", \"ymax\", \"im_width\", \\\n",
        "                                   \"im_height\", \"url\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pERgrECWgNRy"
      },
      "outputs": [],
      "source": [
        "#@title Choose settings to run inference on image batches A-D\n",
        "\n",
        "# Run EOL bundle images through trained model and save results\n",
        "print(\"Running inference on images\")\n",
        "all_predictions = []\n",
        "start, stop, cutoff = set_start_stop(run, df)\n",
        "for i, row in enumerate(df.iloc[start:stop].iterrows()):\n",
        "    try:\n",
        "        # Run image through object detector and export result\n",
        "        image_url = df['url'][row[0]]\n",
        "        image_wboxes, result, im_h, im_w = run_detector_tf(detector, image_url, outfpath, filter, label_map, max_boxes, min_score)\n",
        "        img_id = export_results(image_url, result, outfpath, im_h, im_w)\n",
        "\n",
        "        # Optional: Display detections on images\n",
        "        if (i+1<=50) and display_results:\n",
        "            display_image(image_wboxes)\n",
        "\n",
        "        # Display progress message after each image\n",
        "        all_predictions.append(img_id)\n",
        "        print('\\033[92m {}) Inference complete for image {} of {} \\033[0m \\n'.format(i+1, i+1, cutoff))\n",
        "        if len(all_predictions)>=cutoff:\n",
        "              break\n",
        "\n",
        "    except:\n",
        "        print('Check if URL from {} is valid\\n'.format(df['url'][i]))\n",
        "\n",
        "print(\"\\n\\n~~~\\n\\033[92m Inference complete!\\033[0m \\033[93m Run these steps for remaining batches A-D before proceeding.\\033[0m\\n~~~\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VSLfSiGbW_S"
      },
      "source": [
        "## Post-process detection results\n",
        "---\n",
        "Combine output files for batches A-D. Then, convert detection boxes into square, centered thumbnail cropping coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqSnb1SabbAd"
      },
      "outputs": [],
      "source": [
        "#@title Merge 5k image batch output files A-D\n",
        "\n",
        "# Enter path to any inference result batch file A-D\n",
        "\n",
        "# If you just ran \"Generate crops\" above, you do not need to enter anything\n",
        "# If you ran \"Generate crops\" during a previous session, enter the path for ONE output file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    crops_file = \"aves_cropcoords_tf2_a\" #@param [\"aves_cropcoords_tf2_a\", \"aves_cropcoords_tf2_b\", \"aves_cropcoords_tf2_c\", \"aves_cropcoords_tf2_d\"] {allow-input: true}\n",
        "    outfpath = set_outpath(crops_file, cwd)\n",
        "\n",
        "# Combine 4 batches of detection box coordinates to one dataframe\n",
        "basewd =  os.path.splitext(outfpath)[0].rsplit('_',1)[0] + '_'\n",
        "exts = ['a.tsv', 'b.tsv', 'c.tsv', 'd.tsv']\n",
        "all_filenames = [basewd + e for e in exts]\n",
        "df = pd.concat([pd.read_csv(f, sep='\\t', header=0, na_filter = False) for f in all_filenames], ignore_index=True)\n",
        "\n",
        "# Write results to tsv\n",
        "concat_outfpath = basewd + 'concat.tsv'\n",
        "df.to_csv(concat_outfpath, sep='\\t', index=False)\n",
        "print(\"New concatenated dataframe with all 4 batches saved to: {} \\n{}\".format(concat_outfpath, df.head()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6C68mM_c7Uz"
      },
      "outputs": [],
      "source": [
        "#@title Combine individual detection boxes into one \"superbox\" per image\n",
        "\n",
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "\n",
        "# Read in crop file exported from \"Combine output files A-D\" block above\n",
        "crops = read_datafile(concat_outfpath, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# De-normalize cropping coordinates to pixel values\n",
        "crops = denormalize_coords(crops)\n",
        "\n",
        "# Make 1 superbox per image [coordinates: bottom left (smallest xmin, ymin) and top right (largest xmax, ymax)]\n",
        "superboxes = make_superboxes(crops)\n",
        "\n",
        "# Read in EOL image \"breakdown\" bundle dataframe from \"breakdown_download\" bundle used for cropping\n",
        "if 'bundle' not in locals() or globals():\n",
        "    bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Aves_20K_breakdown_download_000001.txt\" #@param {type:\"string\"}\n",
        "breakdown = bundle.replace(\"download_\", \"\") # Get EOL breakdown bundle url from \"breakdown_download\" address\n",
        "bundle_info = read_datafile(breakdown, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "crops_w_identifiers = add_identifiers(superboxes, bundle_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW0pjR3_HeYg"
      },
      "outputs": [],
      "source": [
        "#@title Make superbox square and within image bounds (Optional: add padding)\n",
        "\n",
        "# Pad by xx% larger crop dimension\n",
        "pad = 2 #@param {type:\"slider\", min:0, max:10, step:2}\n",
        "pad = pad/100 # Convert to percentage\n",
        "\n",
        "# Make crops square and within bounds\n",
        "df = make_square_crops(crops_w_identifiers, pad)\n",
        "\n",
        "# Export crop coordinates to display_test.tsv to visualize results in next code block and confirm crop transformations\n",
        "display_test_fpath = os.path.splitext(concat_outfpath)[0] + '_displaytest' + '.tsv'\n",
        "print(\"\\n File for displaying square crops on images will be saved to: \\n\", display_test_fpath)\n",
        "df.to_csv(display_test_fpath, sep='\\t', index=False)\n",
        "\n",
        "# Format image and cropping dimensions for EOL standards\n",
        "eol_crops = format_crops_for_eol(df)\n",
        "\n",
        "# Write results to tsv\n",
        "eol_crops_fpath = os.path.splitext(display_test_fpath)[0].rsplit('_',2)[0] + '_20k_final' + '.tsv'\n",
        "eol_crops.to_csv(eol_crops_fpath, columns = eol_crops.iloc[:,:-1], sep='\\t', index=False)\n",
        "print(\"EOL formatted crops dataset saved to: {} \\n{}\".format(eol_crops_fpath, eol_crops.head()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsFGmd2PbiCg"
      },
      "source": [
        "## Display cropping results on images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h28VTCzsboqE"
      },
      "outputs": [],
      "source": [
        "#@title Read in cropping file and display results on images\n",
        "from wrangle_data import *\n",
        "import cv2\n",
        "\n",
        "# If you just ran \"Post-process results\" above, you do not need to enter anything\n",
        "# If you ran \"Generate crops\" during a previous session, enter the path for desired cropping file\n",
        "if 'display_test_fpath' not in locals() or globals():\n",
        "    crops_file = \"aves_cropcoords_tf2_a\" #@param [\"aves_cropcoords_tf2_a\", \"aves_cropcoords_tf2_b\", \"aves_cropcoords_tf2_c\", \"aves_cropcoords_tf2_d\"] {allow-input: true}\n",
        "    outfpath = set_outpath(crops_file, cwd)\n",
        "    display_test_fpath =  os.path.splitext(outfpath)[0].rsplit('_',1)[0] + '_concat_displaytest' + '.tsv'\n",
        "    print(display_test_fpath)\n",
        "df = pd.read_csv(display_test_fpath, sep=\"\\t\", header=0)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwoIgqtgbr6U"
      },
      "outputs": [],
      "source": [
        "#@title Choose starting index for crops to display\n",
        "\n",
        "# Adjust line to right to see up to 50 images displayed at a time\n",
        "start = 0 #@param {type:\"slider\", min:0, max:5000, step:50}\n",
        "stop = start+50\n",
        "\n",
        "# Loop through images\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "    # Read in image\n",
        "    url = df['eolMediaURL'][i]\n",
        "    img = url_to_image(url)\n",
        "\n",
        "    # Draw bounding box on image\n",
        "    image_wbox, boxcoords = draw_box_on_image(df, i, img)\n",
        "\n",
        "    # Plot cropping box on image\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(image_wbox)\n",
        "\n",
        "    # Display image URL and coordinatesabove image\n",
        "    # Helps with fine-tuning data transforms in post-processing steps above\n",
        "    plt.title('{} \\n xmin: {}, ymin: {}, xmax: {}, ymax: {}'.format(url, boxcoords[0], boxcoords[1], boxcoords[2], boxcoords[3]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}