{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lepidoptera_generate_crops_tf2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/lepidoptera/lepidoptera_generate_crops_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrXhn1qKWm_"
      },
      "source": [
        "# Use Faster-RCNN and SSD in Tensorflow to automatically crop images of butterflies & moths (Lepidoptera)\n",
        "---   \n",
        "*Last Updated 29 May 2021*  \n",
        "-Now runs in Python 3 with Tensorflow 2.0-     \n",
        "\n",
        "Use trained object detection models to automatically crop images of butterflies & moths (Lepidoptera) to square dimensions centered around animal(s). \n",
        "\n",
        "Models were trained and saved to Google Drive in [lepidoptera_train_tf2_ssd_rcnn.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/lepidoptera/lepidoptera_train_tf2_ssd_rcnn.ipynb).\n",
        "\n",
        "***Models were trained in Python 2 and TF 1 in Jan 2020: RCNN trained for 2 days to 200,000 steps and SSD for 2 days to 200,000 steps.***\n",
        "\n",
        "Notes:   \n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change filepaths/taxon names where you see 'TO DO' \n",
        "* For each 24 hour period on Google Colab, you have up to 12 hours of free GPU access. \n",
        "\n",
        "References:     \n",
        "* [Official Tensorflow Object Detection API Instructions](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)   \n",
        "* [Medium Blog on training using Tensorflow Object Detection API in Colab](https://medium.com/analytics-vidhya/training-an-object-detection-model-with-tensorflow-api-using-google-colab-4f9a688d5e8b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQWTwI7k4Bf"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mAC7PfUrWX1"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlg-GTnKKRa3"
      },
      "source": [
        "# Set working directory\n",
        "# TO DO: Type in the path to your working directory in form field to right\n",
        "basewd = \"/content/drive/MyDrive/train\" #@param {type:\"string\"}\n",
        "%cd $basewd\n",
        "# TO DO: Type in the folder that you want to contain TF2 files\n",
        "folder = \"tf2\" #@param {type:\"string\"}\n",
        "wd = basewd + '/' + folder\n",
        "%cd $wd\n",
        "\n",
        "# For object detection\n",
        "import tensorflow as tf \n",
        "import tensorflow_hub as hub\n",
        "import sys\n",
        "sys.path.append(\"tf_models/models/research/\")\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# For downloading and displaying images\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import tempfile\n",
        "import urllib\n",
        "from urllib.request import urlretrieve\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "\n",
        "# For drawing onto images\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For working with data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pathlib\n",
        "import csv\n",
        "import six.moves.urllib as urllib\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('\\nTensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGx_08UcmtOF"
      },
      "source": [
        "## Generate cropping coordinates for images\n",
        "---\n",
        "Run EOL 20k image bundles through pre-trained object detection models and save results in 4 batches (A-D). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQpr26hJOv5y"
      },
      "source": [
        "### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n_alUkLZ1gl"
      },
      "source": [
        "# Set up model functions and parameters\n",
        "%cd $wd\n",
        "\n",
        "# Use EOL pre-trained model for object detection?\n",
        "# TO DO: Check use_EOL_model if \"Yes\"\n",
        "use_EOL_model = False #@param {type: \"boolean\"}\n",
        "if use_EOL_model: # Downloaded needed files for inference\n",
        "    !gdown --id 1mqJi8gdzOpVCLVSG7pTttr9SfnZfCU1I # Download labelmap.pbtxt\n",
        "    !mkdir -p tf_models/train_demo/rcnn/finetuned_model\n",
        "    !cd tf_models/train_demo/rcnn/finetuned_model\n",
        "    !gdown --id 1H8zDM0zrSIlAAt1H17rvD6d7Vaj-GwVq # Download frozen_inference_graph.pb\n",
        "    PATH_TO_CKPT = 'tf_models/train_demo/rcnn/finetuned_model'\n",
        "# Use your own trained model for object detection?\n",
        "else:\n",
        "    # TO DO: Change path to saved model checkpoint\n",
        "    output_directory = \"tf_models/train_demo/rcnn/finetuned_model\" #@param {type:\"string\"}\n",
        "    PATH_TO_CKPT = output_directory + '/frozen_inference_graph.pb'\n",
        "print(\"Loading trained model from: \\n\", PATH_TO_CKPT)\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = \"labelmap.pbtxt\" #@param {type:\"string\"}\n",
        "NUM_CLASSES = 1 #@param\n",
        "print(\"\\nLoading label map for {} class(es) from: \\n{}\".format(NUM_CLASSES, PATH_TO_LABELS))\n",
        "\n",
        "# Class of interest\n",
        "filter = \"Lepidoptera\" #@param {type:\"string\"}\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# Read in data file exported from \"Combine output files A-D\" block above\n",
        "def read_datafile(fpath, sep=\"\\t\", header=0, disp_head=True):\n",
        "    \"\"\"\n",
        "    Defaults to tab-separated data files with header in row 0\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(fpath, sep=sep, header=header)\n",
        "        if disp_head:\n",
        "          print(\"Data header: \\n\", df.head())\n",
        "    except FileNotFoundError as e:\n",
        "        raise Exception(\"File not found: Enter the path to your file in form field and re-run\").with_traceback(e.__traceback__)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# To display loaded image\n",
        "def display_image(image):\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image)\n",
        "\n",
        "# Define start and stop indices in EOL bundle for running inference   \n",
        "def set_start_stop():\n",
        "    # To test with a tiny subset, use 5 random bundle images\n",
        "    if test_with_tiny_subset:\n",
        "        start=np.random.choice(a=1000, size=1)[0]\n",
        "        stop=start+5\n",
        "    # To run inference on 4 batches of 5k images each\n",
        "    elif \"_a.\" in outfpath: # batch a is from 0-5000\n",
        "        start=0\n",
        "        stop=5000\n",
        "    elif \"_b.\" in outfpath: # batch b is from 5000-1000\n",
        "        start=5000\n",
        "        stop=10000\n",
        "    elif \"_c.\" in outfpath: # batch c is from 10000-15000\n",
        "        start=10000\n",
        "        stop=15000\n",
        "    elif \"_d.\" in outfpath: # batch d is from 15000-20000\n",
        "        start=15000\n",
        "        stop=20000\n",
        "    \n",
        "    return start, stop\n",
        "\n",
        "# Restore trained detection graph    \n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.compat.v1.GraphDef()\n",
        "    with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# For handling bounding boxes\n",
        "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax,\n",
        "                               color, font, thickness=4, display_str_list=()):\n",
        "    \"\"\"Adds a bounding box to an image.\"\"\"\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    im_width, im_height = image.size\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)], width=thickness, fill=color)\n",
        "\n",
        "    # Adjust display string placement if out of bounds\n",
        "    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
        "    # Each display_str has a top and bottom margin of 0.05x.\n",
        "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "    if top > total_display_str_height:\n",
        "        text_bottom = top\n",
        "    else:\n",
        "        text_bottom = top + total_display_str_height\n",
        "    # Reverse list and print from bottom to top.\n",
        "    for display_str in display_str_list[::-1]:\n",
        "        text_width, text_height = font.getsize(display_str)\n",
        "        margin = np.ceil(0.05 * text_height)\n",
        "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "        draw.text((left + margin, text_bottom - text_height - margin),\n",
        "                  display_str, fill=\"black\", font=font)\n",
        "        text_bottom -= text_height - 2 * margin\n",
        "\n",
        "# TO DO: Set the maximum number of detections to keep per image\n",
        "max_boxes = 10 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "# TO DO: Set the minimum confidence score for detections to keep per image\n",
        "min_score = 0.1 #@param {type:\"slider\", min:0, max:0.9, step:0.1}\n",
        "\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
        "    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "    if max_boxes:\n",
        "        max_boxes = max_boxes\n",
        "    if min_score:\n",
        "        min_score = min_score\n",
        "    colors = list(ImageColor.colormap.values())\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "    except IOError:\n",
        "        print(\"Font not found, using default font.\")\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    for i in range(0, max_boxes):\n",
        "        if scores[0][i] >= min_score:\n",
        "            ymin, xmin, ymax, xmax = tuple(boxes[0][i])\n",
        "            display_str = \"{}: {}%\".format(category_index[class_names[0][i]]['name'],\n",
        "                                     int(100 * scores[0][i]))\n",
        "            color = colors[hash(class_names[0][i]) % len(colors)]\n",
        "            image_pil = Image.fromarray(np.squeeze(image))\n",
        "        if filter in display_str: # Only the filtered class is shown on images\n",
        "            draw_bounding_box_on_image(\n",
        "                image_pil,\n",
        "                ymin, xmin, ymax, xmax,\n",
        "                color, font, display_str_list=[display_str])\n",
        "            np.copyto(image, np.array(image_pil))\n",
        "    \n",
        "    return image[0]\n",
        "\n",
        "def url_to_image(url):\n",
        "    resp = urllib.request.urlopen(url)\n",
        "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_np = np.expand_dims(image, axis=0)\n",
        "    im_h, im_w = image.shape[:2]\n",
        "  \n",
        "    return image_np, im_h, im_w\n",
        "\n",
        "# For running inference\n",
        "def run_detector_tf(image_url):\n",
        "    image_np, im_h, im_w = url_to_image(image_url)\n",
        "    with detection_graph.as_default():\n",
        "        with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
        "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "            # Actual detection\n",
        "            start_time = time.time()\n",
        "            result = sess.run([detection_boxes, detection_scores, \n",
        "                               detection_classes, num_detections],\n",
        "                               feed_dict={image_tensor: image_np})\n",
        "            end_time = time.time()\n",
        "            \n",
        "            result = {\"detection_boxes\": result[0], \"detection_scores\": result[1],\n",
        "                      \"detection_classes\": result[2], \"num_detections\": result[3]}\n",
        "            print(\"Found %d objects.\" % result[\"num_detections\"])\n",
        "            print(\"Inference time: %s sec\" % format(end_time-start_time, '.2f'))\n",
        "      \n",
        "            # Draw detection boxes on image\n",
        "            image_with_boxes = draw_boxes(image_np, result[\"detection_boxes\"],\n",
        "                                  result[\"detection_classes\"], result[\"detection_scores\"])\n",
        "\n",
        "            # Export bounding boxes to file in Google Drive\n",
        "            with open(outfpath, 'a') as out_file:\n",
        "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                img_id = os.path.splitext((os.path.basename(image_url)))[0]\n",
        "                # Write one row per detected object with bounding box coordinates\n",
        "                num_detections = min(int(result[\"num_detections\"][0]), max_boxes)\n",
        "                for i in range(0, num_detections):\n",
        "                    class_name = category_index[result[\"detection_classes\"][0][i]]['name']\n",
        "                    if filter in class_name: # Only writes rows for filtered class\n",
        "                        ymin = result[\"detection_boxes\"][0][i][0]\n",
        "                        xmin = result[\"detection_boxes\"][0][i][1]\n",
        "                        ymax = result[\"detection_boxes\"][0][i][2]\n",
        "                        xmax = result[\"detection_boxes\"][0][i][3]\n",
        "                        tsv_writer.writerow([img_id, class_name, \n",
        "                                  xmin, ymin, xmax, ymax, im_w, im_h, image_url])\n",
        "      \n",
        "    return image_with_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3HJoT3kx0a3"
      },
      "source": [
        "#### Test: Run inference on a couple images from URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D585u93x4N2"
      },
      "source": [
        "# TO DO: Type in image URLs 1-3 using form fields to right\n",
        "url_1 = \"https://content.eol.org/data/media/80/38/d3/542.5233503330.jpg\" #@param {type:\"string\"}\n",
        "url_2 = \"https://content.eol.org/data/media/7f/82/be/542.3424226994.jpg\" #@param {type:\"string\"}\n",
        "url_3 = \"https://content.eol.org/data/media/99/0d/fd/84.CalPhotos_4444_4444_0110_1162.jpg\" #@param {type:\"string\"}\n",
        "image_urls = [url_1, url_2, url_3]\n",
        "\n",
        "# Display detection results on images\n",
        "display_results = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Set temporary outfile for tagging results\n",
        "outfpath = \"temp_outfile.tsv\"\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\")\n",
        "for im_num, image_url in enumerate(image_urls, start=1):\n",
        "  try:\n",
        "    image_wboxes = run_detector_tf(image_url)\n",
        "    if display_results:\n",
        "        display_image(image_wboxes)\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for image {} of {}\\n'.format(im_num, len(image_urls)))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid\\n'.format(image_url))\n",
        "  \n",
        "  os.remove(outfpath) # Delete temporary outfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4xGqHxjE0mC"
      },
      "source": [
        "### Generate crops: Run inference on EOL images & save results for cropping\n",
        "Use 20K EOL Lepidoptera image bundle to get bounding boxes of detected bats. Results are saved to [crops_file].tsv.   \n",
        "Run in 4 batches of 5K images to backup regularly in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4aRzjXBh1KZ"
      },
      "source": [
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "# Read in EOL image bundle dataframe\n",
        "# TO DO: Type in image bundle address using form field to right\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Lepidoptera_20K_breakdown_download_000001.txt\" #@param {type:\"string\"}\n",
        "df = read_datafile(bundle, sep='\\n', header=None)\n",
        "df.columns = ['url']\n",
        "print('\\n EOL image bundle head:\\n{}'.format(df.head()))\n",
        "\n",
        "# Write header row of output tagging file\n",
        "# TO DO: Change file name for each bundle/run\n",
        "# Note: If running in 4 batches of 5k images per 20k image bundle (reccomended), use a/b/c/d for each batch\n",
        "base_path = \"results/\" #@param {type:\"string\"}\n",
        "crops_file = \"lepidoptera_cropcoords_tf2_a\" #@param [\"lepidoptera_cropcoords_tf2_a\", \"lepidoptera_cropcoords_tf2_b\", \"lepidoptera_cropcoords_tf2_c\", \"lepidoptera_cropcoords_tf2_d\"] {allow-input: true}\n",
        "outfpath = base_path + crops_file.rsplit('_',1)[0] + '_' + 'rcnn' + '_' + crops_file.rsplit('_',1)[1] + '.tsv'\n",
        "print('\\n Cropping file will be saved to:\\n{}'.format(outfpath))\n",
        "\n",
        "# Write header row of output tag file\n",
        "with open(outfpath, 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"img_id\", \"class_name\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"im_width\", \"im_height\", \"url\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbbHfPXIE6QE"
      },
      "source": [
        "# Run inference on images\n",
        "\n",
        "# Test with a smaller subset than 5k images?\n",
        "# TO DO: If yes, check test_with_tiny_subset box\n",
        "test_with_tiny_subset = True #@param {type: \"boolean\"}\n",
        "\n",
        "# Display detection results on images?\n",
        "# TO DO: Check display_results box if \"Yes\"\n",
        "# Note: Only run for <50 images at a time\n",
        "display_results = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\")\n",
        "start, stop = set_start_stop()\n",
        "for i, row in enumerate(df.iloc[start:stop].iterrows()):\n",
        "    try:\n",
        "        image_wboxes = run_detector_tf(df['url'][i])\n",
        "        if display_results:\n",
        "            display_image(image_wboxes)\n",
        "    \n",
        "        # Display progress message after each image\n",
        "        print('{}) Inference complete for image {} of {}\\n'.format(row[0], i+1, (stop-start)))\n",
        "\n",
        "    except:\n",
        "        print('Check if URL from {} is valid\\n'.format(df['url'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSg85W0WiDao"
      },
      "source": [
        "## Post-process detection results\n",
        "--- \n",
        "Combine output files for batches A-D. Then, convert detection boxes into square, centered thumbnail cropping coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akN1OTCmiI0B"
      },
      "source": [
        "#### Merge batch output files A-D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENqHrC3-iFxQ"
      },
      "source": [
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# Get name of ONE output file (any of A-D)\n",
        "# TO DO: If you just ran \"Generate crops\" above, you do not need to enter anything\n",
        "# TO DO: If you ran \"Generate crops\" during a previous session, enter the path for ONE output file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    outfpath = \"results/lepidoptera_cropcoords_tf2_rcnn_d.tsv\" #@param {type:\"string\"}\n",
        "\n",
        "# Combine 4 batches of detection box coordinates to one dataframe\n",
        "base_path =  os.path.splitext(outfpath)[0].rsplit('_',1)[0] + '_'\n",
        "exts = ['a.tsv', 'b.tsv', 'c.tsv', 'd.tsv']\n",
        "all_filenames = [base_path + e for e in exts]\n",
        "df = pd.concat([pd.read_csv(f, sep='\\t', header=0, na_filter = False) for f in all_filenames], ignore_index=True)\n",
        "\n",
        "# Write results to tsv\n",
        "print(\"New concatenated dataframe with all 4 batches: \\n\", df.head())\n",
        "concat_outfpath = base_path + 'concat.tsv'\n",
        "df.to_csv(concat_outfpath, sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzM6i-fniPc_"
      },
      "source": [
        "#### Combine individual detection boxes into one \"superbox\" per image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-Gj9D4iQLj"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "from functools import reduce\n",
        "from urllib.error import HTTPError\n",
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# Convert normalized detection coordinates (scaled to 0,1) to pixel values\n",
        "def denormalize_coords(crops):\n",
        "    crops.xmin = crops.xmin * crops.im_width\n",
        "    crops.ymin = crops.ymin * crops.im_height\n",
        "    crops.xmax = crops.xmax * crops.im_width\n",
        "    crops.ymax = crops.ymax * crops.im_height\n",
        "    # Round results to 2 decimal places\n",
        "    crops.round(2)\n",
        "    #print(\"De-normalized cropping coordinates: \\n\", crops.head())\n",
        "\n",
        "    return crops\n",
        "\n",
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "def make_superboxes(crops):\n",
        "    # Get superbox coordinates that contain all detection boxes per image\n",
        "    xmin = pd.DataFrame(crops.groupby(['url'])['xmin'].min()) # smallest xmin\n",
        "    ymin = pd.DataFrame(crops.groupby(['url'])['ymin'].min()) # smallest ymin\n",
        "    xmax = pd.DataFrame(crops.groupby(['url'])['xmax'].max()) # largest xmax\n",
        "    ymax = pd.DataFrame(crops.groupby(['url'])['ymax'].max()) # largest ymax\n",
        "\n",
        "    # Workaround to get im_height, im_width and class in same format as 'super box' coords\n",
        "    # There is only one value for im_height and im_width, so taking max will return unchanged values\n",
        "    im_h = pd.DataFrame(crops.groupby(['url'])['im_height'].max())\n",
        "    im_w = pd.DataFrame(crops.groupby(['url'])['im_width'].max())\n",
        "    im_class = pd.DataFrame(crops.groupby(['url'])['class_name'].max())\n",
        "  \n",
        "    # Make list of superboxes\n",
        "    superbox_list = [im_h, im_w, xmin, ymin, xmax, ymax, im_class]\n",
        "\n",
        "    # Make a new dataframe with 1 superbox per image\n",
        "    superbox_df = reduce(lambda  left, right: pd.merge(left, right, on=['url'],\n",
        "                                            how='outer'), superbox_list)\n",
        "    #print(\"Cropping dataframe, 1 superbox per image: \\n\", crops_unq.head())\n",
        "\n",
        "    return superbox_df\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "def add_identifiers(*, bundle_info, crops):\n",
        "    # Get dataObjectVersionIDs, identifiers, and eolMediaURLS from indexed cols\n",
        "    ids = bundle_info.iloc[:, np.r_[0:2,-2]]\n",
        "    ids.set_index('eolMediaURL', inplace=True, drop=True)\n",
        "    #print(\"Bundle identifying info head: \\n\", ids.head())\n",
        "\n",
        "    # Set up superboxes df for mapping to bundle_info\n",
        "    superboxes.reset_index(inplace=True)\n",
        "    superboxes.rename(columns={'url': 'eolMediaURL'}, inplace=True)\n",
        "    superboxes.set_index('eolMediaURL', inplace=True, drop=True)\n",
        "\n",
        "    # Map dataObjectVersionIDs to crops_unq using eolMediaURL as the index\n",
        "    crops_w_identifiers = pd.DataFrame(superboxes.merge(ids, left_index=True, right_index=True))\n",
        "    crops_w_identifiers.reset_index(inplace=True)\n",
        "    print(\"\\n Crops with added EOL identifiers: \\n\", crops_w_identifiers.head())\n",
        "  \n",
        "    return crops_w_identifiers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg2LqfpKiSWk"
      },
      "source": [
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "\n",
        "# Read in crop file exported from \"Combine output files A-D\" block above\n",
        "concat_outfpath = \"results/lepidoptera_cropcoords_tf2_rcnn_concat.tsv\" #@param {type:\"string\"}\n",
        "crops = read_datafile(concat_outfpath, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# De-normalize cropping coordinates to pixel values\n",
        "crops = denormalize_coords(crops)\n",
        "\n",
        "# Make 1 superbox per image [coordinates: bottom left (smallest xmin, ymin) and top right (largest xmax, ymax)]\n",
        "superboxes = make_superboxes(crops)\n",
        "\n",
        "# Read in EOL image \"breakdown\" bundle dataframe from \"breakdown_download\" bundle used for cropping\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Lepidoptera_20K_breakdown_download_000001.txt\" #@param {type:\"string\"}\n",
        "breakdown = bundle.replace(\"download_\", \"\") # Get EOL breakdown bundle url from \"breakdown_download\" address\n",
        "bundle_info = read_datafile(breakdown, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "crops_w_identifiers = add_identifiers(bundle_info=bundle_info, crops=superboxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_08BCyNiUcM"
      },
      "source": [
        "#### Make superbox dimensions square (Optional: Add padding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyyMzfYuiYbe"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "# Suppress pandas warning about writing over a copy of data\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Check if dimensions are out of bounds\n",
        "def are_dims_oob(dim):\n",
        "    # Check if square dimensions are out of image bounds (OOB)\n",
        "    if dim > min(im_h, im_w):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Center padded, square coordinates around object midpoint\n",
        "def center_coords(coord_a, coord_b, crop_w, crop_h, im_dim_a, im_dim_b, pad):\n",
        "    # Centered, padded top-right coordinates\n",
        "    tr_coord_a = coord_a + 0.5*(abs(crop_h - crop_w)) + pad\n",
        "    tr_coord_b = coord_a + pad\n",
        "    # Adjust coordinate positions if OOB (out of bounds)\n",
        "    if crop_h != crop_w: # for cond 1 and 2\n",
        "        # Both coords not OOB\n",
        "        if (tr_coord_a <= im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - 0.5*(abs(crop_h - crop_w)) - pad\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_a OOB (+), shift cropping box down/left a-axis \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = 0.5*(abs(im_dim_a - crop_w))\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_b OOB (+), shift cropping box down/left b-axis    \n",
        "        elif (tr_coord_a <= im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - 0.5*(abs(crop_h - crop_w)) - pad\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "        # Both coords OOB (+), shift cropping box down/left both axes     \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = 0.5*(abs(im_dim_a - crop_w))\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "    else: # for cond 3\n",
        "        # Both coords not OOB\n",
        "        if (tr_coord_a <= im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - pad\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_a OOB (+), shift cropping box down/left a-axis \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - (tr_coord_a - im_dim_a + pad)\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_b OOB (+), shift cropping box down/left b-axis    \n",
        "        elif (tr_coord_a <= im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - pad\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "        # Both coords OOB (+), shift cropping box down/left both axes     \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - (tr_coord_a - im_dim_a + pad)\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "    \n",
        "    return bl_coord_a, bl_coord_b\n",
        "\n",
        "# Set square dimensions = larger bounding box side\n",
        "def make_large_square(dim):\n",
        "    # Set new square crop dims = original larger crop dim\n",
        "    lg_square = crop_w1 = crop_h1 = dim\n",
        "    return lg_square\n",
        "\n",
        "# Set square dimensions = smaller bounding box side\n",
        "def make_small_square(dim):\n",
        "    # Set new square crop dims = original smaller crop dim\n",
        "    sm_square = crop_w1 = crop_h1 = dim\n",
        "    return sm_square\n",
        "\n",
        "# Add x% padding to bounding box dimensions\n",
        "def add_padding(dim):\n",
        "    # Add padding on all sides of square\n",
        "    padded_dim = dim + 2*percent_pad*dim\n",
        "    return padded_dim\n",
        "\n",
        "# Make square crops that are within image bounds for different scenarios\n",
        "def make_square_crops(df):\n",
        "    print(\"Before making square: \\n\", df.head())\n",
        "    start_time = time.time()\n",
        "    df['crop_height'] = round(df['ymax'] - df['ymin'], 1)\n",
        "    df['crop_width'] = round(df['xmax'] - df['xmin'], 1)\n",
        "    for i, row in df.iterrows():\n",
        "        # Define variables for use filtering data through loops below\n",
        "        crop_h0 = df['crop_height'][i]\n",
        "        crop_w0 = df['crop_width'][i]\n",
        "        #print(\"crop_h0: {}, crop_w0: {}\".format(crop_h0, crop_w0))\n",
        "        pad = percent_pad * max(crop_h0, crop_w0)  \n",
        "        global im_h, im_w\n",
        "        im_h = df.im_height[i]\n",
        "        im_w = df.im_width[i]\n",
        "        xmin0 = df.xmin[i]\n",
        "        ymin0 = df.ymin[i]\n",
        "        xmax0 = df.xmax[i]\n",
        "        ymax0 = df.ymax[i]\n",
        "        \n",
        "        # Conditions determine how rectangle bounding boxes are made square\n",
        "        cond1 = crop_h0 > crop_w0 # crop height > width\n",
        "        cond2 = crop_h0 < crop_w0 # crop width > height\n",
        "        cond3 = crop_h0 == crop_w0 # crop height = width (already square)\n",
        "\n",
        "        # Crop Height > Crop Width\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 1\n",
        "        if cond1:\n",
        "            lg_sq = make_large_square(crop_h0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_w0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "\n",
        "            # Where padded crop height is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = lg_padded_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i], df.ymin[i] = center_coords(xmin0, ymin0, crop_w0, crop_h1, im_w, im_h, pad)\n",
        "\n",
        "            # Where unpadded crop height is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is False) and (are_dims_oob(lg_sq) is True):\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = lg_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i] = xmin0 - 0.5*(min(im_h, im_w) - crop_w0)\n",
        "                df.ymin[i] = 0\n",
        "\n",
        "            # Where padded crop width is within image dimensions\n",
        "            elif (are_dims_oob(lg_sq) is False) and (are_dims_oob(sm_padded_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = sm_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i] = xmin0 - 0.5*pad\n",
        "                df.ymin[i] = ymin0 + 0.5*(crop_h0 - crop_w0) - pad   \n",
        "\n",
        "            # Where unpadded crop width is within image dimensions\n",
        "            elif (are_dims_oob(sm_padded_sq) is False) and (are_dims_oob(sm_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = sm_sq\n",
        "\n",
        "            # Where crop width and height are both OOB\n",
        "            elif are_dims_oob(sm_sq) is False:\n",
        "                # Do not crop, set values equal to image dimensions\n",
        "                df.crop_height[i] = crop_h1 = im_h \n",
        "                df.ymin[i] = 0\n",
        "                df.xmin[i] = 0 \n",
        "    \n",
        "        # Crop Width > Crop Height\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 2\n",
        "        elif cond2:\n",
        "            lg_sq = make_large_square(crop_w0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_h0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "\n",
        "            # Where padded crop width is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = lg_padded_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i], df.xmin[i] = center_coords(ymin0, xmin0, crop_w1, crop_h0, im_w, im_h, pad)\n",
        "\n",
        "            # Where unpadded crop width is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is False) and (are_dims_oob(lg_sq) is True):\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = lg_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i] = ymin0 - 0.5*(min(im_h, im_w) - crop_h0)\n",
        "                df.xmin[i] = 0\n",
        "\n",
        "            # Where padded crop height is within image dimensions\n",
        "            elif (are_dims_oob(lg_sq) is False) and (are_dims_oob(sm_padded_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = sm_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i] = ymin0 - pad\n",
        "                df.xmin[i] = xmin0 + 0.5*(crop_w0 - crop_h0) - pad   \n",
        "\n",
        "            # Where unpadded crop height is within image dimensions\n",
        "            elif (are_dims_oob(sm_padded_sq) is False) and (are_dims_oob(sm_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = sm_sq\n",
        "\n",
        "            # Where crop width and height are both OOB\n",
        "            elif are_dims_oob(sm_sq) is False:\n",
        "                # Do not crop, set values equal to image dimensions\n",
        "                df.crop_width[i] = crop_w1 = im_w\n",
        "                df.crop_height[i] = crop_h1 = im_h \n",
        "                df.ymin[i] = 0\n",
        "                df.xmin[i] = 0 \n",
        "\n",
        "        # Crop Width == Crop Height\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 3\n",
        "        elif cond3: \n",
        "            lg_sq = make_large_square(crop_w0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_h0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "        \n",
        "            # Where padded crop width/height is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:            \n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = crop_h1 = lg_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i], df.ymin[i] = center_coords(xmin0, ymin0, crop_w0, crop_w1, im_w, im_h, pad)\n",
        "                \n",
        "            # Where unpadded crop width/height is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is True) and (are_dims_oob(lg_sq) is False):\n",
        "                # Both coords not OOB, no changes needed\n",
        "                if (ymax0 <= im_h) and (xmax0 <= im_w):\n",
        "                    pass\n",
        "                \n",
        "                # Topright X coord OOB (+), shift cropping box left\n",
        "                elif (ymax0 <= im_h) and (xmax0 > im_w):  \n",
        "                    df.xmin[i] = xmin0 - (xmax0 - im_w)\n",
        "                # Topright Y coord OOB (+), shift cropping box down\n",
        "                elif (ymax0 > im_h) and (xmax0 <= im_w):\n",
        "                    df.ymin[i] = ymin0 - (ymax0 - im_h)\n",
        "                # X and Y coords OOB (+), shift cropping box down and left   \n",
        "                elif (ymax0 > im_h) and (xmax0 > im_w):\n",
        "                    df.ymin[i] = ymin0 - (ymax0 - im_h)\n",
        "                    df.xmin[i] = xmin0 - (xmax0 - im_w)\n",
        "\n",
        "    # Image coordinates should be positive, set negative xmin and ymin values to 0\n",
        "    df.xmin[df.xmin < 0] = 0\n",
        "    df.ymin[df.ymin < 0] = 0\n",
        "    print(\"Cropping coordinates, made square and with {}% padding: \\n{}\".format(percent_pad, df.head()))\n",
        "\n",
        "    # Print time to run script\n",
        "    print ('Run time: {} seconds'.format(format(time.time()- start_time, '.2f')))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Format cropping dimensions to EOL standards\n",
        "def format_crops_for_eol(df):\n",
        "# {\"height\":\"423\",\"width\":\"640\",\"crop_x\":123.712,\"crop_y\":53.4249,\"crop_width\":352,\"crop_height\":0}\n",
        "    df['crop_dimensions'] = np.nan\n",
        "    for i, row in df.iterrows():\n",
        "        df.crop_dimensions[i] = ('{{\"height\":\"{}\",\"width\":\"{}\",\"crop_x\":{},\"crop_y\":{},\"crop_width\":{},\"crop_height\":{}}}'\n",
        "        .format(df.im_height[i], df.im_width[i], df.xmin[i], df.ymin[i], df.crop_width[i], df.crop_height[i]))\n",
        "    #print(\"\\n EOL formatted cropping dimensions: \\n\", df.head())\n",
        "\n",
        "    # Add other dataframe elements from cols: identifier, dataobjectversionid, eolmediaurl, im_class, crop_dimensions\n",
        "    eol_crops = pd.DataFrame(df.iloc[:,np.r_[-5,-4,-6,0,-1]])\n",
        "    print(\"\\n EOL formatted cropping dimensions: \\n\", eol_crops.head())\n",
        "\n",
        "    return eol_crops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB4J8S7fibKU"
      },
      "source": [
        "# Make crops square and within image bounds\n",
        "\n",
        "# Optional TO DO: Pad by xx% larger crop dimension\n",
        "percent_pad = 0 #@param {type:\"slider\", min:0, max:10, step:2}\n",
        "\n",
        "# Make crops square and within bounds\n",
        "df = make_square_crops(crops_w_identifiers)\n",
        "\n",
        "# Export crop coordinates to display_test.tsv to visualize results in next code block and confirm crop transformations\n",
        "display_test_fpath = os.path.splitext(concat_outfpath)[0] + '_displaytest' + '.tsv'\n",
        "print(\"\\n File for displaying square crops on images will be saved to: \\n\", display_test_fpath)\n",
        "df.to_csv(display_test_fpath, sep='\\t', index=False)\n",
        "\n",
        "# Format image and cropping dimensions for EOL standards\n",
        "eol_crops = format_crops_for_eol(df)\n",
        "\n",
        "# Write results to tsv\n",
        "eol_crops_fpath = os.path.splitext(display_test_fpath)[0].rsplit('_',2)[0] + '_20k_final' + '.tsv'\n",
        "eol_crops.to_csv(eol_crops_fpath, columns = eol_crops.iloc[:,:-1], sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zIsiYIBiXou"
      },
      "source": [
        "## Display cropping results on images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTXPwvTaie_3"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "import cv2\n",
        "\n",
        "# Read in cropping file for displaying results\n",
        "# Note: If you just ran \"Post-process results\" above, you do not need to enter anything\n",
        "# TO DO: If you ran \"Generate crops\" during a previous session, enter the path for desired cropping file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    outfpath = \"results/lepidoptera_cropcoords_tf2_rcnn_concat_displaytest.tsv\" #@param {type:\"string\"}\n",
        "df = pd.read_csv(outfpath, sep=\"\\t\", header=0)\n",
        "print(df.head())\n",
        "\n",
        "# For uploading an image from url\n",
        "# Modified from https://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/\n",
        "def url_to_image(url):\n",
        "    resp = urllib.request.urlopen(url)\n",
        "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    im_h, im_w = image.shape[:2]\n",
        " \n",
        "    return image\n",
        "\n",
        "# Draw cropping box on image\n",
        "def draw_box_on_image(df, img):\n",
        "    # Get box coordinates\n",
        "    xmin = df['xmin'][i].astype(int)\n",
        "    ymin = df['ymin'][i].astype(int)\n",
        "    xmax = df['xmin'][i].astype(int) + df['crop_width'][i].astype(int)\n",
        "    ymax = df['ymin'][i].astype(int) + df['crop_height'][i].astype(int)\n",
        "    boxcoords = [xmin, ymin, xmax, ymax]\n",
        "\n",
        "    # Set box/font color and size\n",
        "    maxdim = max(df['im_height'][i],df['im_width'][i])\n",
        "    fontScale = maxdim/600\n",
        "    box_col = (255, 0, 157)\n",
        "  \n",
        "    # Add label to image\n",
        "    tag = df['class_name'][i]\n",
        "    image_wbox = cv2.putText(img, tag, (xmin+7, ymax-12), cv2.FONT_HERSHEY_SIMPLEX, fontScale, box_col, 2, cv2.LINE_AA)  \n",
        "  \n",
        "    # Draw box label on image\n",
        "    image_wbox = cv2.rectangle(img, (xmin, ymax), (xmax, ymin), box_col, 5)\n",
        "\n",
        "    return image_wbox, boxcoords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvexyAOfig8M"
      },
      "source": [
        "# Display crop dimensions on images\n",
        "\n",
        "# TO DO: Adjust line below to see up to 50 images displayed at a time\n",
        "start = 0 #@param {type:\"slider\", min:0, max:5000, step:50}\n",
        "stop = start+50\n",
        "\n",
        "# Loop through images\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "    # Read in image \n",
        "    url = df['eolMediaURL'][i]\n",
        "    img = url_to_image(url)\n",
        "  \n",
        "    # Draw bounding box on image\n",
        "    image_wbox, boxcoords = draw_box_on_image(df, img)\n",
        "  \n",
        "    # Plot cropping box on image\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(image_wbox)\n",
        "\n",
        "    # Display image URL and coordinatesabove image\n",
        "    # Helps with fine-tuning data transforms in post-processing steps above\n",
        "    plt.title('{} \\n xmin: {}, ymin: {}, xmax: {}, ymax: {}'.format(url, boxcoords[0], boxcoords[1], boxcoords[2], boxcoords[3]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}