{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_generate_crops_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrXhn1qKWm_"
      },
      "source": [
        "# Use Faster-RCNN ResNet 50 and Inception v2 in Tensorflow to automatically crop images of snakes & lizards (Squamata), beetles (Coleoptera), frogs (Anura), and carnivores (Carnivora)\n",
        "---   \n",
        "*Last Updated 1 February 2025*  \n",
        "-Now runs in Python 3 with Tensorflow 2.0-     \n",
        "\n",
        "Use trained object detection models to automatically crop images of snakes & lizards (Squamata), beetles (Coleoptera), frogs (Anura), and carnivores (Carnivora) to square dimensions centered around animal(s).\n",
        "\n",
        "Models were trained and saved to Google Drive in [multitaxa_train_tf2_rcnns.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_tf2_rcnns.ipynb).\n",
        "\n",
        "***Models were trained in Python 2 and TF 1 in April 2020: Faster RCNN ResNet 50 trained for 12 hours to 200,000 steps and Faster RCNN Inception v2 for 18 hours to 200,000 steps.***\n",
        "\n",
        "Notes:   \n",
        "* Run code blocks by pressing play button in brackets on left\n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change parameters using form fields on right (find details at corresponding lines of code by searching '#@param')\n",
        "\n",
        "References:     \n",
        "* [Official Tensorflow Object Detection API Instructions](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)   \n",
        "* [Medium Blog on training using Tensorflow Object Detection API in Colab](https://medium.com/analytics-vidhya/training-an-object-detection-model-with-tensorflow-api-using-google-colab-4f9a688d5e8b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQWTwI7k4Bf"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvpJJUDTkct3"
      },
      "outputs": [],
      "source": [
        "#@title Choose where to save results\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Note: You can modify \"filter\" to choose detection results for any class of interest the model is trained on\n",
        "filter = \"Multitaxa\" # @param [\"Multitaxa\"] {\"allow-input\":true}\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "basewd = \"/content/drive/MyDrive/train\" #@param [\"/content/drive/MyDrive/train\"] {allow-input: true}\n",
        "# Type in the folder that you want to contain TF2 files\n",
        "folder = \"tf2\" #@param [\"tf2\"] {allow-input: true}\n",
        "# Define current working directory using form field inputs\n",
        "cwd = basewd + '/' + folder\n",
        "\n",
        "# Install dependencies\n",
        "!pip3 install --upgrade gdown\n",
        "!gdown 1-0F-zKYOAV1qk2hu7kqKvlMo5Xsj6Uut # Download helper_funcs folder\n",
        "!tar -xzvf multitaxa_helper_funcs.tar.gz -C .\n",
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO figure out requirements.txt and fix warnings\n",
        "#!pip install -r requirements.txt\n",
        "!pip install numpy==1.24.3\n",
        "!pip install protobuf==3.20.3"
      ],
      "metadata": {
        "id": "YfUAlMlaTuaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose model parameters, set up directory structure, and build Tensorflow Object Detection API\n",
        "\n",
        "# Use EOL pre-trained model for object detection?\n",
        "use_EOL_model = True #@param {type: \"boolean\"}\n",
        "\n",
        "# If using your own trained model, change values to match your trained model\n",
        "filters = [\"Anura\", \"Carnivora\", \"Coleoptera\", \"Squamata\"] #@param [\"[\\\"Anura\\\", \\\"Carnivora\\\", \\\"Coleoptera\\\", \\\"Squamata\\\"]\"] {type:\"raw\", allow-input: true}\n",
        "PATH_TO_LABELS = \"labelmap.pbtxt\" #@param {type:\"string\"}\n",
        "NUM_CLASSES = 4 #@param\n",
        "saved_models_dir = \"tf_models/train_demo/rcnn_i/finetuned_model/\" #@param [\"tf_models/train_demo/rcnn/finetuned_model/\"] {allow-input: true}\n",
        "mod_abbv = \"rcnn_i\"\n",
        "\n",
        "# For working with directories\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "# For downloading and displaying images\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "# For object detection\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Import EOL custom helper_funcs\n",
        "from setup import *\n",
        "from wrangle_data import *\n",
        "\n",
        "# Clone Tensorflow Object Detection Github Repo\n",
        "setup_dirs(cwd)\n",
        "\n",
        "# Build Tensorflow Object Detection API\n",
        "!sudo apt install -y protobuf-compiler\n",
        "%cd $cwd\n",
        "!cd tf_models/models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ],
      "metadata": {
        "id": "a1EA_hX0YhTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build saved model\n",
        "\n",
        "# For object detection\n",
        "import sys\n",
        "#sys.path.append(\"tf_models/models/research/\")\n",
        "sys.path.append('/content')\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# For downloading and displaying images\n",
        "import cv2\n",
        "import tempfile\n",
        "import urllib\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "\n",
        "# For drawing onto images\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For working with data\n",
        "import subprocess\n",
        "import csv\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('\\nTensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())\n",
        "\n",
        "# Unpack EOL saved model\n",
        "PATH_TO_CKPT = saved_models_dir + 'frozen_inference_graph.pb'\n",
        "detector = detection_graph = unpack_EOL_model(use_EOL_model, saved_models_dir, PATH_TO_CKPT, cwd)\n",
        "\n",
        "# Load saved model and label map\n",
        "print(\"\\nLoading label map for {} class(es) from: \\n{}\".format(NUM_CLASSES, PATH_TO_LABELS))\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)"
      ],
      "metadata": {
        "id": "YdMT7PhvlOcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcJET5z5CkBr"
      },
      "source": [
        "## Generate crops: Run inference on EOL images & save resulting coordinates for cropping - Run 4X for batches A-D\n",
        "---\n",
        "Use 20K EOL image bundle to generate bounding boxes around each object with pre-trained object detection models. Results are saved to [crops_file].tsv. Run this section 4 times (to make batches A-D) of 5K images each to incrementally save in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5bi08_cTpYm",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "#@title Define functions\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the maximum number of detections to keep per image\n",
        "max_boxes = 10 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "# Set the minimum confidence score for detections to keep per image\n",
        "min_score = 0.6 #@param {type:\"slider\", min:0, max:0.9, step:0.1}\n",
        "\n",
        "# Set filename for saving classification results\n",
        "def set_outpath(crops_file, cwd):\n",
        "    outpath = cwd + '/' + 'results/' + crops_file.rsplit('_',1)[0] + '_' + mod_abbv + '_' + crops_file.rsplit('_',1)[1] + '.tsv'\n",
        "    print(\"\\nSaving results to: \\n\", outpath)\n",
        "\n",
        "    return outpath\n",
        "\n",
        "# Export object detection results\n",
        "def export_results(image_url, result, outfpath, im_h, im_w, filter=filters):\n",
        "    with open(outfpath, 'a') as out_file:\n",
        "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "        img_id = os.path.splitext((os.path.basename(image_url)))[0]\n",
        "        # Write one row per detected object with bounding box coordinates\n",
        "        num_detections = min(int(result[\"num_detections\"][0]), max_boxes)\n",
        "        for i in range(0, num_detections):\n",
        "            class_name = category_index[result[\"detection_classes\"][0][i]]['name']\n",
        "            if any(fil in class_name for fil in filters): # Only writes rows for filtered class\n",
        "                ymin = result[\"detection_boxes\"][0][i][0]\n",
        "                xmin = result[\"detection_boxes\"][0][i][1]\n",
        "                ymax = result[\"detection_boxes\"][0][i][2]\n",
        "                xmax = result[\"detection_boxes\"][0][i][3]\n",
        "                confidence = result[\"detection_scores\"][0][i]\n",
        "                tsv_writer.writerow([img_id, class_name, confidence,\n",
        "                          xmin, ymin, xmax, ymax, im_h, im_w, image_url])\n",
        "        print(\"\\nObject detection results for Image {} saved to: {}\".format(image_url, outfpath))\n",
        "\n",
        "    return img_id\n",
        "\n",
        "# Format cropping dimensions to EOL standards\n",
        "def format_crops_for_eol(df):\n",
        "# {\"height\":\"423\",\"width\":\"640\",\"crop_x\":123.712,\"crop_y\":53.4249,\"crop_width\":352,\"crop_height\":0}\n",
        "    df['crop_dimensions'] = np.nan\n",
        "    for i, row in df.iterrows():\n",
        "        df.loc[i, 'crop_dimensions'] = ('{{\"height\":\"{}\",\"width\":\"{}\",\"crop_x\":{},\"crop_y\":{},\"crop_width\":{},\"crop_height\":{}}}'\n",
        "        .format(df.im_height[i], df.im_width[i], df.xmin[i], df.ymin[i], df.crop_width[i], df.crop_height[i]))\n",
        "\n",
        "    # Add other dataframe elements from cols: identifier, dataobjectversionid, eolmediaurl, im_class, crop_dimensions\n",
        "    eol_crops = pd.DataFrame(df.iloc[:,np.r_[-5,-4,-6,0,-1]])\n",
        "    print(\"\\n EOL formatted cropping dimensions: \\n\", eol_crops.head())\n",
        "\n",
        "    return eol_crops\n",
        "\n",
        "print('Model loaded and functions defined! \\nGo to next steps to run inference on images.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVpMaZ3on72u"
      },
      "source": [
        "### Generate crops: Run inference on EOL images & save results for cropping - Run 4X for batches A-D\n",
        "Use 20K EOL Anura, Carnivora, Coleoptera, Squamata image bundle to get bounding boxes of detected bats. Results are saved to [crops_file].tsv. Run this section 4 times (to make batches A-D) of 5K images each to incrementally save in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHauEgaTVIEO"
      },
      "outputs": [],
      "source": [
        "#@title Enter EOL image bundle and choose inference settings (change **crops_file** for each batch A-D)\n",
        "\n",
        "# Load in EOL image bundle\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\" #@param [\"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Coleoptera_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Anura_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Carnivora_20K_breakdown_download_000001.txt\"]\n",
        "df = read_datafile(bundle, sep='\\t', header=None, disp_head=False)\n",
        "df.columns = ['url']\n",
        "print('\\n EOL image bundle head:\\n{}'.format(df.head()))\n",
        "\n",
        "# Test pipeline with a smaller subset than 5k images?\n",
        "run = \"test with tiny subset\" #@param [\"test with tiny subset\", \"for all images\"]\n",
        "\n",
        "# Display detection results on images?\n",
        "if 'tiny subset' in run:\n",
        "    display_results = True\n",
        "else:\n",
        "    display_results = False\n",
        "\n",
        "# Take 5k subset of bundle for running inference\n",
        "# Change filename for each batch\n",
        "crops_file = \"multitaxa_cropcoords_tf2_a\" #@param [\"multitaxa_cropcoords_tf2_a\", \"multitaxa_cropcoords_tf2_b\", \"multitaxa_cropcoords_tf2_c\", \"multitaxa_cropcoords_tf2_d\"] {allow-input: true}\n",
        "outfpath = set_outpath(crops_file, cwd)\n",
        "\n",
        "# Write header row of output tag file\n",
        "if not os.path.isfile(outfpath):\n",
        "    with open(outfpath, 'a') as out_file:\n",
        "              tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "              tsv_writer.writerow([\"img_id\", \"class_name\", \"confidence\",\n",
        "                                   \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"im_width\", \\\n",
        "                                   \"im_height\", \"url\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: Refactor draw bounding boxes on an image; multitaxa has errors\n",
        "# To draw bounding boxes on an image\n",
        "# Modified from TF Hub https://www.tensorflow.org/hub/tutorials/object_detection\n",
        "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax,\n",
        "                               color, font, thickness=4, display_str_list=()):\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    im_width, im_height = image.size\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                 ymin * im_height, ymax * im_height)\n",
        "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "              (left, top)],\n",
        "              width=thickness,\n",
        "              fill=color)\n",
        "\n",
        "    # Adjust display string placement if out of bounds\n",
        "    display_str_heights = [font.getbbox(ds)[3]-font.getbbox(ds)[1] for ds in display_str_list]\n",
        "    # Each display_str has a top and bottom margin of 0.05x.\n",
        "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "    if top > total_display_str_height:\n",
        "        text_bottom = top\n",
        "    else:\n",
        "        text_bottom = top + total_display_str_height\n",
        "    # Reverse list and print from bottom to top.\n",
        "    for ds in display_str_list[::-1]:\n",
        "        text_height = font.getbbox(ds)[3] - font.getbbox(ds)[1]\n",
        "        text_width = font.getbbox(ds)[2] - font.getbbox(ds)[0]\n",
        "        margin = np.ceil(0.05 * text_height)\n",
        "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "        draw.text((left + margin, text_bottom - text_height - margin),\n",
        "                  ds, fill=\"black\", font=font)\n",
        "        text_bottom -= text_height - 2 * margin\n",
        "\n",
        "# Filter detections and annotate images with results\n",
        "# Modified from TF Hub https://www.tensorflow.org/hub/tutorials/object_detection\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes, min_score, filter, label_map, category_index):\n",
        "    # Format text above boxes\n",
        "    colors = list(ImageColor.colormap.values())\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\", 25)\n",
        "    except IOError:\n",
        "        print(\"Font not found, using default font.\")\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # Draw up to N-max boxes with confidence > score threshold\n",
        "    for i in range(0, max_boxes):\n",
        "        if scores[0][i] >= min_score:\n",
        "            ymin, xmin, ymax, xmax = tuple(boxes[0][i])\n",
        "            display_str = \"{}: {}%\".format(category_index[class_names[0][i]]['name'],\n",
        "                                     int(100 * scores[0][i]))\n",
        "            color = colors[hash(class_names[0][i]) % len(colors)]\n",
        "            image_pil = Image.fromarray(np.squeeze(image))\n",
        "            # Only the filtered class is shown on images\n",
        "            if any(fil in display_str for fil in filters):\n",
        "                draw_bounding_box_on_image(\n",
        "                    image_pil,\n",
        "                    ymin, xmin, ymax, xmax,\n",
        "                    color, font, display_str_list=[display_str])\n",
        "                np.copyto(image, np.array(image_pil))\n",
        "    return image[0]\n",
        "\n",
        "# For running inference\n",
        "# Modified from TF Hub https://www.tensorflow.org/hub/tutorials/object_detection\n",
        "def run_detector_tf(detection_graph, image_url, outfpath, filter, label_map, max_boxes, min_score, category_index):\n",
        "    image_np, im_h, im_w = url_to_image(image_url)\n",
        "    with detection_graph.as_default():\n",
        "        with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
        "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "            # Actual detection\n",
        "            start_time = time.time()\n",
        "            result = sess.run([detection_boxes, detection_scores,\n",
        "                               detection_classes, num_detections],\n",
        "                               feed_dict={image_tensor: image_np})\n",
        "            end_time = time.time()\n",
        "\n",
        "            result = {\"detection_boxes\": result[0], \"detection_scores\": result[1],\n",
        "                      \"detection_classes\": result[2], \"num_detections\": result[3]}\n",
        "\n",
        "            print(\"Found %d objects with > %s confidence\" % (min(result[\"num_detections\"], max_boxes), min_score))\n",
        "            print(\"Inference time: %s sec\" % format(end_time-start_time, '.2f'))\n",
        "\n",
        "            # Draw detection boxes on image\n",
        "            #image_wboxes = draw_boxes(image_np, result[\"detection_boxes\"],\n",
        "                                      #result[\"detection_classes\"], result[\"detection_scores\"],\n",
        "                                      #max_boxes, min_score, filter, label_map, category_index)\n",
        "\n",
        "    #return image_wboxes, result, im_h, im_w\n",
        "    return result, im_h, im_w"
      ],
      "metadata": {
        "id": "R9EOsgJmAK0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pERgrECWgNRy"
      },
      "outputs": [],
      "source": [
        "#@title Run inference on image batches A-D\n",
        "\n",
        "# Run EOL bundle images through trained model and save results\n",
        "print(\"Running inference on images\")\n",
        "all_predictions = []\n",
        "start, stop, cutoff = set_start_stop(run, df)\n",
        "start = 0\n",
        "stop = 5\n",
        "for i, row in enumerate(df.iloc[start:stop].iterrows()):\n",
        "    try:\n",
        "        # Run image through object detector and export result\n",
        "        image_url = df['url'][row[0]]\n",
        "        #image_wboxes, result, im_h, im_w = run_detector_tf(detector, image_url, outfpath, filters, label_map, max_boxes, min_score, category_index)\n",
        "        result, im_h, im_w = run_detector_tf(detector, image_url, outfpath, filters, label_map, max_boxes, min_score, category_index)\n",
        "        img_id = export_results(image_url, result, outfpath, im_h, im_w)\n",
        "\n",
        "        # Optional: Display detections on images\n",
        "        #if (i+1<=50) and display_results:\n",
        "            #display_image(image_wboxes)\n",
        "\n",
        "        # Display progress message after each image\n",
        "        all_predictions.append(img_id)\n",
        "        print('\\033[92m {}) Inference complete for image {} of {} \\033[0m \\n'.format(i+1, i+1, cutoff))\n",
        "        #if len(all_predictions)>=cutoff:\n",
        "              #break\n",
        "\n",
        "    except:\n",
        "        print('Check if URL from {} is valid\\n'.format(df['url'][i]))\n",
        "\n",
        "print(\"\\n\\n~~~\\n\\033[92m Inference complete!\\033[0m \\033[93m Run these steps for remaining batches A-D before proceeding.\\033[0m\\n~~~\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VSLfSiGbW_S"
      },
      "source": [
        "## Post-process detection results\n",
        "---\n",
        "Combine output files for batches A-D. Then, convert detection boxes into square, centered thumbnail cropping coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqSnb1SabbAd"
      },
      "outputs": [],
      "source": [
        "#@title Merge 5k image batch output files A-D\n",
        "\n",
        "# Enter path to any inference result batch file A-D\n",
        "\n",
        "# If you just ran \"Generate crops\" above, you do not need to enter anything\n",
        "# If you ran \"Generate crops\" during a previous session, enter the path for ONE output file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    crops_file = \"multitaxa_cropcoords_tf2_a\" #@param [\"multitaxa_cropcoords_tf2_a\", \"multitaxa_cropcoords_tf2_b\", \"multitaxa_cropcoords_tf2_c\", \"multitaxa_cropcoords_tf2_d\"] {allow-input: true}\n",
        "    outfpath = set_outpath(crops_file, cwd)\n",
        "\n",
        "# Combine 4 batches of detection box coordinates to one dataframe\n",
        "basewd =  os.path.splitext(outfpath)[0].rsplit('_',1)[0] + '_'\n",
        "exts = ['a.tsv', 'b.tsv', 'c.tsv', 'd.tsv']\n",
        "all_filenames = [basewd + e for e in exts]\n",
        "df = pd.concat([pd.read_csv(f, sep='\\t', header=0, na_filter = False) for f in all_filenames], ignore_index=True)\n",
        "\n",
        "# Write results to tsv\n",
        "concat_outfpath = basewd + 'concat.tsv'\n",
        "df.to_csv(concat_outfpath, sep='\\t', index=False)\n",
        "print(\"New concatenated dataframe with all 4 batches saved to: {} \\n{}\".format(concat_outfpath, df.head()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6C68mM_c7Uz"
      },
      "outputs": [],
      "source": [
        "#@title Combine individual detection boxes into one \"superbox\" per image\n",
        "\n",
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "\n",
        "# Read in crop file exported from \"Combine output files A-D\" block above\n",
        "crops = read_datafile(concat_outfpath, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# De-normalize cropping coordinates to pixel values\n",
        "crops = denormalize_coords(crops)\n",
        "\n",
        "# Make 1 superbox per image [coordinates: bottom left (smallest xmin, ymin) and top right (largest xmax, ymax)]\n",
        "superboxes = make_superboxes(crops)\n",
        "\n",
        "# Read in EOL image \"breakdown\" bundle dataframe from \"breakdown_download\" bundle used for cropping\n",
        "if 'bundle' not in locals() or globals():\n",
        "    bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\" #@param [\"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Coleoptera_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Anura_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Carnivora_20K_breakdown_download_000001.txt\"]\n",
        "breakdown = bundle.replace(\"download_\", \"\") # Get EOL breakdown bundle url from \"breakdown_download\" address\n",
        "bundle_info = read_datafile(breakdown, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "crops_w_identifiers = add_identifiers(superboxes, bundle_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW0pjR3_HeYg"
      },
      "outputs": [],
      "source": [
        "#@title Make superbox square and within image bounds (Optional: add padding)\n",
        "\n",
        "# Pad by xx% larger crop dimension\n",
        "pad = 2 #@param {type:\"slider\", min:0, max:10, step:2}\n",
        "pad = pad/100 # Convert to percentage\n",
        "\n",
        "# Make crops square and within bounds\n",
        "df = make_square_crops(crops_w_identifiers, pad)\n",
        "\n",
        "# Export crop coordinates to display_test.tsv to visualize results in next code block and confirm crop transformations\n",
        "display_test_fpath = os.path.splitext(concat_outfpath)[0] + '_displaytest' + '.tsv'\n",
        "print(\"\\n File for displaying square crops on images will be saved to: \\n\", display_test_fpath)\n",
        "df.to_csv(display_test_fpath, sep='\\t', index=False)\n",
        "\n",
        "# Format image and cropping dimensions for EOL standards\n",
        "eol_crops = format_crops_for_eol(df)\n",
        "\n",
        "# Write results to tsv\n",
        "eol_crops_fpath = os.path.splitext(display_test_fpath)[0].rsplit('_',2)[0] + '_20k_final' + '.tsv'\n",
        "eol_crops.to_csv(eol_crops_fpath, columns = eol_crops.iloc[:,:-1], sep='\\t', index=False)\n",
        "print(\"EOL formatted crops dataset saved to: {} \\n{}\".format(eol_crops_fpath, eol_crops.head()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsFGmd2PbiCg"
      },
      "source": [
        "## Display cropping results on images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h28VTCzsboqE"
      },
      "outputs": [],
      "source": [
        "#@title Read in cropping file and display results on images\n",
        "from wrangle_data import *\n",
        "import cv2\n",
        "\n",
        "# If you just ran \"Post-process results\" above, you do not need to enter anything\n",
        "# If you ran \"Generate crops\" during a previous session, enter the path for desired cropping file\n",
        "if 'display_test_fpath' not in locals() or globals():\n",
        "    crops_file = \"multitaxa_cropcoords_tf2_a\" #@param [\"multitaxa_cropcoords_tf2_a\", \"multitaxa_cropcoords_tf2_b\", \"multitaxa_cropcoords_tf2_c\", \"multitaxa_cropcoords_tf2_d\"] {allow-input: true}\n",
        "    outfpath = set_outpath(crops_file, cwd)\n",
        "    display_test_fpath =  os.path.splitext(outfpath)[0].rsplit('_',1)[0] + '_concat_displaytest' + '.tsv'\n",
        "    print(display_test_fpath)\n",
        "df = pd.read_csv(display_test_fpath, sep=\"\\t\", header=0)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwoIgqtgbr6U"
      },
      "outputs": [],
      "source": [
        "#@title Choose starting index for crops to display\n",
        "\n",
        "# Adjust line to right to see up to 50 images displayed at a time\n",
        "start = 0 #@param {type:\"slider\", min:0, max:5000, step:50}\n",
        "stop = start+50\n",
        "\n",
        "# Loop through images\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "    # Read in image\n",
        "    url = df['eolMediaURL'][i]\n",
        "    image, im_h, im_w = url_to_image(url)\n",
        "\n",
        "    # Draw bounding box on image\n",
        "    image_wbox, boxcoords = draw_box_on_image(df, i, image, filters)\n",
        "\n",
        "    # Plot cropping box on image\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(image_wbox)\n",
        "\n",
        "    # Display image URL and coordinatesabove image\n",
        "    # Helps with fine-tuning data transforms in post-processing steps above\n",
        "    plt.title('{} \\n xmin: {}, ymin: {}, xmax: {}, ymax: {}'.format(url, boxcoords[0], boxcoords[1], boxcoords[2], boxcoords[3]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}