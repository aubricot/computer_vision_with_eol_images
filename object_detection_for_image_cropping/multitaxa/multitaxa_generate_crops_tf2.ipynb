{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_generate_crops_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrXhn1qKWm_"
      },
      "source": [
        "# Use Faster-RCNN ResNet 50 and Inception v2 in Tensorflow to automatically crop images of snakes & lizards (Squamata), beetles (Coleoptera), frogs (Anura), and carnivores (Carnivora)\n",
        "---   \n",
        "*Last Updated 15 March 2023*  \n",
        "-Now runs in Python 3 with Tensorflow 2.0-     \n",
        "\n",
        "Use trained object detection models to automatically crop images of snakes & lizards (Squamata), beetles (Coleoptera), frogs (Anura), and carnivores (Carnivora) to square dimensions centered around animal(s). \n",
        "\n",
        "Models were trained and saved to Google Drive in [multitaxa_train_tf2_rcnns.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_tf2_rcnns.ipynb).\n",
        "\n",
        "***Models were trained in Python 2 and TF 1 in April 2020: Faster RCNN ResNet 50 trained for 12 hours to 200,000 steps and Faster RCNN Inception v2 for 18 hours to 200,000 steps.***\n",
        "\n",
        "Notes:   \n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change parameters using form fields on right (/where you see 'TO DO' in code)\n",
        "* For each 24 hour period on Google Colab, you have up to 12 hours of free GPU access. \n",
        "\n",
        "References:     \n",
        "* [Official Tensorflow Object Detection API Instructions](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)   \n",
        "* [Medium Blog on training using Tensorflow Object Detection API in Colab](https://medium.com/analytics-vidhya/training-an-object-detection-model-with-tensorflow-api-using-google-colab-4f9a688d5e8b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQWTwI7k4Bf"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mAC7PfUrWX1"
      },
      "source": [
        "#@title Choose where to save results & set up directory structure\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "basewd = \"/content/drive/MyDrive/train\" #@param [\"/content/drive/MyDrive/train\"] {allow-input: true}\n",
        "# Type in the folder that you want to contain TF2 files\n",
        "folder = \"tf2\" #@param [\"tf2\"] {allow-input: true}\n",
        "# Define current working directory using form field inputs\n",
        "cwd = basewd + '/' + folder\n",
        "\n",
        "# Install dependencies\n",
        "!pip3 install --upgrade gdown\n",
        "!gdown 1fIEf387CNrWk0ziPY-ltvwN9VrRXrRkY # Download helper_funcs folder\n",
        "!tar -xzvf helper_funcs.tar.gz -C .\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up directory structure and clone TF Object Detection API\n",
        "!python setup.py $cwd\n",
        "\n",
        "# Build TF Object Detection API\n",
        "%cd $cwd\n",
        "!cd tf_models/models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install ."
      ],
      "metadata": {
        "id": "h961A_1sPM28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlg-GTnKKRa3"
      },
      "source": [
        "# For object detection\n",
        "import tensorflow as tf \n",
        "import tensorflow_hub as hub\n",
        "import sys\n",
        "sys.path.append(\"tf_models/models/research/\")\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# For downloading and displaying images\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import tempfile\n",
        "import urllib\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "\n",
        "# For drawing onto images\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For working with data\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import pathlib\n",
        "import csv\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('\\nTensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGx_08UcmtOF"
      },
      "source": [
        "## Generate cropping coordinates for images\n",
        "---\n",
        "Run EOL 20k image bundles through pre-trained object detection models and save results in 4 batches (A-D). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQpr26hJOv5y"
      },
      "source": [
        "### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n_alUkLZ1gl",
        "cellView": "code"
      },
      "source": [
        "#@title Choose model and parameters for running inference\n",
        "%matplotlib inline\n",
        "sys.path.append('/content')\n",
        "from wrangle_data import read_datafile, display_image\n",
        "\n",
        "# Use EOL pre-trained model for object detection or your own custom trained model?\n",
        "model = \"pre-trained EOL model\" #@param [\"pre-trained EOL model\", \"my custom model\"]\n",
        "\n",
        "# Use EOL pre-trained model & download needed files\n",
        "if 'EOL' in model: \n",
        "    PATH_TO_CKPT = 'tf_models/train_demo/rcnn_i/finetuned_model' + '/frozen_inference_graph.pb'\n",
        "    if not os.path.exists(PATH_TO_CKPT):\n",
        "        # Download labelmap.pbtxt\n",
        "        !gdown 1DAwX6gj77r3YHLEgaTJgfn8VMLIf3ruw\n",
        "        # Download frozen_inference_graph.pb\n",
        "        !mkdir -p tf_models/train_demo/rcnn_i/finetuned_model\n",
        "        %cd tf_models/train_demo/rcnn_i/finetuned_model\n",
        "        !gdown 1hb4LI9nq1eHGQZ1d84Rj4VQpeu0UUPQZ\n",
        "        PATH_TO_CKPT = 'tf_models/train_demo/rcnn_i/finetuned_model' + '/frozen_inference_graph.pb'\n",
        "        %cd $cwd\n",
        "\n",
        "# Use your own custom trained model\n",
        "else:\n",
        "    # Change path to saved model checkpoint\n",
        "    my_custom_model_path = \"tf_models/train_demo/rcnn_i/finetuned_model\" #@param {type:\"string\"}\n",
        "    PATH_TO_CKPT = my_custom_model_path + '/frozen_inference_graph.pb'\n",
        "\n",
        "# Label Map for model (maps model output # to text label)\n",
        "# Note: You can modify \"filter\" to choose detection results for any class of interest the model is trained on\n",
        "filters = [\"Anura\", \"Carnivora\", \"Coleoptera\", \"Squamata\"] #@param [\"[\\\"Anura\\\", \\\"Carnivora\\\", \\\"Coleoptera\\\", \\\"Squamata\\\"]\"] {type:\"raw\", allow-input: true}\n",
        "PATH_TO_LABELS = \"labelmap.pbtxt\" #@param {type:\"string\"}\n",
        "NUM_CLASSES = 4 #@param\n",
        "\n",
        "# Define functions\n",
        "\n",
        "# Restore frozen detection graph (trained model)    \n",
        "print(\"\\nLoading trained model from: \\n\", PATH_TO_CKPT)\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.compat.v1.GraphDef()\n",
        "    with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "print(\"\\nLoading label map for {} class(es) from: \\n{}\".format(NUM_CLASSES, PATH_TO_LABELS))        \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# For handling bounding boxes\n",
        "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax,\n",
        "                               color, font, thickness=4, display_str_list=()):\n",
        "    \"\"\"Adds a bounding box to an image.\"\"\"\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    im_width, im_height = image.size\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)], width=thickness, fill=color)\n",
        "\n",
        "    # Adjust display string placement if out of bounds\n",
        "    display_str_heights = [font.getbbox(ds)[3]-font.getbbox(ds)[1] for ds in display_str_list]\n",
        "    # Each display_str has a top and bottom margin of 0.05x.\n",
        "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "    if top > total_display_str_height:\n",
        "        text_bottom = top\n",
        "    else:\n",
        "        text_bottom = top + total_display_str_height\n",
        "    # Reverse list and print from bottom to top.\n",
        "    for ds in display_str_list[::-1]:\n",
        "        text_height = font.getbbox(ds)[3] - font.getbbox(ds)[1]\n",
        "        text_width = font.getbbox(ds)[2] - font.getbbox(ds)[0]\n",
        "        margin = np.ceil(0.05 * text_height)\n",
        "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "        draw.text((left + margin, text_bottom - text_height - margin),\n",
        "                  ds, fill=\"black\", font=font)\n",
        "        text_bottom -= text_height - 2 * margin\n",
        "\n",
        "# TO DO: Set the maximum number of detections to keep per image\n",
        "max_boxes = 10 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "# TO DO: Set the minimum confidence score for detections to keep per image\n",
        "min_score = 0.1 #@param {type:\"slider\", min:0, max:0.9, step:0.1}\n",
        "\n",
        "def draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n",
        "    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "    if max_boxes:\n",
        "        max_boxes = max_boxes\n",
        "    if min_score:\n",
        "        min_score = min_score\n",
        "    colors = list(ImageColor.colormap.values())\n",
        "\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "    except IOError:\n",
        "        print(\"Font not found, using default font.\")\n",
        "        font = ImageFont.load_default()\n",
        "    # Draw up to N-max boxes with confidence > score threshold\n",
        "    for i in range(0, max_boxes):\n",
        "        if scores[0][i] >= min_score:\n",
        "            ymin, xmin, ymax, xmax = tuple(boxes[0][i])\n",
        "            display_str = \"{}: {}%\".format(category_index[class_names[0][i]]['name'],\n",
        "                                     int(100 * scores[0][i]))\n",
        "            color = colors[hash(class_names[0][i]) % len(colors)]\n",
        "            image_pil = Image.fromarray(np.squeeze(image))\n",
        "            \n",
        "        # Only the filtered class is shown on images\n",
        "        if any(fil in display_str for fil in filters):\n",
        "            draw_bounding_box_on_image(\n",
        "                image_pil,\n",
        "                ymin, xmin, ymax, xmax,\n",
        "                color, font, display_str_list=[display_str])\n",
        "            np.copyto(image, np.array(image_pil))\n",
        "\n",
        "    return image[0]\n",
        "\n",
        "# For uploading an image from url\n",
        "# Modified from https://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/\n",
        "def url_to_image(url):\n",
        "    resp = urllib.request.urlopen(url)\n",
        "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_np = np.expand_dims(image, axis=0)\n",
        "    im_h, im_w = image.shape[:2]\n",
        "  \n",
        "    return image_np, im_h, im_w\n",
        "\n",
        "# Define start and stop indices in EOL bundle for running inference   \n",
        "def set_start_stop(run):\n",
        "    # To test with a tiny subset, use 5 random bundle images\n",
        "    if \"tiny subset\" in run:\n",
        "        start=np.random.choice(a=1000, size=1)[0]\n",
        "        stop=start+5\n",
        "    # To run inference on 4 batches of 5k images each\n",
        "    elif \"_a.\" in outfpath: # batch a is from 0-5000\n",
        "        start=0\n",
        "        stop=5000\n",
        "    elif \"_b.\" in outfpath: # batch b is from 5000-1000\n",
        "        start=5000\n",
        "        stop=10000\n",
        "    elif \"_c.\" in outfpath: # batch c is from 10000-15000\n",
        "        start=10000\n",
        "        stop=15000\n",
        "    elif \"_d.\" in outfpath: # batch d is from 15000-20000\n",
        "        start=15000\n",
        "        stop=20000\n",
        "    \n",
        "    return start, stop\n",
        "\n",
        "# For running inference\n",
        "def run_detector_tf(image_url):\n",
        "    image_np, im_h, im_w = url_to_image(image_url)\n",
        "    with detection_graph.as_default():\n",
        "        with tf.compat.v1.Session(graph=detection_graph) as sess:\n",
        "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "            # Actual detection\n",
        "            start_time = time.time()\n",
        "            result = sess.run([detection_boxes, detection_scores, \n",
        "                               detection_classes, num_detections],\n",
        "                               feed_dict={image_tensor: image_np})\n",
        "            end_time = time.time()\n",
        "            \n",
        "            result = {\"detection_boxes\": result[0], \"detection_scores\": result[1],\n",
        "                      \"detection_classes\": result[2], \"num_detections\": result[3]}\n",
        "            print(\"Found %d objects with > %s confidence\" % (min(result[\"num_detections\"], max_boxes), min_score))\n",
        "            print(\"Inference time: %s sec\" % format(end_time-start_time, '.2f'))\n",
        "      \n",
        "            # Draw detection boxes on image\n",
        "            image_with_boxes = draw_boxes(image_np, result[\"detection_boxes\"],\n",
        "                                  result[\"detection_classes\"], result[\"detection_scores\"])\n",
        "\n",
        "            # Export bounding boxes to file in Google Drive\n",
        "            with open(outfpath, 'a') as out_file:\n",
        "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                img_id = os.path.splitext((os.path.basename(image_url)))[0]\n",
        "                # Write one row per detected object with bounding box coordinates\n",
        "                num_detections = min(int(result[\"num_detections\"][0]), max_boxes)\n",
        "                for i in range(0, num_detections):\n",
        "                    class_name = category_index[result[\"detection_classes\"][0][i]]['name']\n",
        "                    if any(fil in class_name for fil in filters): # Only writes rows for filtered class\n",
        "                        ymin = result[\"detection_boxes\"][0][i][0]\n",
        "                        xmin = result[\"detection_boxes\"][0][i][1]\n",
        "                        ymax = result[\"detection_boxes\"][0][i][2]\n",
        "                        xmax = result[\"detection_boxes\"][0][i][3]\n",
        "                        tsv_writer.writerow([img_id, class_name, \n",
        "                                  xmin, ymin, xmax, ymax, im_w, im_h, image_url])\n",
        "      \n",
        "    return image_with_boxes\n",
        "\n",
        "print('\\nModel loaded and functions defined! \\nGo to next steps to run inference on images.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3HJoT3kx0a3"
      },
      "source": [
        "#### Test: Run inference on a couple images from URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D585u93x4N2"
      },
      "source": [
        "# TO DO: Type in image URLs 1-3 using form fields to right\n",
        "url_1 = \"https://content.eol.org/data/media/80/26/c6/542.5058695994.jpg\" #@param {type:\"string\"}\n",
        "url_2 = \"https://content.eol.org/data/media/80/16/40/542.4836766484.jpg\" #@param {type:\"string\"}\n",
        "url_3 = \"https://content.eol.org/data/media/9f/84/64/776.27478384.jpg\" #@param {type:\"string\"}\n",
        "image_urls = [url_1, url_2, url_3]\n",
        "\n",
        "# Display detection results on images\n",
        "display_results = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Set temporary outfile for tagging results\n",
        "outfpath = \"temp_outfile.tsv\"\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\\n\")\n",
        "for im_num, image_url in enumerate(image_urls, start=1):\n",
        "  try:\n",
        "    image_wboxes = run_detector_tf(image_url)\n",
        "    if display_results:\n",
        "        display_image(image_wboxes)\n",
        "    # Display progress message after each image\n",
        "    print('Inference complete for image {} of {}\\n'.format(im_num, len(image_urls)))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid\\n'.format(image_url))\n",
        "  \n",
        "  os.remove(outfpath) # Delete temporary outfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4xGqHxjE0mC"
      },
      "source": [
        "### Generate crops: Run inference on EOL images & save results for cropping - Run 4X for batches A-D\n",
        "Use 20K EOL Anura, Carnivora, Coleoptera, Squamata image bundles to get bounding boxes of detected animals. Results are saved to [crops_file].tsv. Run this section 4 times (to make batches A-D) of 5K images each to incrementally save in case of Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4aRzjXBh1KZ"
      },
      "source": [
        "#@title Enter EOL image bundle to run inference on\n",
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "# Read in EOL image bundle dataframe\n",
        "# Type in image bundle address using form field to right\n",
        "bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\" #@param [\"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Coleoptera_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Anura_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Carnivora_20K_breakdown_download_000001.txt\"]\n",
        "df = read_datafile(bundle, sep='\\n', header=None)\n",
        "df.columns = ['url']\n",
        "print('\\n EOL image bundle head:\\n{}'.format(df.head()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter path for saving results and output crops file name (change **crops_file** for each batch A-D)\n",
        "\n",
        "# Write header row of output tagging file\n",
        "basewd = \"/content/drive/MyDrive/train/tf2/results/\" #@param [\"/content/drive/MyDrive/train/results/\"] {allow-input: true}\n",
        "crops_file = \"multitaxa_cropcoords_tf2_a\" #@param [\"multitaxa_cropcoords_tf2_a\", \"multitaxa_cropcoords_tf2_b\", \"multitaxa_cropcoords_tf2_c\", \"multitaxa_cropcoords_tf2_d\"] {allow-input: true}\n",
        "outfpath = basewd + crops_file.rsplit('_',1)[0] + '_rcnn' + '_' + crops_file.rsplit('_',1)[1] + '.tsv'\n",
        "print('\\n Cropping file *batch {}* will be saved to:\\n{}'.format(crops_file.rsplit('_',1)[1], outfpath))\n",
        "\n",
        "# Write header row of output tag file\n",
        "with open(outfpath, 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"img_id\", \"class_name\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"im_width\", \"im_height\", \"url\"])"
      ],
      "metadata": {
        "id": "70sxbQCLWQS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbbHfPXIE6QE"
      },
      "source": [
        "#@title Choose settings to run inference on image batches A-D\n",
        "\n",
        "# Test pipeline with a smaller subset than 5k images?\n",
        "run = \"test with tiny subset\" #@param [\"test with tiny subset\", \"for all images\"]\n",
        "\n",
        "# Display detection results on images?\n",
        "display_results = \"yes (use this option if testing tiny subsets; only works for \\u003C50 images)\" #@param [\"yes (use this option if testing tiny subsets; only works for \\u003C50 images)\", \"no (use this option if running batches)\"]\n",
        "\n",
        "# Loop through EOL image bundle to add bounding boxes to images\n",
        "print(\"Running inference on images\")\n",
        "start, stop = set_start_stop(run)\n",
        "for i, row in enumerate(df.iloc[start:stop].iterrows()):\n",
        "    try:\n",
        "        image_wboxes = run_detector_tf(df['url'][i])\n",
        "        if (i+1<=50) and display_results:\n",
        "            display_image(image_wboxes)\n",
        "    \n",
        "        # Display progress message after each image\n",
        "        print('{}) Inference complete for image {} of {}\\n'.format(row[0], i+1, (stop-start)))\n",
        "\n",
        "    except:\n",
        "        print('Check if URL from {} is valid\\n'.format(df['url'][i]))\n",
        "\n",
        "print(\"Run inference using these two code blocks for all batches A-D before proceeding\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSg85W0WiDao"
      },
      "source": [
        "## Post-process detection results\n",
        "--- \n",
        "Combine output files for batches A-D. Then, convert detection boxes into square, centered thumbnail cropping coordinates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akN1OTCmiI0B"
      },
      "source": [
        "#### Merge batch output files A-D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENqHrC3-iFxQ"
      },
      "source": [
        "#@title Enter path to any inference result batch file A-D\n",
        "\n",
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# If you just ran \"Generate crops\" above, you do not need to enter anything\n",
        "# If you ran \"Generate crops\" during a previous session, enter the path for ONE output file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    outfpath = \"/content/drive/MyDrive/train/tf2/results/multitaxa_cropcoords_tf2_d.tsv\" #@param [\"/content/drive/MyDrive/train/tf2/results/multitaxa_cropcoords_tf2_rcnn_i_d.tsv\"] {allow-input: true}\n",
        "\n",
        "# Combine 4 batches of detection box coordinates to one dataframe\n",
        "base_path =  os.path.splitext(outfpath)[0].rsplit('_',1)[0] + '_'\n",
        "exts = ['a.tsv', 'b.tsv', 'c.tsv', 'd.tsv']\n",
        "all_filenames = [base_path + e for e in exts]\n",
        "df = pd.concat([pd.read_csv(f, sep='\\t', header=0, na_filter = False) for f in all_filenames], ignore_index=True)\n",
        "\n",
        "# Write results to tsv\n",
        "print(\"New concatenated dataframe with all 4 batches: \\n\", df.head())\n",
        "concat_outfpath = base_path + 'concat.tsv'\n",
        "df.to_csv(concat_outfpath, sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzM6i-fniPc_"
      },
      "source": [
        "#### Combine individual detection boxes into one \"superbox\" per image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8g-Gj9D4iQLj"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "from functools import reduce\n",
        "from urllib.error import HTTPError\n",
        "# So URL's don't get truncated in display\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "# Convert normalized detection coordinates (scaled to 0,1) to pixel values\n",
        "def denormalize_coords(crops):\n",
        "    crops.xmin = crops.xmin * crops.im_width\n",
        "    crops.ymin = crops.ymin * crops.im_height\n",
        "    crops.xmax = crops.xmax * crops.im_width\n",
        "    crops.ymax = crops.ymax * crops.im_height\n",
        "    # Round results to 2 decimal places\n",
        "    crops.round(2)\n",
        "    #print(\"De-normalized cropping coordinates: \\n\", crops.head())\n",
        "\n",
        "    return crops\n",
        "\n",
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "def make_superboxes(crops):\n",
        "    # Get superbox coordinates that contain all detection boxes per image\n",
        "    xmin = pd.DataFrame(crops.groupby(['url'])['xmin'].min()) # smallest xmin\n",
        "    ymin = pd.DataFrame(crops.groupby(['url'])['ymin'].min()) # smallest ymin\n",
        "    xmax = pd.DataFrame(crops.groupby(['url'])['xmax'].max()) # largest xmax\n",
        "    ymax = pd.DataFrame(crops.groupby(['url'])['ymax'].max()) # largest ymax\n",
        "\n",
        "    # Workaround to get im_height, im_width and class in same format as 'super box' coords\n",
        "    # There is only one value for im_height and im_width, so taking max will return unchanged values\n",
        "    im_h = pd.DataFrame(crops.groupby(['url'])['im_height'].max())\n",
        "    im_w = pd.DataFrame(crops.groupby(['url'])['im_width'].max())\n",
        "    im_class = pd.DataFrame(crops.groupby(['url'])['class_name'].max())\n",
        "  \n",
        "    # Make list of superboxes\n",
        "    superbox_list = [im_h, im_w, xmin, ymin, xmax, ymax, im_class]\n",
        "\n",
        "    # Make a new dataframe with 1 superbox per image\n",
        "    superbox_df = reduce(lambda  left, right: pd.merge(left, right, on=['url'],\n",
        "                                            how='outer'), superbox_list)\n",
        "    #print(\"Cropping dataframe, 1 superbox per image: \\n\", crops_unq.head())\n",
        "\n",
        "    return superbox_df\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "def add_identifiers(*, bundle_info, crops):\n",
        "    # Get dataObjectVersionIDs, identifiers, and eolMediaURLS from indexed cols\n",
        "    ids = bundle_info.iloc[:, np.r_[0:2,-2]]\n",
        "    ids.set_index('eolMediaURL', inplace=True, drop=True)\n",
        "    #print(\"Bundle identifying info head: \\n\", ids.head())\n",
        "\n",
        "    # Set up superboxes df for mapping to bundle_info\n",
        "    superboxes.reset_index(inplace=True)\n",
        "    superboxes.rename(columns={'url': 'eolMediaURL'}, inplace=True)\n",
        "    superboxes.set_index('eolMediaURL', inplace=True, drop=True)\n",
        "\n",
        "    # Map dataObjectVersionIDs to crops_unq using eolMediaURL as the index\n",
        "    crops_w_identifiers = pd.DataFrame(superboxes.merge(ids, left_index=True, right_index=True))\n",
        "    crops_w_identifiers.reset_index(inplace=True)\n",
        "    print(\"\\n Crops with added EOL identifiers: \\n\", crops_w_identifiers.head())\n",
        "  \n",
        "    return crops_w_identifiers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg2LqfpKiSWk"
      },
      "source": [
        "#@title Enter EOL image bundle used for running inference\n",
        "\n",
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "\n",
        "# Read in crop file exported from \"Combine output files A-D\" block above\n",
        "crops = read_datafile(concat_outfpath, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# De-normalize cropping coordinates to pixel values\n",
        "crops = denormalize_coords(crops)\n",
        "\n",
        "# Make 1 superbox per image [coordinates: bottom left (smallest xmin, ymin) and top right (largest xmax, ymax)]\n",
        "superboxes = make_superboxes(crops)\n",
        "\n",
        "# Read in EOL image \"breakdown\" bundle dataframe from \"breakdown_download\" bundle used for cropping\n",
        "if 'bundle' not in locals() or globals():\n",
        "    bundle = \"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\" #@param [\"https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Coleoptera_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Anura_20K_breakdown_download_000001.txt\", \"https://editors.eol.org/other_files/bundle_images/files/images_for_Carnivora_20K_breakdown_download_000001.txt\"]\n",
        "breakdown = bundle.replace(\"download_\", \"\") # Get EOL breakdown bundle url from \"breakdown_download\" address\n",
        "bundle_info = read_datafile(breakdown, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "crops_w_identifiers = add_identifiers(bundle_info=bundle_info, crops=superboxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_08BCyNiUcM"
      },
      "source": [
        "#### Make superbox dimensions square"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyyMzfYuiYbe"
      },
      "source": [
        "# Define functions\n",
        "\n",
        "# Suppress pandas warning about writing over a copy of data\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Check if dimensions are out of bounds\n",
        "def are_dims_oob(dim):\n",
        "    # Check if square dimensions are out of image bounds (OOB)\n",
        "    if dim > min(im_h, im_w):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Center padded, square coordinates around object midpoint\n",
        "def center_coords(coord_a, coord_b, crop_w, crop_h, im_dim_a, im_dim_b, pad):\n",
        "    # Centered, padded top-right coordinates\n",
        "    tr_coord_a = coord_a + 0.5*(abs(crop_h - crop_w)) + pad\n",
        "    tr_coord_b = coord_a + pad\n",
        "    # Adjust coordinate positions if OOB (out of bounds)\n",
        "    if crop_h != crop_w: # for cond 1 and 2\n",
        "        # Both coords not OOB\n",
        "        if (tr_coord_a <= im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - 0.5*(abs(crop_h - crop_w)) - pad\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_a OOB (+), shift cropping box down/left a-axis \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = 0.5*(abs(im_dim_a - crop_w))\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_b OOB (+), shift cropping box down/left b-axis    \n",
        "        elif (tr_coord_a <= im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - 0.5*(abs(crop_h - crop_w)) - pad\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "        # Both coords OOB (+), shift cropping box down/left both axes     \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = 0.5*(abs(im_dim_a - crop_w))\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "    else: # for cond 3\n",
        "        # Both coords not OOB\n",
        "        if (tr_coord_a <= im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - pad\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_a OOB (+), shift cropping box down/left a-axis \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b <= im_dim_b):\n",
        "            bl_coord_a = coord_a - (tr_coord_a - im_dim_a + pad)\n",
        "            bl_coord_b = coord_b - pad\n",
        "        # Topright coord_b OOB (+), shift cropping box down/left b-axis    \n",
        "        elif (tr_coord_a <= im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - pad\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "        # Both coords OOB (+), shift cropping box down/left both axes     \n",
        "        elif (tr_coord_a > im_dim_a) and (tr_coord_b > im_dim_b):\n",
        "            bl_coord_a = coord_a - (tr_coord_a - im_dim_a + pad)\n",
        "            bl_coord_b = coord_b - (tr_coord_b - im_dim_b + pad)\n",
        "    \n",
        "    return bl_coord_a, bl_coord_b\n",
        "\n",
        "# Set square dimensions = larger bounding box side\n",
        "def make_large_square(dim):\n",
        "    # Set new square crop dims = original larger crop dim\n",
        "    lg_square = crop_w1 = crop_h1 = dim\n",
        "    return lg_square\n",
        "\n",
        "# Set square dimensions = smaller bounding box side\n",
        "def make_small_square(dim):\n",
        "    # Set new square crop dims = original smaller crop dim\n",
        "    sm_square = crop_w1 = crop_h1 = dim\n",
        "    return sm_square\n",
        "\n",
        "# Add x% padding to bounding box dimensions\n",
        "def add_padding(dim):\n",
        "    # Add padding on all sides of square\n",
        "    padded_dim = dim + 2*percent_pad*dim\n",
        "    return padded_dim\n",
        "\n",
        "# Make square crops that are within image bounds for different scenarios\n",
        "def make_square_crops(df):\n",
        "    print(\"Before making square: \\n\", df.head())\n",
        "    start_time = time.time()\n",
        "    df['crop_height'] = round(df['ymax'] - df['ymin'], 1)\n",
        "    df['crop_width'] = round(df['xmax'] - df['xmin'], 1)\n",
        "    for i, row in df.iterrows():\n",
        "        # Define variables for use filtering data through loops below\n",
        "        crop_h0 = df['crop_height'][i]\n",
        "        crop_w0 = df['crop_width'][i]\n",
        "        #print(\"crop_h0: {}, crop_w0: {}\".format(crop_h0, crop_w0))\n",
        "        pad = percent_pad * max(crop_h0, crop_w0)  \n",
        "        global im_h, im_w\n",
        "        im_h = df.im_height[i]\n",
        "        im_w = df.im_width[i]\n",
        "        xmin0 = df.xmin[i]\n",
        "        ymin0 = df.ymin[i]\n",
        "        xmax0 = df.xmax[i]\n",
        "        ymax0 = df.ymax[i]\n",
        "        \n",
        "        # Conditions determine how rectangle bounding boxes are made square\n",
        "        cond1 = crop_h0 > crop_w0 # crop height > width\n",
        "        cond2 = crop_h0 < crop_w0 # crop width > height\n",
        "        cond3 = crop_h0 == crop_w0 # crop height = width (already square)\n",
        "\n",
        "        # Crop Height > Crop Width\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 1\n",
        "        if cond1:\n",
        "            lg_sq = make_large_square(crop_h0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_w0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "\n",
        "            # Where padded crop height is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = lg_padded_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i], df.ymin[i] = center_coords(xmin0, ymin0, crop_w0, crop_h1, im_w, im_h, pad)\n",
        "\n",
        "            # Where unpadded crop height is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is False) and (are_dims_oob(lg_sq) is True):\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = lg_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i] = xmin0 - 0.5*(min(im_h, im_w) - crop_w0)\n",
        "                df.ymin[i] = 0\n",
        "\n",
        "            # Where padded crop width is within image dimensions\n",
        "            elif (are_dims_oob(lg_sq) is False) and (are_dims_oob(sm_padded_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = sm_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i] = xmin0 - 0.5*pad\n",
        "                df.ymin[i] = ymin0 + 0.5*(crop_h0 - crop_w0) - pad   \n",
        "\n",
        "            # Where unpadded crop width is within image dimensions\n",
        "            elif (are_dims_oob(sm_padded_sq) is False) and (are_dims_oob(sm_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = sm_sq\n",
        "\n",
        "            # Where crop width and height are both OOB\n",
        "            elif are_dims_oob(sm_sq) is False:\n",
        "                # Do not crop, set values equal to image dimensions\n",
        "                df.crop_height[i] = crop_h1 = im_h \n",
        "                df.ymin[i] = 0\n",
        "                df.xmin[i] = 0 \n",
        "    \n",
        "        # Crop Width > Crop Height\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 2\n",
        "        elif cond2:\n",
        "            lg_sq = make_large_square(crop_w0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_h0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "\n",
        "            # Where padded crop width is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = lg_padded_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i], df.xmin[i] = center_coords(ymin0, xmin0, crop_w1, crop_h0, im_w, im_h, pad)\n",
        "\n",
        "            # Where unpadded crop width is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is False) and (are_dims_oob(lg_sq) is True):\n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = lg_sq  \n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i] = ymin0 - 0.5*(min(im_h, im_w) - crop_h0)\n",
        "                df.xmin[i] = 0\n",
        "\n",
        "            # Where padded crop height is within image dimensions\n",
        "            elif (are_dims_oob(lg_sq) is False) and (are_dims_oob(sm_padded_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = sm_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.ymin[i] = ymin0 - pad\n",
        "                df.xmin[i] = xmin0 + 0.5*(crop_w0 - crop_h0) - pad   \n",
        "\n",
        "            # Where unpadded crop height is within image dimensions\n",
        "            elif (are_dims_oob(sm_padded_sq) is False) and (are_dims_oob(sm_sq) is True):\n",
        "                # Make new crop dimensions equal to small padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_h1 = sm_sq\n",
        "\n",
        "            # Where crop width and height are both OOB\n",
        "            elif are_dims_oob(sm_sq) is False:\n",
        "                # Do not crop, set values equal to image dimensions\n",
        "                df.crop_width[i] = crop_w1 = im_w\n",
        "                df.crop_height[i] = crop_h1 = im_h \n",
        "                df.ymin[i] = 0\n",
        "                df.xmin[i] = 0 \n",
        "\n",
        "        # Crop Width == Crop Height\n",
        "        # See project wiki \"Detailed explanation with drawings: convert_bboxdims.py\", Scenario 3\n",
        "        elif cond3: \n",
        "            lg_sq = make_large_square(crop_w0)\n",
        "            lg_padded_sq = add_padding(lg_sq)\n",
        "            sm_sq = make_small_square(crop_h0)\n",
        "            sm_padded_sq = add_padding(sm_sq)\n",
        "        \n",
        "            # Where padded crop width/height is within image dimensions\n",
        "            if are_dims_oob(lg_padded_sq) is False:            \n",
        "                # Make new crop dims equal to large padded square dims\n",
        "                df.crop_width[i] = df.crop_height[i] = crop_w1 = crop_h1 = lg_padded_sq\n",
        "                # Center position of new crop dims (adjust xmin, ymin)\n",
        "                df.xmin[i], df.ymin[i] = center_coords(xmin0, ymin0, crop_w0, crop_w1, im_w, im_h, pad)\n",
        "                \n",
        "            # Where unpadded crop width/height is within image dimensions\n",
        "            elif (are_dims_oob(lg_padded_sq) is True) and (are_dims_oob(lg_sq) is False):\n",
        "                # Both coords not OOB, no changes needed\n",
        "                if (ymax0 <= im_h) and (xmax0 <= im_w):\n",
        "                    pass\n",
        "                \n",
        "                # Topright X coord OOB (+), shift cropping box left\n",
        "                elif (ymax0 <= im_h) and (xmax0 > im_w):  \n",
        "                    df.xmin[i] = xmin0 - (xmax0 - im_w)\n",
        "                # Topright Y coord OOB (+), shift cropping box down\n",
        "                elif (ymax0 > im_h) and (xmax0 <= im_w):\n",
        "                    df.ymin[i] = ymin0 - (ymax0 - im_h)\n",
        "                # X and Y coords OOB (+), shift cropping box down and left   \n",
        "                elif (ymax0 > im_h) and (xmax0 > im_w):\n",
        "                    df.ymin[i] = ymin0 - (ymax0 - im_h)\n",
        "                    df.xmin[i] = xmin0 - (xmax0 - im_w)\n",
        "\n",
        "    # Image coordinates should be positive, set negative xmin and ymin values to 0\n",
        "    df.xmin[df.xmin < 0] = 0\n",
        "    df.ymin[df.ymin < 0] = 0\n",
        "    print(\"Cropping coordinates, made square and with {}% padding: \\n{}\".format(percent_pad, df.head()))\n",
        "\n",
        "    # Print time to run script\n",
        "    print ('Run time: {} seconds'.format(format(time.time()- start_time, '.2f')))\n",
        "\n",
        "    return df\n",
        "\n",
        "# Format cropping dimensions to EOL standards\n",
        "def format_crops_for_eol(df):\n",
        "# {\"height\":\"423\",\"width\":\"640\",\"crop_x\":123.712,\"crop_y\":53.4249,\"crop_width\":352,\"crop_height\":0}\n",
        "    df['crop_dimensions'] = np.nan\n",
        "    for i, row in df.iterrows():\n",
        "        df.crop_dimensions[i] = ('{{\"height\":\"{}\",\"width\":\"{}\",\"crop_x\":{},\"crop_y\":{},\"crop_width\":{},\"crop_height\":{}}}'\n",
        "        .format(df.im_height[i], df.im_width[i], df.xmin[i], df.ymin[i], df.crop_width[i], df.crop_height[i]))\n",
        "    #print(\"\\n EOL formatted cropping dimensions: \\n\", df.head())\n",
        "\n",
        "    # Add other dataframe elements from cols: identifier, dataobjectversionid, eolmediaurl, im_class, crop_dimensions\n",
        "    eol_crops = pd.DataFrame(df.iloc[:,np.r_[-5,-4,-6,0,-1]])\n",
        "    print(\"\\n EOL formatted cropping dimensions: \\n\", eol_crops.head())\n",
        "\n",
        "    return eol_crops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB4J8S7fibKU"
      },
      "source": [
        "# Make crops square and within image bounds\n",
        "\n",
        "# Optional TO DO: Pad by xx% larger crop dimension\n",
        "percent_pad = 0 #@param {type:\"slider\", min:0, max:10, step:2}\n",
        "\n",
        "# Make crops square and within bounds\n",
        "df = make_square_crops(crops_w_identifiers)\n",
        "\n",
        "# Export crop coordinates to display_test.tsv to visualize results in next code block and confirm crop transformations\n",
        "display_test_fpath = os.path.splitext(concat_outfpath)[0] + '_displaytest' + '.tsv'\n",
        "print(\"\\n File for displaying square crops on images will be saved to: \\n\", display_test_fpath)\n",
        "df.to_csv(display_test_fpath, sep='\\t', index=False)\n",
        "\n",
        "# Format image and cropping dimensions for EOL standards\n",
        "eol_crops = format_crops_for_eol(df)\n",
        "\n",
        "# Write results to tsv\n",
        "eol_crops_fpath = os.path.splitext(display_test_fpath)[0].rsplit('_',2)[0] + '_20k_final' + '.tsv'\n",
        "eol_crops.to_csv(eol_crops_fpath, columns = eol_crops.iloc[:,:-1], sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zIsiYIBiXou"
      },
      "source": [
        "## Display cropping results on images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTXPwvTaie_3"
      },
      "source": [
        "#@title Read in cropping file and display results on images\n",
        "\n",
        "# Define functions\n",
        "\n",
        "import cv2\n",
        "\n",
        "# If you just ran \"Generate crops\" above, you do not need to enter anything\n",
        "# If you ran \"Generate crops\" during a previous session, enter the path for ONE output file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    outfpath = \"/content/drive/MyDrive/train/tf2/results/multitaxa_cropcoords_tf2_d.tsv\" #@param [\"/content/drive/MyDrive/train/tf2/results/multitaxa_cropcoords_tf2_d.tsv\"] {allow-input: true}\n",
        "df = pd.read_csv(outfpath, sep=\"\\t\", header=0)\n",
        "print(df.head())\n",
        "\n",
        "# For uploading an image from url\n",
        "# Modified from https://www.pyimagesearch.com/2015/03/02/convert-url-to-image-with-python-and-opencv/\n",
        "def url_to_image(url):\n",
        "    resp = urllib.request.urlopen(url)\n",
        "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_np = np.expand_dims(image, axis=0)\n",
        "    im_h, im_w = image.shape[:2]\n",
        "  \n",
        "    return image_np\n",
        "\n",
        "# For handling bounding boxes\n",
        "def draw_bounding_box_on_image(image, ymin, xmin, ymax, xmax,\n",
        "                               color, font, thickness=4, display_str_list=()):\n",
        "    \"\"\"Adds a bounding box to an image.\"\"\"\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    im_width, im_height = image.size\n",
        "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
        "                                ymin * im_height, ymax * im_height)\n",
        "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
        "             (left, top)], width=thickness, fill=color)\n",
        "\n",
        "    # Adjust display string placement if out of bounds\n",
        "    display_str_heights = [font.getbbox(ds)[3]-font.getbbox(ds)[1] for ds in display_str_list]\n",
        "    # Each display_str has a top and bottom margin of 0.05x.\n",
        "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
        "    if top > total_display_str_height:\n",
        "        text_bottom = top\n",
        "    else:\n",
        "        text_bottom = top + total_display_str_height\n",
        "    # Reverse list and print from bottom to top.\n",
        "    for ds in display_str_list[::-1]:\n",
        "        text_height = font.getbbox(ds)[3] - font.getbbox(ds)[1]\n",
        "        text_width = font.getbbox(ds)[2] - font.getbbox(ds)[0]\n",
        "        margin = np.ceil(0.05 * text_height)\n",
        "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
        "                    (left + text_width, text_bottom)],\n",
        "                   fill=color)\n",
        "        draw.text((left + margin, text_bottom - text_height - margin),\n",
        "                  ds, fill=\"black\", font=font)\n",
        "        text_bottom -= text_height - 2 * margin\n",
        "\n",
        "# Draw bounding box on image\n",
        "def draw_boxes(image, df):\n",
        "    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n",
        "    # Set font\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSansNarrow-Regular.ttf\",\n",
        "                              25)\n",
        "    except IOError:\n",
        "        print(\"Font not found, using default font.\")\n",
        "        font = ImageFont.load_default()\n",
        "    colors = list(ImageColor.colormap.values())\n",
        "    # Format box and label design\n",
        "    ymin, xmin, ymax, xmax = tuple([df[\"ymin\"][i], df[\"xmin\"][i], df[\"xmax\"][i], df[\"ymax\"][i]])\n",
        "    box_dims = [ymin, xmin, ymax, xmax]\n",
        "    display_str = \"{}\".format(df[\"class_name\"][i])\n",
        "    color = colors[hash(df[\"class_name\"][i]) % len(colors)]\n",
        "    image_pil = Image.fromarray(np.squeeze(image))\n",
        "    # Draw the box\n",
        "    draw_bounding_box_on_image(\n",
        "                image_pil,\n",
        "                ymin, xmin, ymax, xmax,\n",
        "                color, font, display_str_list=[display_str])\n",
        "    np.copyto(image, np.array(image_pil))\n",
        "    \n",
        "    return image[0], box_dims"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvexyAOfig8M"
      },
      "source": [
        "#@title Choose starting index for crops to display\n",
        "\n",
        "# Adjust line to right to see up to 50 images displayed at a time\n",
        "start = 0 #@param {type:\"slider\", min:0, max:5000, step:50}\n",
        "stop = start+50\n",
        "\n",
        "# Loop through images\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "    # Read in image \n",
        "    url = df['url'][i]\n",
        "    img = url_to_image(url)\n",
        "  \n",
        "    # Draw bounding box on image\n",
        "    image_wbox, box_dims = draw_boxes(img, df)\n",
        "\n",
        "    # Plot cropping box on image\n",
        "    _, ax = plt.subplots(figsize=(20, 15))\n",
        "    ax.imshow(image_wbox)\n",
        "\n",
        "    # Display image URL and coordinatesabove image\n",
        "    # Helps with fine-tuning data transforms in post-processing steps above\n",
        "    plt.title('{} \\n xmin: {}, ymin: {}, xmax: {}, ymax: {}'.format(url, box_dims[0], box_dims[1], box_dims[2], box_dims[3]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}