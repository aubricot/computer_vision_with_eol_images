{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "multitaxa_train_yolo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X2fF0fSxmJZR",
        "w3ouL5RM-mpX"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB"
      },
      "source": [
        "# Training YOLO in Darkflow to detect snakes & lizards (Squamata), beetles (Coleoptera), frogs (Anura), and carnivores (Carnivora) from EOL images\n",
        "---\n",
        "Last Updated 8 April 2020  \n",
        "--*Update as of 29 May 2021--Darkflow builds are no longer being updated and only support Tensorflow 1.x builds. As a result, this notebook is left in its state from 8 April 2020. Functions may become deprecated or lose functionality. For updated inference with Multitaxa, refer to [Tensorflow notebooks](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_tf2_ssd_rcnn.ipynb). For object detection with YOLO v4 in it's native state, see [Object Detection for Image Tagging Notebooks](https://github.com/aubricot/computer_vision_with_eol_images/tree/master/object_detection_for_image_tagging)*--   \n",
        "\n",
        "Use images and annotation files to train YOLO in Darkflow to detect snakes & lizards, beetles, frogs and carnivores from EOL images.\n",
        "\n",
        "Datasets exported from [multitaxa_preprocessing.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_preprocessing.ipynb) were converted to xml formatted annotation files before use in this notebook. Images were already downloaded to Google Drive in multitaxa_preprocessing.ipynb. \n",
        "\n",
        "Annotations for train and test datasets will be uploaded to your Google Drive in this notebook. \n",
        "\n",
        "Exported detection results (json files) can be used to calculate model precision for comparison with Faster-RCNN models using [calculate_error_mAP.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/calculate_error_mAP.ipynb). \n",
        "\n",
        "Notes:   \n",
        "* This was done using ColabPro. If not using Pro, for each 24 hour period on Google Colab, you have up to 12 hours of GPU access. Training the object detection model on snakes & lizards, beetles, frogs and carnivores took 36 hours and 37,875 epochs.\n",
        "\n",
        "* Make sure to set the runtime to Python 2 with GPU Hardware Accelerator.   \n",
        "\n",
        "References:   \n",
        "* [Official Darkflow training instructions](https://github.com/thtrieu/darkflow)   \n",
        "* [Medium Blog on training using YOLO via Darkflow in Colab](https://medium.com/coinmonks/detecting-custom-objects-in-images-video-using-yolo-with-darkflow-1ff119fa002f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJz5m4BKmJZD"
      },
      "source": [
        "## Installs\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu"
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj3-JKOsIPtc"
      },
      "source": [
        "# Change to your working directory\n",
        "%cd drive/My Drive/fall19_smithsonian_informatics/train\n",
        "\n",
        "# Install libraries\n",
        "# Make sure you are using Python 3.6\n",
        "!python --version\n",
        "!pip install tensorflow-gpu==1.15.0rc2\n",
        "!pip install cython\n",
        "!pip install opencv-python\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import shutil "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2fF0fSxmJZR"
      },
      "source": [
        "### Only run once: Model preparation uploads to Google Drive\n",
        "For detailed instructions on training YOLO using a custom dataset, see the [Darkflow GitHub Repository](https://github.com/thtrieu/darkflow)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKYxuyq5Ib_f"
      },
      "source": [
        "# Download train and test dataset annotation files and install darkflow\n",
        "\n",
        "# Download darkflow (the tensorflow implementation of YOLO)\n",
        "if os.path.exists(\"darkflow-master\"):\n",
        "  %cd darkflow-master\n",
        "  !pwd\n",
        "\n",
        "elif not os.path.exists(\"darkflow-master\"):\n",
        "  !git clone --depth 1 https://github.com/thtrieu/darkflow.git\n",
        "  # Compile darkflow\n",
        "  %cd darkflow\n",
        "  !python setup.py build_ext --inplace\n",
        "  # Rename darkflow to darkflow-master to distinguish between folder names\n",
        "  shutil.move('/content/drive/My Drive/fall19_smithsonian_informatics/train/darkflow', \n",
        "          '/content/drive/My Drive/fall19_smithsonian_informatics/train/darkflow-master')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268I2Ev_mJZL"
      },
      "source": [
        "# Test installation, you should see an output with different parameters for flow\n",
        "!python flow --h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SXQ5bjUzA0u"
      },
      "source": [
        "# Download other needed files for training\n",
        "\n",
        "# Download Multitaxa test annotation files to train/test_ann\n",
        "# TO DO: If training new model in same directory as previous model, need to first delete test_ann folder\n",
        "%cd ../\n",
        "if not os.path.exists(\"test_ann\"):\n",
        "  !mkdir test_ann\n",
        "  %cd test_ann\n",
        "  !wget -nc -i https://editors.eol.org/other_files/bundle_images/xml_for_multitaxa_crops_test_notaug_fin.txt\n",
        "  !rm -rf xml_for_multitaxa_crops_test_notaug_fin.txt\n",
        "  %cd ../\n",
        "\n",
        "# Download Multitaxa training annotation files to the darkflow directory\n",
        "ann_file = 'darkflow-master/test/training/annotations/27250493.xml'\n",
        "if not os.path.isfile(\"ann_file\"):\n",
        "  %cd darkflow-master/test/training\n",
        "  !rm -rf annotations\n",
        "  !mkdir annotations\n",
        "  %cd annotations\n",
        "  !wget -nc -i https://editors.eol.org/other_files/bundle_images/xml_for_multitaxa_crops_train_aug_all_fin.txt\n",
        "  !rm -rf xml_for_multitaxa_crops_train_aug_all_fin.txt\n",
        "  %cd ../..\n",
        "\n",
        "# Upload yolo.weights, pre-trained weights file (for YOLO v2) from Google drive \n",
        "weights_file = 'bin/yolo.weights'\n",
        "if not os.path.exists('weights_file'):\n",
        "  !gdown --id 0B1tW_VtY7oniTnBYYWdqSHNGSUU\n",
        "  !mkdir bin\n",
        "  !mv yolo.weights bin\n",
        "\n",
        "# Make new label file/overwrite existing labels.txt downloaded with darkflow\n",
        "!echo 'Squamata' >labels.txt\n",
        "!echo 'Coleoptera' >>labels.txt\n",
        "!echo 'Anura' >>labels.txt\n",
        "!echo 'Carnivora' >>labels.txt\n",
        "\n",
        "# Download model config file edited for training darkflow to identify 4 classes (yolo-4c = 4 classes)\n",
        "mod_config_file = 'cfg/yolo-4c.cfg'\n",
        "if not os.path.exists('mod_config_file'):\n",
        "  %cd cfg\n",
        "  !gdown --id 1N6-eEEfZchps2NejwgvzGr5xJanhSP4m\n",
        "  %cd ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz"
      },
      "source": [
        "## Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP"
      },
      "source": [
        "%cd darkflow-master\n",
        "%tensorflow_version 1.0\n",
        "\n",
        "# For importing/exporting files, working with arrays, etc\n",
        "from google.colab import files\n",
        "import os\n",
        "import pathlib\n",
        "import imageio\n",
        "import time\n",
        "import csv\n",
        "import urllib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For the actual object detection\n",
        "from darkflow.net.build import TFNet\n",
        "\n",
        "# For drawing onto and plotting the images\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3ouL5RM-mpX"
      },
      "source": [
        "## Train the model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnBpq-ytrAxi"
      },
      "source": [
        "# List different parameters for flow\n",
        "!python flow --h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn4U3HOTgPVX"
      },
      "source": [
        "# Train model (yolo-4c.cfg) using pre-trained weights from basal layers of yolo.weights, the top layer will be trained from scracth to detect Squamata, Coleoptera, Anura and Carnivora\n",
        "# Change the dataset and annotation directories to your paths in Google Drive\n",
        "%cd darkflow-master\n",
        "!python flow --model cfg/yolo-4c.cfg --train --trainer adam --load bin/yolo.weights --gpu 0.8 --epoch 38 --dataset \"/content/drive/My Drive/fall19_smithsonian_informatics/train/images\" --annotation \"test/training/annotations\" --savepb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZoKtS-xhbQm"
      },
      "source": [
        "# Resume training from last checkpoint\n",
        "!python flow --load -1 --model cfg/yolo-4c.cfg --train --savepb --trainer adam --gpu 0.8 --epoch 3000 --dataset \"/content/drive/My Drive/fall19_smithsonian_informatics/train/images\" --annotation \"test/training/annotations\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqA_lW7tbLeH"
      },
      "source": [
        "# Save the last checkpoint to protobuf file\n",
        "!python flow --model cfg/yolo-4c.cfg --load -1 --savepb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IZw-HlWrVxn"
      },
      "source": [
        "# Resume training from protobuf file\n",
        "!python flow --load -1 --pbLoad built_graph/yolo-4c.pb --metaLoad built_graph/yolo-4c.meta --train --savepb --trainer adam --gpu 0.8 --epoch 3000 --dataset \"/content/drive/My Drive/fall19_smithsonian_informatics/train/images\" --annotation \"test/training/annotations\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLGOQiwSr-Gd"
      },
      "source": [
        "### When finished training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GhWXLLlPNrr"
      },
      "source": [
        "# Export detection results for test images as json files to calculate mAP (mean average precision, a performance measure to compare models) using calculate_error_mAP.ipynb\n",
        "!python flow --pbLoad built_graph/yolo-4c.pb --gpu 0.8 --metaLoad built_graph/yolo-4c.meta --imgdir \"/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images\" --json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zL-PctVuqZv"
      },
      "source": [
        "## Run test images through the trained object detector & don't save results\n",
        "---\n",
        "Test image detection boxes are only needed for calculating mAP (mean average precision, a performance measure to compare models) and not for cropping. The functions below will only display resulting detection boxes on test images for visualization, but does not save their coordinates to a spreadsheet. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy3lSNqWOjq0"
      },
      "source": [
        "### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PXMk2Rtsnk-"
      },
      "source": [
        "# For loading images into computer-readable format\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Function for loading images from urls\n",
        "def url_to_image(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image\n",
        "\n",
        "# For drawing bounding boxes around detected objects on images\n",
        "def boxing(image, predictions):\n",
        "    newImage = np.copy(image)\n",
        "    im_height, im_width, im_depth = image.shape\n",
        "\n",
        "    # Organize results of object detection for plotting and export\n",
        "    for result in predictions:\n",
        "        xmin = result['topleft']['x']\n",
        "        ymin = result['topleft']['y']\n",
        "\n",
        "        xmax = result['bottomright']['x']\n",
        "        ymax = result['bottomright']['y']\n",
        "\n",
        "        confidence = result['confidence']\n",
        "        #label = result['label'] + \" \" + str(round(confidence, 3))\n",
        "        label = result['label']\n",
        "\n",
        "        # Only show boxes that are above set confidence and for the label Chiroptera\n",
        "        # Optional: change confidence and label values\n",
        "        #if confidence > 0 and result['label'] == 'Lepidoptera' :\n",
        "        if confidence > 0:\n",
        "            # Draw boxes on images\n",
        "            fontScale = max(im_width,im_height)/(600)\n",
        "            # Add labels to boxes\n",
        "            if label == 'Squamata':\n",
        "              newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "              box_col = (255,199,15)\n",
        "            elif label == 'Coleoptera':\n",
        "              newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "              box_col = (255,127,0)\n",
        "            elif label == 'Anura':\n",
        "              newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "              box_col = (255,42,22)\n",
        "            elif label == 'Carnivora':\n",
        "              newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "              box_col = (0,191,255)  \n",
        "            # Draw detection boxes on images\n",
        "            newImage = cv2.rectangle(newImage, (xmin, ymax), (xmax, ymin), box_col, 3)\n",
        "    \n",
        "    return newImage\n",
        "\n",
        "# Define parameters for \"flow\"ing the images through the model\n",
        "# Optional: adjust detection confidence threshold\n",
        "params = {\n",
        "    'model': 'cfg/yolo-4c.cfg',\n",
        "    'load': 'bin/yolo.weights',\n",
        "    'gpu': 0.8,\n",
        "    #'threshold': 0.1, \n",
        "    'pbLoad': 'built_graph/yolo-4c.pb', \n",
        "    'metaLoad': 'built_graph/yolo-4c.meta' \n",
        "}\n",
        "\n",
        "# Run the model\n",
        "tfnet = TFNet(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxI1a4iRSNQU"
      },
      "source": [
        "### Run test images (from file) through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UbyA9e1omJZZ"
      },
      "source": [
        "# Test trained model on test images\n",
        "from PIL import Image\n",
        "\n",
        "# Update path to your test images\n",
        "PATH_TO_TEST_IMAGES_DIR = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images'\n",
        "PATH_TO_OUT_IMAGES_DIR = '/content/drive/My Drive/fall19_smithsonian_informatics/train/out/yolo/'\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "OUT_IMAGE_PATHS = [os.path.join(PATH_TO_OUT_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS[420:], start=1):\n",
        "\n",
        "    # Load in image\n",
        "    image = Image.open(im_path)\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image_np)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on image\n",
        "    boxing(image_np, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 429 test images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    #_, ax = plt.subplots(figsize=(10, 10))\n",
        "    #ax.imshow(boxing(image_np, result))\n",
        "    #plt.title('{}) Inference time: {}'.format(im_num, format(end_time-start_time, '.2f')))\n",
        "\n",
        "    # Optional: Save image with detection boxes to Google Drive\n",
        "    img = Image.fromarray(boxing(image_np, result))\n",
        "    img.save(OUT_IMAGE_PATHS[TEST_IMAGE_PATHS.index(im_path)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjDSAbX4L4kg"
      },
      "source": [
        "### Get inference info for test images to compare object detection model times for YOLO, SSD, and Faster-RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfcie12ULym-"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "# For exporting inference times\n",
        "inf_time = []\n",
        "img_path = []\n",
        "im_dims = []\n",
        "\n",
        "# Update path to your test images\n",
        "PATH_TO_TEST_IMAGES_DIR = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images'\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "#for im_num, im_path in enumerate(TEST_IMAGE_PATHS[:5], start=1):\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS, start=1):\n",
        "\n",
        "    # Load in image\n",
        "    image = Image.open(im_path)\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image_np)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on image\n",
        "    boxing(image_np, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 429 test images'.format(im_num))\n",
        "\n",
        "    # Record inference time, image name and image dimensions to export\n",
        "    inf_time.append(end_time-start_time)\n",
        "    img_path.append(im_path)\n",
        "    im_dims.append(image_np.shape)\n",
        "    \n",
        "inf_times = pd.DataFrame((inf_time, img_path, im_dims))\n",
        "inf_times = inf_times.transpose()\n",
        "inf_times.to_csv(\"multitaxa_inference_times_yolo.csv\", index=False, header=(\"time (sec)\", \"filepath\", \"image_dims (h, w, d)\"))\n",
        "!mv multitaxa_inference_times_yolo.csv ..\n",
        "print(inf_times.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErD78FKVr0sc"
      },
      "source": [
        "### Run other images (from individual URLs) through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szb8S54dr5Qy"
      },
      "source": [
        "# Test trained model on images\n",
        "from PIL import Image\n",
        "\n",
        "# Use individual URLs (use your own here)\n",
        "image_urls = [\"https://upload.wikimedia.org/wikipedia/commons/b/be/Batman_%28retouched%29.jpg\",\n",
        "              \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Bela_Lugosi_as_Dracula%2C_anonymous_photograph_from_1931%2C_Universal_Studios.jpg/690px-Bela_Lugosi_as_Dracula%2C_anonymous_photograph_from_1931%2C_Universal_Studios.jpg\"]\n",
        "\n",
        "# Loops through image_urls\n",
        "for im_num, image_url in enumerate(image_urls, start=1):\n",
        "  try:\n",
        "    # Load in image\n",
        "    image_np = url_to_image(image_url)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image_np)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on image\n",
        "    boxing(image_np, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 2 test images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(boxing(image_np, result))\n",
        "    plt.title('{}) Inference time: {}'.format(im_num, format(end_time-start_time, '.2f')))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV08UMa0wjd0"
      },
      "source": [
        "## Run EOL image bundles through the trained object detector & save results for cropping\n",
        "---\n",
        "Display resulting detection boxes on images and save their coordinates to lepidoptera_det_crops_yolo.tsv for use cropping EOL images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0SGWtlJxEYa"
      },
      "source": [
        "### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0INfzqLNw-_D"
      },
      "source": [
        "# For loading images into computer-readable format\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Function for loading images from urls\n",
        "def url_to_image(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image\n",
        "\n",
        "# For drawing bounding boxes around detected objects on images\n",
        "def boxing(image, predictions):\n",
        "    newImage = np.copy(image)\n",
        "    im_height, im_width, im_depth = image.shape\n",
        "\n",
        "    # Organize results of object detection for plotting and export\n",
        "    for result in predictions:\n",
        "        xmin = result['topleft']['x']\n",
        "        ymin = result['topleft']['y']\n",
        "\n",
        "        xmax = result['bottomright']['x']\n",
        "        ymax = result['bottomright']['y']\n",
        "\n",
        "        confidence = result['confidence']\n",
        "        label = result['label'] + \" \" + str(round(confidence, 3))\n",
        "\n",
        "        # Only show boxes that are above set confidence and for the label Chiroptera\n",
        "        if confidence > 0 and result['label'] == 'Lepidoptera' :\n",
        "            # Draw boxes on images\n",
        "            fontScale = min(im_width,im_height)/(600)\n",
        "            newImage = cv2.rectangle(newImage, (xmin, ymax), (xmax, ymin), (255, 0, 157), 3)\n",
        "            newImage = cv2.putText(newImage, label, (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (153, 255, 255), 5, cv2.LINE_AA)\n",
        "\n",
        "            # Export detection results to det_crops_yolo.tsv\n",
        "            # Change filename here if using 1000 or 20000 images dataset\n",
        "            with open('/content/drive/My Drive/fall19_smithsonian_informatics/lepidoptera_det_crops_1000.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([image_url, im_height, im_width, \n",
        "                            xmin, ymin, xmax, ymax])\n",
        "\n",
        "    return newImage\n",
        "\n",
        "# Define parameters for \"flow\"ing the images through the model\n",
        "# Optional: adjust detection confidence threshold\n",
        "params = {\n",
        "    'model': 'cfg/yolo-4c.cfg',\n",
        "    'load': 'bin/yolo.weights',\n",
        "    'gpu': 0.8,\n",
        "    #'threshold': 0.1, \n",
        "    'pbLoad': 'built_graph/yolo-4c.pb', \n",
        "    'metaLoad': 'built_graph/yolo-4c.meta' \n",
        "}\n",
        "\n",
        "# Run the model\n",
        "tfnet = TFNet(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYMwPPLZvpqW"
      },
      "source": [
        "### Run images (from EOL image URL bundles) through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr6szvqZtno0"
      },
      "source": [
        "# Use URLs from EOL image URL bundles\n",
        "# Comment out to use either 1000 or 20000 image bundles\n",
        "# 1000 Lepidoptera images\n",
        "urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Lepidoptera_breakdown_download_000001.txt'\n",
        "# 20000 Lepidoptera images\n",
        "#urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Lepidoptera_20K_breakdown_download_000001.txt'\n",
        "\n",
        "df = pd.read_csv(urls)\n",
        "df.columns = [\"link\"]\n",
        "pd.DataFrame.head(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVA5olgfwZl8"
      },
      "source": [
        "# Write header row of output crops file\n",
        "# For 1000 or 20000 image datasets, change filename here and in \"Prepare object detection functions and settings -> def boxing -> Export detection results\" above\n",
        "with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/lepidoptera_det_crops_1000.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"image_url\", \"im_height\", \"im_width\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyVGNtubzGlw"
      },
      "source": [
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "for i, row in df.head(5).itertuples(index=True, name='Pandas'):\n",
        "\n",
        "# For ranges of rows or all rows, use df.iloc\n",
        "# Can be useful if running detection in batches\n",
        "#for i, row in df.iloc[500:800].iterrows():\n",
        "\n",
        "  try:\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Load in image\n",
        "    image_url = df1.get_value(i, \"link\")\n",
        "    image = url_to_image(image_url)\n",
        "    # Detection\n",
        "    result = tfnet.return_predict(image)\n",
        "    end_time = time.time()\n",
        "    # Draw boxes on image\n",
        "    boxing(image, result)\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 429 test images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(boxing(image, result))\n",
        "    plt.title('{}) Inference time: {}'.format(i+1, format(end_time-start_time, '.2f')))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
