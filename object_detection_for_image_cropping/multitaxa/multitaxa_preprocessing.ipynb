{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "multitaxa_preprocessing.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rnwb_rgmJZB",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing and image augmentation for object detection model training and testing datasets\n",
        "---\n",
        "*Last Updated 22 April 2020*   \n",
        "Test and train datasets (images and cropping dimensions) exported from [split_train_test.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/tree/master/object_detection_for_image_cropping/split_train_test.ipynb) are pre-processed and transformed to formatting standards for use with YOLO via Darkflow and SSD and R-FCN object detection models implemented in Tensorflow. All train and test images are also downloaded to Google Drive for future use training and testing.\n",
        "\n",
        "Before reformatting to object detection model standards, training data for each taxon (Coleoptera, Anura, Squamata and Carnivora) is augmented using the [imgaug library](https://github.com/aleju/imgaug). Image augmentation is used to increase training data sample size and diversity to reduce overfitting when training object detection models. Both images and cropping coordinates are augmented. Augmented and original datasets are combined to make the final training dataset pooled for all taxa before being transformed to object detection model formatting standards.\n",
        "\n",
        "**Note: Train Image sections need to be run once for each taxon. Change taxon names where you see '# TO DO' (ie. find -> \"squamata\" and replace it with \"coleoptera\").**\n",
        "\n",
        "After exporting augmented box coordinates from this notebook, test displaying them using [coordinates_display_test.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/tree/master/object_detection_for_image_cropping/coordinates_display_test.ipynb). If they are not as expected, modify data cleaning steps in the section **Remove out of bounds values from train crops and export results for use with object detection models** for train and test images below until the desired results are achieved. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJz5m4BKmJZD",
        "colab_type": "text"
      },
      "source": [
        "## Installs\n",
        "---\n",
        "Install required libraries directly to this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01UXykSJp610",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install libraries for augmenting and displaying images\n",
        "!pip install imgaug\n",
        "!pip install pillow\n",
        "!pip install scipy==1.1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAbU5tW1ONu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwKdj73Wpnlz",
        "colab_type": "text"
      },
      "source": [
        "## Imports   \n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSLXg6G7mJZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change to your training directory within Google Drive\n",
        "%cd drive/My Drive/fall19_smithsonian_informatics/train\n",
        "\n",
        "# For importing/exporting files, working with arrays, etc\n",
        "import pathlib\n",
        "import os\n",
        "import imageio\n",
        "import time\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "from scipy.misc import imread\n",
        "\n",
        "# For augmenting the images and bounding boxes\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
        "\n",
        "# For drawing onto and plotting the images\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lMvu5M0oUrY",
        "colab_type": "text"
      },
      "source": [
        "### Train images - Run once for each taxon\n",
        "---\n",
        "Run all steps once for each taxon (Coleoptera, Anura, Squamata and Carnivora).\n",
        "Must change names where you see '# TO DO' (ie. find -> \"carnivora\" and replace with \"coleoptera\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGMDGahzm44x",
        "colab_type": "text"
      },
      "source": [
        "#### Augment & download train images to Google Drive  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGyqdFFznIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set-up augmentation parameters and write the header of output file crops_train_aug.tsv generated in the next step\n",
        "\n",
        "# Read in EOL images and user-generated cropping coordinate training data\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train.tsv\n",
        "crops = pd.read_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train.tsv', sep='\\t', header=0)\n",
        "print(crops.head())\n",
        "\n",
        "# For image augmentation\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "# For saving images to Google Drive\n",
        "from scipy import misc\n",
        "\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Define image augmentation pipeline\n",
        "# modified from https://github.com/aleju/imgaug\n",
        "seq = iaa.Sequential([\n",
        "    iaa.Crop(px=(1, 16), keep_size=False), # crop by 1-16px, resize resulting image to orig dims\n",
        "    iaa.Affine(rotate=(-25, 25)), # rotate -25 to 25 degrees\n",
        "    iaa.GaussianBlur(sigma=(0, 3.0)), # blur using gaussian kernel with sigma of 0-3\n",
        "    iaa.AddToHueAndSaturation((-50, 50), per_channel=True)\n",
        "])\n",
        "\n",
        "# Write header of crops_aug.tsv before looping through crops for remaining data\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug.tsv\n",
        "if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([\"data_object_id\",\t\"obj_url\",\t\"height\",\t\"width\",\t\"xmin\",\n",
        "                                 \"ymin\",\t\"xmax\",\t\"ymax\",\t\"filename\",\t\"path\",\t\"class\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P31JjHddVSEm",
        "colab": {}
      },
      "source": [
        "# Augment train images and bounding boxes\n",
        "# Then download train images to Google Drive and write new df with updated filenames and paths\n",
        "# Saved train images will be used with bounding box dimensions for future use with the object detection models\n",
        "\n",
        "# Optional: set seed to make augmentations reproducible across runs, otherwise will be random each time\n",
        "ia.seed(1) \n",
        "\n",
        "# Loop to perform image augmentation for each image in crops\n",
        "# First test on 5 images from crops\n",
        "#for i, row in crops.head(5).iterrows():\n",
        "# Next run on all rows\n",
        "for i, row in crops.iterrows():\n",
        "  try:\n",
        "    # Import image from url\n",
        "    # Use imread instead of imageio.imread to load images from url and get consistent output type and shape\n",
        "    url = crops.at[i, \"obj_url\"]\n",
        "    with urlopen(url) as file:\n",
        "      image = imread(file, mode='RGB')\n",
        "\n",
        "    # Import bounding box coordinates\n",
        "    bb  = ia.BoundingBox(x1=crops.xmin[i].astype(int), y1=crops.ymin[i].astype(int), \n",
        "        x2=crops.xmax[i].astype(int), y2=crops.ymax[i].astype(int))        \n",
        "    bb = BoundingBoxesOnImage([bb], shape=image.shape)\n",
        "    \n",
        "    # Augment image using settings defined above in seq\n",
        "    image_aug, bb_aug = seq.augment(image=image, bounding_boxes=bb)\n",
        "    \n",
        "    # Define augmentation results needed in exported dataset\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/images/'\n",
        "    path_aug = pathbase + str(crops.data_object_id[i]) + '_aug' + '.jpg'\n",
        "    filename_aug = str(crops.data_object_id[i]) + '_aug' + '.jpg'\n",
        "    obj_id = crops.data_object_id[i]\n",
        "    height, width, depth = image_aug.shape\n",
        "    xmin_aug = bb_aug.bounding_boxes[0].x1.astype(int)\n",
        "    ymin_aug = bb_aug.bounding_boxes[0].y1.astype(int)\n",
        "    xmax_aug = bb_aug.bounding_boxes[0].x2.astype(int)\n",
        "    ymax_aug = bb_aug.bounding_boxes[0].y2.astype(int)\n",
        "    # TO DO: Change to Anura, Coleoptera, Squamata, and Carnivora\n",
        "    name = str(\"Anura\")\n",
        "\n",
        "    # Export augmented images to Google Drive\n",
        "    misc.imsave(path_aug, image_aug)\n",
        "    \n",
        "    # Draw augmented bounding box and image\n",
        "    # Only use this for 20-30 images, otherwise comment out\n",
        "    #imagewbox = cv2.rectangle(image_aug, (xmin_aug, ymin_aug), \n",
        "                      #(xmax_aug, ymax_aug), \n",
        "                      #(255, 0, 157), 3) # change box color and thickness\n",
        "    #_, ax = plt.subplots(figsize=(10, 10))\n",
        "    #ax.imshow(imagewbox)\n",
        "    #plt.title('{}) Successfully augmented image from {}'.format(format(i+1, '.0f'), url))\n",
        "    \n",
        "    # Export augmentation results to crops_aug.tsv\n",
        "    # TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug.tsv\n",
        "    if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([crops.data_object_id[i], crops.obj_url[i], height, width,\n",
        "                                 xmin_aug, ymin_aug, xmax_aug, ymax_aug, filename_aug, path_aug, name])\n",
        "    \n",
        "    # Display message to track augmentation process by image\n",
        "    print('{}) Successfully augmented image from {}'.format(format(i+1, '.0f'), url))\n",
        "  \n",
        "  except:\n",
        "    print('{}) Error: check if web address for image from {} is valid'.format(format(i+1, '.0f'), url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdW2BS2tLcdD",
        "colab_type": "text"
      },
      "source": [
        "#### Make full training dataset by combining augmented and un-augmented bounding boxes and images   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSaSmJC8hc-j",
        "colab": {}
      },
      "source": [
        "# Download original (un-augmented) training images to Google Drive \n",
        "\n",
        "# For saving images to Google Drive\n",
        "from scipy import misc\n",
        "\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "for i, row in crops.iterrows():\n",
        "  try:\n",
        "    # Import image from url\n",
        "    # Use imread instead of imageio.imread to load images from url and get consistent output type and shape\n",
        "    url = crops.at[i, \"obj_url\"]\n",
        "    with urlopen(url) as file:\n",
        "      image = imread(file, mode='RGB')\n",
        "\n",
        "    # Define paths and filenames for augmented and unaugmented images\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/images/'\n",
        "    path = pathbase + str(crops.data_object_id[i]) + '.jpg'\n",
        "    filename = str(crops.data_object_id[i]) + '.jpg'\n",
        "     \n",
        "    # Export augmented images to Google Drive\n",
        "    misc.imsave(path, image)\n",
        "  \n",
        "    # Display message to track augmentation process by image\n",
        "    print('{}) Successfully downloaded original (un-augmented) image from {} to Google Drive'.format(format(i+1, '.0f'), url))\n",
        "  \n",
        "  except:\n",
        "    print('{}) Error: check if web address for image from {} is valid'.format(format(i+1, '.0f'), url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2p52DyDH5jv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create new df with original (un-augmented) bounding boxes and images that is formatted the same as the augmented data\n",
        "\n",
        "# Write header of crops_notaug.tsv before looping through crops for other data\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_notaug.tsv\n",
        "if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([\"data_object_id\",\t\"obj_url\",\t\"height\",\t\"width\",\t\"xmin\",\n",
        "                                 \"ymin\",\t\"xmax\",\t\"ymax\",\t\"filename\",\t\"path\",\t\"class\"])\n",
        "\n",
        "# Loop through crops to get images and bounding boxes\n",
        "for i, row in crops.iterrows():\n",
        "  try:\n",
        "    # Import images from crops\n",
        "    # Use imread instead of imageio.imread to load images from url and get consistent output type and shape\n",
        "    url = crops.at[i, \"obj_url\"]\n",
        "    with urlopen(url) as file:\n",
        "      image = imread(file, mode='RGB')\n",
        "    height, width, depth = image.shape\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/images/'\n",
        "    path = pathbase + str(crops.data_object_id[i]) + '.jpg'\n",
        "    filename = str(crops.data_object_id[i]) + '.jpg'\n",
        "    # TO DO: Change to Anura, Coleoptera, Squamata, and Carnivora\n",
        "    name = str(\"Anura\")\n",
        "    \n",
        "    # Write results to crops_notaug.tsv\n",
        "    # TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_notaug.tsv\n",
        "    if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([crops.data_object_id[i], crops.obj_url[i], height, width, \n",
        "                                 crops.xmin[i], crops.ymin[i], crops.xmax[i], crops.ymax[i], filename, path, name])\n",
        "    \n",
        "    # Display message to track augmentation process by image\n",
        "    print('{}) Successfully loaded image from {}'.format(format(i+1, '.0f'), url))\n",
        "  \n",
        "  except:\n",
        "    print('{}) Error: check if web address for image from {} is valid'.format(format(i+1, '.0f'), url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nME32iVVn1mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine augmented and un-augmented datasets to make one full training dataset\n",
        "\n",
        "# File names to be combined\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug.tsv and _crops_train_notaug.tsv\n",
        "all_filenames = [\"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_notaug.tsv\"]\n",
        "\n",
        "# Combine all files in the list\n",
        "combined = pd.concat([pd.read_csv(f, sep='\\t') for f in all_filenames])\n",
        "\n",
        "# Export to tsv\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug_all.tsv\n",
        "combined.to_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug_all.tsv\", index=False, sep='\\t')\n",
        "print(combined.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2USBcEu3osBM",
        "colab_type": "text"
      },
      "source": [
        "#### Remove out of bounds values from train crops and export results for use with object detection models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yJvWJ7_CbX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in crops_all.tsv from above\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug_all.tsv\n",
        "allcrops = pd.read_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug_all.tsv\", sep='\\t')\n",
        "print(allcrops.head())\n",
        "\n",
        "# Set negative values to 0\n",
        "allcrops.xmin[allcrops.xmin < 0] = 0\n",
        "allcrops.ymin[allcrops.ymin < 0] = 0\n",
        "\n",
        "# Remove out of bounds cropping dimensions\n",
        "for i, row in allcrops.iterrows():\n",
        "    # When crop height > image height, set crop height equal to image height:\n",
        "    if allcrops.ymax[i] > allcrops.height[i]:\n",
        "            allcrops.ymin[i] = 0\n",
        "            allcrops.ymax[i] = allcrops.height[i]\n",
        "\n",
        "for i, row in allcrops.iterrows(): \n",
        "    # When crop width > image width, set crop width equal to image width:\n",
        "    if allcrops.xmax[i] > allcrops.width[i]:\n",
        "        allcrops.xmin[i] = 0\n",
        "        allcrops.xmax[i] = allcrops.width[i]\n",
        "\n",
        "# Write results to tsv for records with all info\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug_all_transf.tsv\n",
        "allcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug_all_transf.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by tensorflow and yolo\n",
        "df1 = allcrops.iloc[:, 4:8]\n",
        "df2 = allcrops[['filename', 'width', 'height', 'class']]\n",
        "traincrops = pd.concat([df2, df1], axis=1)\n",
        "traincrops.insert(0, 'folder', 'images')\n",
        "print(traincrops.head())\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug_fin.tsv\n",
        "traincrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug_fin.csv', sep=',', index=False)\n",
        "\n",
        "# Write relevant results to tsv formatted for training and annotations needed by yolo\n",
        "traincrops = allcrops[['filename', 'path', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax', 'class']]\n",
        "traincrops.rename(columns={'class': 'name'}, inplace=True)\n",
        "traincrops.insert(0, 'folder', 'images')\n",
        "# Remove leading '/' from filepaths (format needed for xmls)\n",
        "traincrops['path'] = traincrops['path'].str.lstrip('/')\n",
        "print(traincrops.head())\n",
        "# TO DO: Change to anura, coleoptera, squamata, and carnivora _crops_train_aug_fin_foreli.tsv\n",
        "traincrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug_fin_foreli.tsv', sep='\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eccB6nqb8kpk",
        "colab_type": "text"
      },
      "source": [
        "#### Make multitaxa training dataset by combining train files for all taxa\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nULT2Xy4949k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine augmented and un-augmented datasets to make pooled multitaxa training dataset\n",
        "# Files to be combined\n",
        "all_filenames = [\"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_train.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_train.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_train.tsv\"]\n",
        "# Combine all files in the list\n",
        "combined = pd.concat([pd.read_csv(f, sep='\\t') for f in all_filenames])\n",
        "# Write results to tsv\n",
        "combined.to_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_train.tsv\", index=False, sep='\\t')\n",
        "print(combined.head())\n",
        "\n",
        "# Combine final, transformed datasets to make pooled multitaxa training dataset for Tensorflow models\n",
        "# Files to be combined\n",
        "all_filenames = [\"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_train_aug_fin.csv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_train_aug_fin.csv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug_fin.csv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_train_aug_fin.csv\"]\n",
        "# Combine all files in the list\n",
        "combined = pd.concat([pd.read_csv(f, sep=',') for f in all_filenames])\n",
        "# Write results to tsv\n",
        "combined.to_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_train_aug_all_fin.csv\", index=False, sep=',')\n",
        "print(combined.head())\n",
        "\n",
        "# Combine final, transformed datasets to make pooled multitaxa training dataset for YOLO\n",
        "# Files to be combined\n",
        "all_filenames = [\"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_train_aug_fin_foreli.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_train_aug_fin_foreli.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_train_aug_fin_foreli.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_train_aug_fin_foreli.tsv\"]\n",
        "# Combine all files in the list\n",
        "combined = pd.concat([pd.read_csv(f, sep='\\t') for f in all_filenames])\n",
        "# Write results to tsv\n",
        "combined.to_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_train_aug_all_fin_foreli.tsv\", index=False, sep='\\t')\n",
        "print(combined.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7JGKXra8TTQ",
        "colab_type": "text"
      },
      "source": [
        "### Test Images\n",
        "---\n",
        "Run blocks to prepare testing datasets for each taxon separately and to make one pooled taxa dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCRv71xhRAe3",
        "colab_type": "text"
      },
      "source": [
        "#### Squamata (lizards, snakes)\n",
        "---\n",
        "Download test images to Google Drive and write new dataframe with image filenames and paths used to prepare image annotation files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW7oK8U_SPnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saved test images will be used with bounding box dimensions for future use with the object detection models\n",
        "\n",
        "from scipy import misc\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Read in EOL images and user-generated cropping coordinate testing data\n",
        "crops_test = pd.read_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test.tsv', sep='\\t', header=0)\n",
        "crops_test.head()\n",
        "\n",
        "# Write header of crops_test_transf.tsv before looping through crops for other data\n",
        "#if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        #with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            #tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            #tsv_writer.writerow([\"data_object_id\",\t\"obj_url\",\t\"height\",\t\"width\",\t\"xmin\",\n",
        "                                 #\"ymin\",\t\"xmax\",\t\"ymax\",\t\"filename\",\t\"path\",\t\"class\"])\n",
        "\n",
        "# Loop through crop testing data\n",
        "for i, row in crops_test.iterrows():\n",
        "  try:\n",
        "    # Import image from url\n",
        "    # Use imread instead of imageio.imread to load images from url and get consistent output type and shape\n",
        "    url = crops_test.at[i, \"obj_url\"]\n",
        "    with urlopen(url) as file:\n",
        "      image = imread(file, mode='RGB')\n",
        "\n",
        "    # Define variables needed in exported dataset\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images_squamata/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    filename = str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    obj_id = crops_test.data_object_id[i]\n",
        "    height, width, depth = image.shape\n",
        "    name = str(\"Squamata\")\n",
        "    \n",
        "    # Export image to Google Drive test_images_taxon/ for testing by each taxon\n",
        "    #misc.imsave(path, image)\n",
        "    \n",
        "    # Make path for test images file set to test_images folder, can change test_images_squamata to test_images if testing with only squamata\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    \n",
        "    # Export image to Google Drive test_images/ for testing with multitaxa images (all taxa pooled)\n",
        "    misc.imsave(path, image)\n",
        "\n",
        "    # Export to crops_test.tsv\n",
        "    #if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        #with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            #tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            #tsv_writer.writerow([crops_test.data_object_id[i], crops_test.obj_url[i], height, width, \n",
        "                                 #crops_test.xmin[i], crops_test.ymin[i], crops_test.xmax[i], crops_test.ymax[i], filename, path, name])\n",
        "    \n",
        "    # Display message to track download process by image\n",
        "    print('{}) Successfully downloaded image from {}'.format(format(i+1, '.0f'), url))\n",
        "  \n",
        "  except:\n",
        "    print('{}) Error: check if web address for image from {} is valid'.format(format(i+1, '.0f'), url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu1VmY8r31K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in crops_test_notaug.tsv from above\n",
        "crops = pd.read_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug.tsv\", sep='\\t')\n",
        "print(crops.head())\n",
        "\n",
        "# Remove out of bounds (OOB) cropping dimensions\n",
        "# Set negative values (OOB -) equal to 0\n",
        "crops.xmin[crops.xmin < 0] = 0\n",
        "crops.ymin[crops.ymin < 0] = 0\n",
        "# Set positive out of bounds values (OOB +) equal to image dimensions\n",
        "for i, row in crops.iterrows():\n",
        "    # When crop height > image height, set crop height equal to image height:\n",
        "    if crops.ymax[i] > crops.height[i]:\n",
        "        crops.ymax[i] = crops.height[i]\n",
        "    # When crop width > image width, set crop width equal to image width:\n",
        "    if crops.xmax[i] > crops.width[i]:\n",
        "        crops.xmax[i] = crops.width[i]            \n",
        "\n",
        "# Write results to tsv for records with all info\n",
        "crops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug_transf.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by tensorflow\n",
        "df1 = crops.iloc[:, 4:8]\n",
        "df2 = crops[['filename', 'width', 'height', 'class']]\n",
        "testcrops = pd.concat([df2, df1], axis=1)\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug_fin.csv', sep=',', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by yolo\n",
        "testcrops = crops[['filename', 'path', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax', 'class']]\n",
        "testcrops.rename(columns={'class': 'name'}, inplace=True)\n",
        "# Remove leading '/' from filepaths (format needed for xmls)\n",
        "testcrops['path'] = testcrops['path'].str.lstrip('/')\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "\n",
        "# Write results to tsv\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug_fin_foreli.tsv', sep='\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30wVSM5TR2eH",
        "colab_type": "text"
      },
      "source": [
        "#### Coleoptera (beetles)\n",
        "---\n",
        "Download test images to Google Drive and write new dataframe with image filenames and paths used to prepare image annotation files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qby1_lpJSQmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saved test images will be used with bounding box dimensions for future use with the object detection models\n",
        "\n",
        "from scipy import misc\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Read in EOL images and user-generated cropping coordinate testing data\n",
        "crops_test = pd.read_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test.tsv', sep='\\t', header=0)\n",
        "crops_test.head()\n",
        "\n",
        "# Write header of crops_test_transf.tsv before looping through crops for other data\n",
        "if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([\"data_object_id\",\t\"obj_url\",\t\"height\",\t\"width\",\t\"xmin\",\n",
        "                                 \"ymin\",\t\"xmax\",\t\"ymax\",\t\"filename\",\t\"path\",\t\"class\"])\n",
        "\n",
        "# Loop through crop testing data\n",
        "for i, row in crops_test.iterrows():\n",
        "  try:\n",
        "    # Import image from url\n",
        "    # Use imread instead of imageio.imread to load images from url and get consistent output type and shape\n",
        "    url = crops_test.at[i, \"obj_url\"]\n",
        "    with urlopen(url) as file:\n",
        "      image = imread(file, mode='RGB')\n",
        "\n",
        "    # Define variables needed in exported dataset\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images_coleoptera/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    filename = str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    obj_id = crops_test.data_object_id[i]\n",
        "    height, width, depth = image.shape\n",
        "    name = str(\"Coleoptera\")\n",
        "\n",
        "    # Export image to Google Drive test_images_taxon/ for testing by each taxon\n",
        "    misc.imsave(path, image)\n",
        "    \n",
        "    # Make path for test images file set to test_images folder, can change test_images_coleoptera to test_images if testing with only coleopterans\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    \n",
        "    # Export image to Google Drive test_images/ for testing with multitaxa images (all taxa pooled)\n",
        "    misc.imsave(path, image)\n",
        "\n",
        "    # Export to crops_test.tsv\n",
        "    if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([crops_test.data_object_id[i], crops_test.obj_url[i], height, width, \n",
        "                                 crops_test.xmin[i], crops_test.ymin[i], crops_test.xmax[i], crops_test.ymax[i], filename, path, name])\n",
        "    \n",
        "    # Display message to track download process by image\n",
        "    print('{}) Successfully downloaded image from {}'.format(format(i+1, '.0f'), url))\n",
        "  \n",
        "  except:\n",
        "    print('{}) Error: check if web address for image from {} is valid'.format(format(i+1, '.0f'), url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvn2xUjs30Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in crops_test_notaug.tsv from above\n",
        "crops = pd.read_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug.tsv\", sep='\\t')\n",
        "print(crops.head())\n",
        "\n",
        "# Remove out of bounds (OOB) cropping dimensions\n",
        "# Set negative values (OOB -) equal to 0\n",
        "crops.xmin[crops.xmin < 0] = 0\n",
        "crops.ymin[crops.ymin < 0] = 0\n",
        "# Set positive out of bounds values (OOB +) equal to image dimensions\n",
        "for i, row in crops.iterrows():\n",
        "    # When crop height > image height, set crop height equal to image height:\n",
        "    if crops.ymax[i] > crops.height[i]:\n",
        "        crops.ymax[i] = crops.height[i]\n",
        "    # When crop width > image width, set crop width equal to image width:\n",
        "    if crops.xmax[i] > crops.width[i]:\n",
        "        crops.xmax[i] = crops.width[i]            \n",
        "\n",
        "# Write results to tsv for records with all info\n",
        "crops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug_transf.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by tensorflow\n",
        "df1 = crops.iloc[:, 4:8]\n",
        "df2 = crops[['filename', 'width', 'height', 'class']]\n",
        "testcrops = pd.concat([df2, df1], axis=1)\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug_fin.csv', sep=',', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by yolo\n",
        "testcrops = crops[['filename', 'path', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax', 'class']]\n",
        "testcrops.rename(columns={'class': 'name'}, inplace=True)\n",
        "# Remove leading / from filepaths (format needed for xmls)\n",
        "testcrops['path'] = testcrops['path'].str.lstrip('/')\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "\n",
        "# Write results to tsv\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug_fin_foreli.tsv', sep='\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw5RCgM-R2xd",
        "colab_type": "text"
      },
      "source": [
        "#### Anura (frogs)\n",
        "---\n",
        "Download test images to Google Drive and write new dataframe with image filenames and paths used to prepare image annotation files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEwoNKzQSRYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saved test images will be used with bounding box dimensions for future use with the object detection models\n",
        "\n",
        "from scipy import misc\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Read in EOL images and user-generated cropping coordinate testing data\n",
        "crops_test = pd.read_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test.tsv', sep='\\t', header=0)\n",
        "crops_test.head()\n",
        "\n",
        "# Write header of crops_test_transf.tsv before looping through crops for other data\n",
        "if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([\"data_object_id\",\t\"obj_url\",\t\"height\",\t\"width\",\t\"xmin\",\n",
        "                                 \"ymin\",\t\"xmax\",\t\"ymax\",\t\"filename\",\t\"path\",\t\"class\"])\n",
        "\n",
        "# Loop through crop testing data\n",
        "for i, row in crops_test.iterrows():\n",
        "  try:\n",
        "    # Import image from url\n",
        "    # Use imread instead of imageio.imread to load images from url and get consistent output type and shape\n",
        "    url = crops_test.at[i, \"obj_url\"]\n",
        "    with urlopen(url) as file:\n",
        "      image = imread(file, mode='RGB')\n",
        "\n",
        "    # Define variables needed in exported dataset\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images_anura/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    filename = str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    obj_id = crops_test.data_object_id[i]\n",
        "    height, width, depth = image.shape\n",
        "    name = str(\"Anura\")\n",
        "\n",
        "    # Export image to Google Drive test_images_taxon/ for testing by each taxon \n",
        "    misc.imsave(path, image)\n",
        "    \n",
        "    # Make path for test images file set to test_images folder, can change test_images_anura to test_images if testing with only anurans\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    \n",
        "    # Export image to Google Drive test_images/ for testing with multitaxa images (all taxa pooled)\n",
        "    misc.imsave(path, image)\n",
        "\n",
        "    # Export to crops_test.tsv\n",
        "    if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([crops_test.data_object_id[i], crops_test.obj_url[i], height, width, \n",
        "                                 crops_test.xmin[i], crops_test.ymin[i], crops_test.xmax[i], crops_test.ymax[i], filename, path, name])\n",
        "    \n",
        "    # Display message to track download process by image\n",
        "    print('{}) Successfully downloaded image from {}'.format(format(i+1, '.0f'), url))\n",
        "  \n",
        "  except:\n",
        "    print('{}) Error: check if web address for image from {} is valid'.format(format(i+1, '.0f'), url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOKz1UmB3vKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in crops_test_notaug.tsv from above\n",
        "crops = pd.read_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug.tsv\", sep='\\t')\n",
        "print(crops.head())\n",
        "\n",
        "# Remove out of bounds (OOB) cropping dimensions\n",
        "# Set negative values (OOB -) equal to 0\n",
        "crops.xmin[crops.xmin < 0] = 0\n",
        "crops.ymin[crops.ymin < 0] = 0\n",
        "# Set positive out of bounds values (OOB +) equal to image dimensions\n",
        "for i, row in crops.iterrows():\n",
        "    # When crop height > image height, set crop height equal to image height:\n",
        "    if crops.ymax[i] > crops.height[i]:\n",
        "        crops.ymax[i] = crops.height[i]\n",
        "    # When crop width > image width, set crop width equal to image width:\n",
        "    if crops.xmax[i] > crops.width[i]:\n",
        "        crops.xmax[i] = crops.width[i]            \n",
        "\n",
        "# Write results to tsv for records with all info\n",
        "crops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug_transf.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by tensorflow\n",
        "df1 = crops.iloc[:, 4:8]\n",
        "df2 = crops[['filename', 'width', 'height', 'class']]\n",
        "testcrops = pd.concat([df2, df1], axis=1)\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug_fin.csv', sep=',', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by yolo\n",
        "testcrops = crops[['filename', 'path', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax', 'class']]\n",
        "testcrops.rename(columns={'class': 'name'}, inplace=True)\n",
        "# Remove leading / from filepaths (format needed for xmls)\n",
        "testcrops['path'] = testcrops['path'].str.lstrip('/')\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "\n",
        "# Write results to tsv\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug_fin_foreli.tsv', sep='\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FViwUUd8R26z",
        "colab_type": "text"
      },
      "source": [
        "#### Carnivora (carnivores)\n",
        "---\n",
        "Download test images to Google Drive and write new dataframe with image filenames and paths used to prepare image annotation files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDzSp9jv8Sak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saved test images will be used with bounding box dimensions for future use with the object detection models\n",
        "\n",
        "from scipy import misc\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Read in EOL images and user-generated cropping coordinate testing data\n",
        "crops_test = pd.read_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test.tsv', sep='\\t', header=0)\n",
        "crops_test.head()\n",
        "\n",
        "# Write header of crops_test_transf.tsv before looping through crops for other data\n",
        "if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([\"data_object_id\",\t\"obj_url\",\t\"height\",\t\"width\",\t\"xmin\",\n",
        "                                 \"ymin\",\t\"xmax\",\t\"ymax\",\t\"filename\",\t\"path\",\t\"class\"])\n",
        "\n",
        "# Loop through crop testing data\n",
        "for i, row in crops_test.iterrows():\n",
        "  try:\n",
        "    # Import image from url\n",
        "    # Use imread instead of imageio.imread to load images from url and get consistent output type and shape\n",
        "    url = crops_test.at[i, \"obj_url\"]\n",
        "    with urlopen(url) as file:\n",
        "      image = imread(file, mode='RGB')\n",
        "\n",
        "    # Define variables needed in exported dataset\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images_carnivora/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    filename = str(crops_test.data_object_id[i]) + '.jpg'\n",
        "    obj_id = crops_test.data_object_id[i]\n",
        "    height, width, depth = image.shape\n",
        "    name = str(\"Carnivora\")\n",
        "    \n",
        "    # Export image to Google Drive test_images_taxon/ for testing by each taxon\n",
        "    misc.imsave(path, image)\n",
        "\n",
        "    # Make path for test images file set to test_images folder, can change test_images_carnivora to test_images if testing with only carnivores\n",
        "    pathbase = '/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images/'\n",
        "    path = pathbase + str(crops_test.data_object_id[i]) + '.jpg'\n",
        "\n",
        "    # Export image to Google Drive test_images/ for testing with multitaxa images (all taxa pooled)\n",
        "    misc.imsave(path, image)\n",
        "\n",
        "    # Export to crops_test.tsv\n",
        "    if os.path.exists('/content/drive/My Drive/fall19_smithsonian_informatics/train'):\n",
        "        with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug.tsv', 'a') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow([crops_test.data_object_id[i], crops_test.obj_url[i], height, width, \n",
        "                                 crops_test.xmin[i], crops_test.ymin[i], crops_test.xmax[i], crops_test.ymax[i], filename, path, name])\n",
        "    \n",
        "    # Display message to track download process by image\n",
        "    print('{}) Successfully downloaded image from {}'.format(format(i+1, '.0f'), url))\n",
        "  \n",
        "  except:\n",
        "    print('{}) Error: check if web address for image from {} is valid'.format(format(i+1, '.0f'), url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6Jmv1olta_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in crops_test_notaug.tsv from above\n",
        "crops = pd.read_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug.tsv\", sep='\\t')\n",
        "print(crops.head())\n",
        "\n",
        "# Remove out of bounds (OOB) cropping dimensions\n",
        "# Set negative values (OOB -) equal to 0\n",
        "crops.xmin[crops.xmin < 0] = 0\n",
        "crops.ymin[crops.ymin < 0] = 0\n",
        "# Set positive out of bounds values (OOB +) equal to image dimensions\n",
        "for i, row in crops.iterrows():\n",
        "    # When crop height > image height, set crop height equal to image height:\n",
        "    if crops.ymax[i] > crops.height[i]:\n",
        "        crops.ymax[i] = crops.height[i]\n",
        "    # When crop width > image width, set crop width equal to image width:\n",
        "    if crops.xmax[i] > crops.width[i]:\n",
        "        crops.xmax[i] = crops.width[i]            \n",
        "\n",
        "# Write results to tsv for records with all info\n",
        "crops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug_transf.tsv', sep='\\t', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by tensorflow\n",
        "df1 = crops.iloc[:, 4:8]\n",
        "df2 = crops[['filename', 'width', 'height', 'class']]\n",
        "testcrops = pd.concat([df2, df1], axis=1)\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug_fin.csv', sep=',', index=False)\n",
        "\n",
        "# Write relevant results to csv formatted for training and annotations needed by yolo\n",
        "testcrops = crops[['filename', 'path', 'width', 'height', 'xmin', 'ymin', 'xmax', 'ymax', 'class']]\n",
        "testcrops.rename(columns={'class': 'name'}, inplace=True)\n",
        "# Remove leading / from filepaths (format needed for xmls)\n",
        "testcrops['path'] = testcrops['path'].str.lstrip('/')\n",
        "testcrops.insert(0, 'folder', 'test_images')\n",
        "print(testcrops.head())\n",
        "\n",
        "# Write results to tsv\n",
        "testcrops.to_csv('/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug_fin_foreli.tsv', sep='\\t', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxKEXLBlpON0",
        "colab_type": "text"
      },
      "source": [
        "### Multitaxa (all groups pooled)\n",
        "---   \n",
        "Make pooled multitaxa test image datasets by combining test datasets for all taxa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chn3aLJ__FxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make original multitaxa test image dataset\n",
        "# Files to be combined\n",
        "all_filenames = [\"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test.tsv\"]\n",
        "# Combine all files in the list\n",
        "combined = pd.concat([pd.read_csv(f, sep='\\t') for f in all_filenames])\n",
        "# Export to tsv\n",
        "combined.to_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_test.tsv\", index=False, sep='\\t')\n",
        "print(combined.head())\n",
        "\n",
        "# Make final multitaxa test test image dataset (for making R-FCN and SSD annotation data) by combining test datasets for all taxa\n",
        "# Files to be combined\n",
        "all_filenames = [\"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug_fin.csv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug_fin.csv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug_fin.csv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug_fin.csv\"]\n",
        "# Combine all files in the list\n",
        "combined = pd.concat([pd.read_csv(f, sep=',') for f in all_filenames])\n",
        "# Export to tsv\n",
        "combined.to_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_test_notaug_fin.csv.csv\", index=False, sep=',')\n",
        "print(combined.head())\n",
        "\n",
        "# Make final multitaxa test dataset (for use by Eli to make YOLO annotation xmls) by combining test datasets for all taxa\n",
        "# Files to be combined\n",
        "all_filenames = [\"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/squamata_crops_test_notaug_fin_foreli.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/coleoptera_crops_test_notaug_fin_foreli.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/anura_crops_test_notaug_fin_foreli.tsv\",\n",
        "                 \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/carnivora_crops_test_notaug_fin_foreli.tsv\"]\n",
        "# Combine all files in the list\n",
        "combined = pd.concat([pd.read_csv(f, sep='\\t') for f in all_filenames])\n",
        "\n",
        "# Export to tsv\n",
        "combined.to_csv( \"/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_test_notaug_fin_foreli.tsv\", index=False, sep='\\t')\n",
        "print(combined.head())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}