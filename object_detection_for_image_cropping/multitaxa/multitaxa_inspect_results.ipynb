{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_inspect_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv89gfSGjs3B"
      },
      "source": [
        "# Compare model outputs with known crops\n",
        "---   \n",
        "*Last Updated 13 Feb 2025*  \n",
        "-Now runs in Python 3 with Tensorflow 2.0-     \n",
        "\n",
        "Compare trained model predicted and post-processed square crops with known square crops from EOL user generated test data.\n",
        "\n",
        "Use trained object detection models to automatically crop images of snakes & lizards (Squamata), beetles (Coleoptera), frogs (Anura), and carnivores (Carnivora) to square dimensions centered around animal(s).\n",
        "\n",
        "Models were trained and saved to Google Drive in [multitaxa_train_tf2_rcnns.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_tf2_rcnns.ipynb).\n",
        "\n",
        "***Models were trained in Python 2 and TF 1 in April 2020: Faster RCNN ResNet 50 trained for 12 hours to 200,000 steps and Faster RCNN Inception v2 for 18 hours to 200,000 steps.***\n",
        "\n",
        "Notes:   \n",
        "* Run code blocks by pressing play button in brackets on left\n",
        "* Before you you start: change the runtime to \"GPU\" with \"High RAM\"\n",
        "* Change parameters using form fields on right (find details at corresponding lines of code by searching '#@param')\n",
        "\n",
        "References:     \n",
        "* [Official Tensorflow Object Detection API Instructions](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)   \n",
        "* [Medium Blog on training using Tensorflow Object Detection API in Colab](https://medium.com/analytics-vidhya/training-an-object-detection-model-with-tensorflow-api-using-google-colab-4f9a688d5e8b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-5awa5YDAjy"
      },
      "source": [
        "## Installs & Imports\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VooXJOohjmLB"
      },
      "outputs": [],
      "source": [
        "#@title Choose where to save results\n",
        "# Use dropdown menu on right\n",
        "save = \"in Colab runtime (files deleted after each session)\" #@param [\"in my Google Drive\", \"in Colab runtime (files deleted after each session)\"]\n",
        "\n",
        "# Mount google drive to export image cropping coordinate file(s)\n",
        "if 'Google Drive' in save:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Note: You can modify \"filter\" to choose detection results for any class of interest the model is trained on\n",
        "filter = \"Multitaxa\" # @param [\"Multitaxa\"] {\"allow-input\":true}\n",
        "\n",
        "# Type in the path to your project wd in form field on right\n",
        "basewd = \"/content/drive/MyDrive/train\" #@param [\"/content/drive/MyDrive/train\"] {allow-input: true}\n",
        "# Type in the folder that you want to contain TF2 files\n",
        "folder = \"tf2\" #@param [\"tf2\"] {allow-input: true}\n",
        "# Define current working directory using form field inputs\n",
        "cwd = basewd + '/' + folder + '/' + filter\n",
        "\n",
        "# Install dependencies\n",
        "!pip3 install --upgrade gdown\n",
        "!gdown 1-0F-zKYOAV1qk2hu7kqKvlMo5Xsj6Uut # Download helper_funcs folder\n",
        "!tar -xzvf multitaxa_helper_funcs.tar.gz -C .\n",
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6zYEeg2C79B"
      },
      "outputs": [],
      "source": [
        "# TO DO figure out requirements.txt and fix warnings\n",
        "!pip install numpy==1.24.3\n",
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqADYAPDDF8a"
      },
      "outputs": [],
      "source": [
        "#@title Choose model parameters, set up directory structure, and build Tensorflow Object Detection API\n",
        "\n",
        "# Use EOL pre-trained model for object detection?\n",
        "use_EOL_model = True #@param {type: \"boolean\"}\n",
        "\n",
        "# If using your own trained model, change values to match your trained model\n",
        "filters = [\"Anura\", \"Carnivora\", \"Coleoptera\", \"Squamata\"] #@param [\"[\\\"Anura\\\", \\\"Carnivora\\\", \\\"Coleoptera\\\", \\\"Squamata\\\"]\"] {type:\"raw\", allow-input: true}\n",
        "PATH_TO_LABELS = \"labelmap.pbtxt\" #@param {type:\"string\"}\n",
        "NUM_CLASSES = 4 #@param\n",
        "saved_models_dir = \"tf_models/train_demo/rcnn_i/finetuned_model/\" #@param [\"tf_models/train_demo/rcnn/finetuned_model/\"] {allow-input: true}\n",
        "mod_abbv = \"rcnn_i\"\n",
        "\n",
        "# For working with directories\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "# For downloading and displaying images\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "# For object detection\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Import EOL custom helper_funcs\n",
        "from setup import *\n",
        "from wrangle_data import *\n",
        "\n",
        "# Clone Tensorflow Object Detection Github Repo\n",
        "setup_dirs(cwd)\n",
        "\n",
        "# Download Multitaxa_crops_test.tsv\n",
        "%cd $cwd\n",
        "%cd results\n",
        "!gdown 18SRUuOxKvLdJOt3fq9z_jmfBpVBbFLF2\n",
        "%cd $cwd\n",
        "\n",
        "# Build Tensorflow Object Detection API\n",
        "!sudo apt install -y protobuf-compiler\n",
        "!cd tf_models/models/research/ && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python -m pip install .\n",
        "%cd $cwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwrKVbSSDHnm"
      },
      "outputs": [],
      "source": [
        "# Build saved model\n",
        "\n",
        "# For object detection\n",
        "import sys\n",
        "sys.path.append('/content')\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# For downloading and displaying images\n",
        "import cv2\n",
        "import tempfile\n",
        "import urllib\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from IPython.display import display\n",
        "\n",
        "# For drawing onto images\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "# For measuring inference time\n",
        "import time\n",
        "\n",
        "# For working with data\n",
        "import subprocess\n",
        "import csv\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "# Print Tensorflow version\n",
        "print('\\nTensorflow Version: %s' % tf.__version__)\n",
        "\n",
        "# Check available GPU devices\n",
        "print('The following GPU devices are available: %s' % tf.test.gpu_device_name())\n",
        "\n",
        "# Unpack EOL saved model\n",
        "PATH_TO_CKPT = saved_models_dir + 'frozen_inference_graph.pb'\n",
        "detector = detection_graph = unpack_EOL_model(use_EOL_model, saved_models_dir, PATH_TO_CKPT, cwd)\n",
        "\n",
        "# Load saved model and label map\n",
        "print(\"\\nLoading label map for {} class(es) from: \\n{}\".format(NUM_CLASSES, PATH_TO_LABELS))\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyeMFYrlDVGS"
      },
      "source": [
        "## Generate crops: Run inference on EOL images & save resulting coordinates for cropping - 1K images\n",
        "---\n",
        "Use 20K EOL image bundle to generate bounding boxes around each object with pre-trained object detection models. Results are saved to [crops_file].tsv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdk2mI5fDXZQ"
      },
      "outputs": [],
      "source": [
        "#@title Define functions\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the maximum number of detections to keep per image\n",
        "max_boxes = 10 #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "# Set the minimum confidence score for detections to keep per image\n",
        "min_score = 0.6 #@param {type:\"slider\", min:0, max:0.9, step:0.1}\n",
        "\n",
        "# Set filename for saving classification results\n",
        "def set_outpath(crops_file, cwd):\n",
        "    outpath = cwd + '/' + 'results/' + os.path.splitext(crops_file)[0] + '_' + mod_abbv + '.tsv'\n",
        "    print(\"\\nSaving results to: \\n\", outpath)\n",
        "\n",
        "    return outpath\n",
        "\n",
        "# Export object detection results\n",
        "def export_results(image_url, result, outfpath, im_h, im_w, filter=filters):\n",
        "    with open(outfpath, 'a') as out_file:\n",
        "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "        img_id = os.path.splitext((os.path.basename(image_url)))\n",
        "        # Write one row per detected object with bounding box coordinates\n",
        "        num_detections = min(int(result[\"num_detections\"][0]), max_boxes)\n",
        "        for i in range(0, num_detections):\n",
        "            class_name = category_index[result[\"detection_classes\"][0][i]]['name']\n",
        "            if any(fil in class_name for fil in filters): # Only writes rows for filtered class\n",
        "                ymin = result[\"detection_boxes\"][0][i][0]\n",
        "                xmin = result[\"detection_boxes\"][0][i][1]\n",
        "                ymax = result[\"detection_boxes\"][0][i][2]\n",
        "                xmax = result[\"detection_boxes\"][0][i][3]\n",
        "                confidence = result[\"detection_scores\"][0][i]\n",
        "                tsv_writer.writerow([img_id, class_name, confidence,\n",
        "                          xmin, ymin, xmax, ymax, im_h, im_w, image_url])\n",
        "        print(\"\\nObject detection results for Image {} saved to: {}\".format(image_url, outfpath))\n",
        "\n",
        "    return img_id\n",
        "\n",
        "# Format cropping dimensions to EOL standards\n",
        "def format_crops_for_eol(df):\n",
        "# {\"height\":\"423\",\"width\":\"640\",\"crop_x\":123.712,\"crop_y\":53.4249,\"crop_width\":352,\"crop_height\":0}\n",
        "    df['crop_dimensions'] = np.nan\n",
        "    for i, row in df.iterrows():\n",
        "        df.loc[i, 'crop_dimensions'] = ('{{\"height\":\"{}\",\"width\":\"{}\",\"crop_x\":{},\"crop_y\":{},\"crop_width\":{},\"crop_height\":{}}}'\n",
        "        .format(df.im_height[i], df.im_width[i], df.xmin[i], df.ymin[i], df.crop_width[i], df.crop_height[i]))\n",
        "\n",
        "    # Add other dataframe elements from cols: identifier, dataobjectversionid, eolmediaurl, im_class, crop_dimensions\n",
        "    eol_crops = pd.DataFrame(df.iloc[:,np.r_[-5,-4,-6,0,-1]])\n",
        "    print(\"\\n EOL formatted cropping dimensions: \\n\", eol_crops.head())\n",
        "\n",
        "    return eol_crops\n",
        "\n",
        "print('Model loaded and functions defined! \\nGo to next steps to run inference on images.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiy_g1NaDqc8"
      },
      "source": [
        "### Generate predictions (crops): Run inference on EOL test images with known ground truths\n",
        "Use EOL Multitaxa test images to get bounding boxes of detected animals. Results are saved to [crops_file].tsv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZVQSNQpMI0v"
      },
      "outputs": [],
      "source": [
        "#@title Enter EOL cropping test image tsv and choose inference settings\n",
        "%cd $cwd\n",
        "\n",
        "# Load in EOL image bundle\n",
        "bundle = \"Multitaxa_crops_test.tsv\" #@param {type:\"string\"}\n",
        "bundle_path = 'results/' + bundle\n",
        "df1 = read_datafile(bundle_path, sep='\\t', header=0, disp_head=False)\n",
        "ground_truth = df1.copy()\n",
        "df = pd.DataFrame(df1['obj_url'])\n",
        "df.columns = ['url']\n",
        "print('\\n EOL test images head:\\n{}'.format(df.head()))\n",
        "\n",
        "# Test pipeline with a smaller subset than 5k images?\n",
        "run = \"test with tiny subset\" #@param [\"test with tiny subset\", \"for all images\"]\n",
        "\n",
        "# Display detection results on images?\n",
        "if 'tiny subset' in run:\n",
        "    display_results = True\n",
        "else:\n",
        "    display_results = False\n",
        "\n",
        "# Run inference on EOL test images (with known ground truths)\n",
        "crops_file = \"multitaxa_cropcoords_tf2_d\" #@param [\"multitaxa_cropcoords_tf2_a\", \"multitaxa_cropcoords_tf2_b\", \"multitaxa_cropcoords_tf2_c\", \"multitaxa_cropcoords_tf2_d\"] {allow-input: true}\n",
        "outfpath = set_outpath(crops_file, cwd)\n",
        "\n",
        "# Write header row of output tag file\n",
        "if not os.path.isfile(outfpath):\n",
        "    with open(outfpath, 'a') as out_file:\n",
        "              tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "              tsv_writer.writerow([\"img_id\", \"class_name\", \"confidence\", \"xmin\", \\\n",
        "                                   \"ymin\", \"xmax\", \"ymax\", \"im_width\", \\\n",
        "                                   \"im_height\", \"url\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJVvyl4vDxQw"
      },
      "outputs": [],
      "source": [
        "#@title Run EOL test images through trained model and save results\n",
        "print(\"Running inference on images\")\n",
        "all_predictions = []\n",
        "stop = len(df)\n",
        "#stop = 5\n",
        "start = 0\n",
        "display_results = False\n",
        "for i, row in enumerate(df.iloc[start:stop].iterrows()):\n",
        "    try:\n",
        "        # Run image through object detector and export result\n",
        "        image_url = df['url'][row[0]]\n",
        "        image_wboxes, result, im_h, im_w = run_detector_tf(detector, image_url, outfpath, filters, label_map, max_boxes, min_score, category_index)\n",
        "        img_id = export_results(image_url, result, outfpath, im_h, im_w)\n",
        "\n",
        "        # Optional: Display detections on images\n",
        "        if (i+1<=50) and display_results:\n",
        "            display_image(image_wboxes)\n",
        "\n",
        "        # Display progress message after each image\n",
        "        all_predictions.append(img_id)\n",
        "        print('\\033[92m {}) Inference complete for image {} of {} \\033[0m \\n'.format(i+1, i+1, stop))\n",
        "\n",
        "    except:\n",
        "        print('Check if URL from {} is valid\\n'.format(df['url'][i]))\n",
        "\n",
        "print(\"\\n\\n~~~\\n\\033[92m Inference complete!\\033[0m \\033[93m Run these steps for remaining batches A-D before proceeding.\\033[0m\\n~~~\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSjtwcuYDzV3"
      },
      "source": [
        "## Post-process detection results\n",
        "---\n",
        "Convert detection boxes into square, centered thumbnail cropping coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rBZMx9BD1_4"
      },
      "outputs": [],
      "source": [
        "#@title Enter path to inference result file\n",
        "\n",
        "# If you just ran \"Generate crops\" above, you do not need to enter anything\n",
        "# If you ran \"Generate crops\" during a previous session, enter the path for ONE output file\n",
        "if 'outfpath' not in locals() or globals():\n",
        "    crops_file = \"multitaxa_cropcoords_tf2_d\" #@param [\"multitaxa_cropcoords_tf2_a\", \"multitaxa_cropcoords_tf2_b\", \"multitaxa_cropcoords_tf2_c\", \"multitaxa_cropcoords_tf2_d\"] {allow-input: true}\n",
        "    outfpath = set_outpath(crops_file, cwd)\n",
        "\n",
        "df = pd.read_csv(outfpath, sep='\\t', header=0, na_filter = False)\n",
        "df.columns = ['img_id', 'class_name', 'confidence', 'xmin', 'ymin', 'xmax', 'ymax', 'im_width', 'im_height', 'url']\n",
        "print(\"Bounding box dataframe, before post-processing: {} \\n{}\".format(outfpath, df.head()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "def add_identifiers(superboxes, bundle_info):\n",
        "    # Get dataObjectVersionIDs, identifiers, and eolMediaURLS from indexed cols\n",
        "    ids = pd.DataFrame(bundle_info[['data_object_id', 'obj_url']])\n",
        "    ids.set_index('obj_url', inplace=True, drop=True)\n",
        "    #print(\"Bundle identifying info head: \\n\", ids.head())\n",
        "\n",
        "    # Set up superboxes df for mapping to bundle_info\n",
        "    superboxes.reset_index(inplace=True)\n",
        "    superboxes.rename(columns={'url': 'obj_url'}, inplace=True)\n",
        "    superboxes.set_index('obj_url', inplace=True, drop=True)\n",
        "\n",
        "    # Map dataObjectVersionIDs to crops_unq using eolMediaURL as the index\n",
        "    crops_w_identifiers = pd.DataFrame(superboxes.merge(ids, left_index=True, right_index=True))\n",
        "    crops_w_identifiers.reset_index(inplace=True)\n",
        "    print(\"\\n Crops with added EOL identifiers: \\n\", crops_w_identifiers.head())\n",
        "\n",
        "    return crops_w_identifiers"
      ],
      "metadata": {
        "id": "C2LeFO8jQrrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttP8IH5_EVa1"
      },
      "outputs": [],
      "source": [
        "#@title Combine individual detection boxes into one \"superbox\" per image\n",
        "\n",
        "# For images with >1 detection, make a 'super box' that containings all boxes\n",
        "\n",
        "# Read in crop file exported from \"Combine output files A-D\" block above\n",
        "crops = df.copy()\n",
        "print(crops.head())\n",
        "\n",
        "# De-normalize cropping coordinates to pixel values\n",
        "crops = denormalize_coords(crops)\n",
        "print(crops.head())\n",
        "\n",
        "# Make 1 superbox per image [coordinates: bottom left (smallest xmin, ymin) and top right (largest xmax, ymax)]\n",
        "superboxes = make_superboxes(crops)\n",
        "\n",
        "# Read in EOL image \"breakdown\" bundle dataframe from \"breakdown_download\" bundle used for cropping\n",
        "if 'bundle' not in locals() or globals():\n",
        "    bundle = \"Multitaxa_crops_test.tsv\" #@param {type:\"string\"}\n",
        "    bundle_path = 'results/' + bundle\n",
        "bundle_info = read_datafile(bundle_path, sep='\\t', header=0, disp_head=False)\n",
        "\n",
        "# Add EOL img identifying info from breakdown file to cropping data\n",
        "crops_w_identifiers = add_identifiers(superboxes, bundle_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APThvzPcEXMT"
      },
      "outputs": [],
      "source": [
        "#@title Make superbox square and within image bounds (Optional: add padding)\n",
        "\n",
        "# Pad by xx% larger crop dimension\n",
        "pad = 0 #@param {type:\"slider\", min:0, max:10, step:2}\n",
        "pad = pad/100 # Convert to percentage\n",
        "\n",
        "# Make crops square and within bounds\n",
        "df = make_square_crops(crops_w_identifiers, pad)\n",
        "\n",
        "# Export crop coordinates to display_test.tsv to visualize results in next code block and confirm crop transformations\n",
        "display_test_fpath = os.path.splitext(outfpath)[0] + '_displaytest' + '.tsv'\n",
        "print(\"\\n File for displaying square crops on images will be saved to: \\n\", display_test_fpath)\n",
        "df.to_csv(display_test_fpath, sep='\\t', index=False)\n",
        "\n",
        "# Format image and cropping dimensions for EOL standards\n",
        "eol_crops = format_crops_for_eol(df)\n",
        "\n",
        "# Write results to tsv\n",
        "eol_crops_fpath = os.path.splitext(display_test_fpath)[0].rsplit('_',2)[0] + '_20k_final' + '.tsv'\n",
        "eol_crops.to_csv(eol_crops_fpath, columns = eol_crops.iloc[:,:-1], sep='\\t', index=False)\n",
        "print(\"EOL formatted crops dataset saved to: {} \\n{}\".format(eol_crops_fpath, eol_crops.head()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGWwa3YGpMzS"
      },
      "outputs": [],
      "source": [
        "#@title Format EOL user crops to match display test crops\n",
        "\n",
        "# Read in EOL image \"breakdown\" bundle dataframe from \"breakdown_download\" bundle used for cropping\n",
        "if 'bundle' not in locals() or globals():\n",
        "    bundle = \"Multitaxa_crops_test.tsv\" #@param {type:\"string\"}\n",
        "    bundle_path = 'results/' + bundle\n",
        "bundle_info = read_datafile(bundle_path, sep='\\t', header=0, disp_head=False)\n",
        "bundle_info.rename(columns={'name_x': 'class_name'}, inplace=True)\n",
        "\n",
        "# Take and rename relevant columns\n",
        "ground_truth = pd.DataFrame(bundle_info[['obj_url', 'im_height', 'im_width', 'xmin', 'ymin', 'xmax', 'ymax', 'class_name', 'data_object_id']])\n",
        "ground_truth['crop_height'] = ground_truth['ymax'] - ground_truth['ymin']\n",
        "ground_truth['crop_width'] = ground_truth['xmax'] - ground_truth['xmin']\n",
        "\n",
        "# Export ground truth formatted crops\n",
        "ground_truth_fpath = os.path.splitext(bundle_path)[0] + '_groundtruth' + '.tsv'\n",
        "ground_truth.to_csv(ground_truth_fpath, sep='\\t', index=False)\n",
        "print(\"Ground truths saved to: \", ground_truth_fpath)\n",
        "print(ground_truth.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T30GOVYpEaXd"
      },
      "source": [
        "## Display ground truths versus model predictions on images and save outputs\n",
        "---\n",
        "Images will have two boxes each - one ground truth and one model prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWpSgxcpxA7b"
      },
      "outputs": [],
      "source": [
        "#@title Read in cropping file and display results on images\n",
        "from wrangle_data import *\n",
        "import cv2\n",
        "\n",
        "# Load in prediction df\n",
        "pred_fname = \"multitaxa_cropcoords_tf2_d_rcnn_i_displaytest.tsv\" # @param [\"Multitaxa_crops_test_groundtruth.tsv\",\"multitaxa_cropcoords_tf2_d_rcnn_i_displaytest.tsv\"] {\"allow-input\":true}\n",
        "display_test_fpath = 'results/' + pred_fname\n",
        "df = pd.read_csv(display_test_fpath, sep=\"\\t\", header=0)\n",
        "df_pred = df.copy()\n",
        "\n",
        "# Load in ground truth df\n",
        "gt_fname = \"Multitaxa_crops_test_groundtruth.tsv\" # @param [\"Multitaxa_crops_test_groundtruth.tsv\",\"multitaxa_cropcoords_tf2_rcnn_displaytest.tsv\"] {\"allow-input\":true}\n",
        "display_test_fpath = 'results/' + gt_fname\n",
        "df_gt = pd.read_csv(display_test_fpath, sep=\"\\t\", header=0)\n",
        "df_gt = df_gt.set_index('obj_url')\n",
        "\n",
        "print(df.head())\n",
        "print(df_gt.head())\n",
        "print(df_pred.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw cropping box on image\n",
        "def draw_box_on_image(df, i, img, tags, tag_gt):\n",
        "    # Get box coordinates\n",
        "    xmin = df['xmin'][i].astype(int)\n",
        "    ymin = df['ymin'][i].astype(int)\n",
        "    xmax = df['xmax'][i].astype(int)\n",
        "    ymax = df['ymax'][i].astype(int)\n",
        "    boxcoords = [xmin, ymin, xmax, ymax]\n",
        "    class_name = str(df['class_name'][i])\n",
        "    if any(tag in class_name for tag in tags):\n",
        "        tag = class_name + \" \" + str(tag_gt)\n",
        "    else:\n",
        "        tag = \"None\" + \" \" + str(tag_gt)\n",
        "\n",
        "    # Set box/font color and size\n",
        "    mindim = min(df['im_height'][i], df['im_width'][i])\n",
        "    thicknessScale = 1e-3  # Adjust for larger thickness in all images\n",
        "    fontScale = 2e-3\n",
        "    fontsize = mindim * fontScale\n",
        "    thickness = math.ceil(mindim * thicknessScale)\n",
        "    box_col = (255, 0, 157)\n",
        "\n",
        "    # Add label to image\n",
        "    img = np.squeeze(img)\n",
        "    buffer = int(mindim * 0.01) # some of the boxes are not drawing on images by edges,makes easier to see\n",
        "    image_wbox = cv2.putText(img, tag, ((xmin+buffer), (ymax-buffer)), cv2.FONT_HERSHEY_SIMPLEX, fontsize, box_col, 2, cv2.LINE_AA)\n",
        "\n",
        "    # Draw box label on image\n",
        "    image_wbox = cv2.rectangle(img, ((xmin+buffer), (ymax-buffer)), ((xmax-buffer), (ymin+buffer)), box_col, thickness)\n",
        "\n",
        "    return image_wbox, boxcoords"
      ],
      "metadata": {
        "id": "9uEEkCwymxtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0SAM4RUv6xc"
      },
      "outputs": [],
      "source": [
        "#@title Draw bounding boxes on image & save resulting images\n",
        "%matplotlib inline\n",
        "import imageio\n",
        "\n",
        "# Display results on images (only use for 5 images as a test)\n",
        "display_results = False #@param {type:\"boolean\"}\n",
        "print(\"Displaying results on images for small subset of 5 images. Uncheck display_results to run and save for all images.\")\n",
        "\n",
        "# Adjust line to right to see up to 50 images displayed at a time\n",
        "start = 0 # @param {\"type\":\"number\",\"placeholder\":\"0\"}\n",
        "if display_results:\n",
        "    stop = start + 5\n",
        "else:\n",
        "    stop = start + len(df)\n",
        "\n",
        "# Loop through images\n",
        "for i, row in df.iloc[start:stop].iterrows():\n",
        "    try:\n",
        "        # Read in image\n",
        "        url = df['obj_url'][i]\n",
        "        image, im_h, im_w = url_to_image(url)\n",
        "\n",
        "        # Draw bounding box on image - Ground Truth\n",
        "        image_wbox, boxcoords_gt = draw_box_on_image(df_gt, i, image, filters, \"ground truth\")\n",
        "\n",
        "        # Draw bounding box on image - Prediction\n",
        "        image_wboxes, boxcoords_pred = draw_box_on_image(df_pred, i, image_wbox, filters, \"prediction\")\n",
        "\n",
        "        if display_results:\n",
        "            # Plot cropping box on image\n",
        "            _, ax = plt.subplots(figsize=(10, 10))\n",
        "            ax.imshow(image_wboxes)\n",
        "\n",
        "            # Display image URL and coordinates above image\n",
        "            plt.title('{} \\n im_h: {} im_w: {} \\n Ground Truth - xmin: {}, ymin: {}, xmax: {}, ymax: {} \\n Prediction - xmin: {}, ymin: {}, xmax: {}, ymax: {}'.format(url, im_h, im_w, boxcoords_gt[0], boxcoords_gt[1], boxcoords_gt[2], boxcoords_gt[3], boxcoords_pred[0], boxcoords_pred[1], boxcoords_pred[2], boxcoords_pred[3]))\n",
        "\n",
        "        # Save Results\n",
        "        path = \"results/inspect_results/\" + str(df['data_object_id'][i]) + \"_wbox.png\"\n",
        "        imageio.imwrite(path, image_wboxes)\n",
        "        print(\"Image with boxes saved to: \", path)\n",
        "\n",
        "    except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unzipped_fpath = cwd + '/results/' + \"inspect_results\" #@param [\"images.zip\"] {allow-input: true}\n",
        "zip_fpath = '/content/' + \"inspect_results.zip\" #@param [\"images\"] {allow-input: true}\n",
        "\n",
        "# Zip the file/folder\n",
        "!zip -r -j $zip_fpath $unzipped_fpath"
      ],
      "metadata": {
        "id": "_d41YpBVVI-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT39cFtSNRHI"
      },
      "source": [
        "## Caculate mAP for test images\n",
        "---\n",
        "Convert detection boxes into square, centered thumbnail cropping coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss8vyYiq6lQn"
      },
      "outputs": [],
      "source": [
        "#@title Convert cropping coordinates for EOL user generated ground truths (square crops) to mAP format\n",
        "%cd $cwd\n",
        "\n",
        "# Load in prediction df\n",
        "pred_fname = \"multitaxa_cropcoords_tf2_d_rcnn_i_displaytest.tsv\" # @param [\"Multitaxa_crops_test_groundtruth.tsv\",\"multitaxa_cropcoords_tf2_d_rcnn_i_displaytest.tsv\"] {\"allow-input\":true}\n",
        "display_test_fpath = 'results/' + pred_fname\n",
        "df = pd.read_csv(display_test_fpath, sep=\"\\t\", header=0)\n",
        "df_pred = df.copy()\n",
        "\n",
        "# Load in ground truth df\n",
        "gt_fname = \"Multitaxa_crops_test_groundtruth.tsv\" # @param [\"Multitaxa_crops_test_groundtruth.tsv\",\"multitaxa_cropcoords_tf2_rcnn_displaytest.tsv\"] {\"allow-input\":true}\n",
        "display_test_fpath = 'results/' + gt_fname\n",
        "df_gt = pd.read_csv(display_test_fpath, sep=\"\\t\", header=0)\n",
        "df_gt = df_gt.set_index('obj_url')\n",
        "df_gt.reset_index(inplace=True)\n",
        "\n",
        "print(\"df\", df.head())\n",
        "print(\"gt\", df_gt.head())\n",
        "print(\"pred\", df_pred.head())\n",
        "\n",
        "df['xmin_gt'] = df_gt['xmin'].copy()\n",
        "df['ymax_gt'] = df_gt['ymax'].copy()\n",
        "df['xmax_gt'] = df_gt['xmax'].copy()\n",
        "df['ymin_gt'] = df_gt['ymin'].copy()\n",
        "df['class_name_gt'] = df_gt['class_name'].copy()\n",
        "print(\"Df with new cols: \", df.head())\n",
        "\n",
        "# Format bounding boxes for calculating mAP and IoU\n",
        "pred_bboxes = []\n",
        "pred_classes = []\n",
        "pred_confs = []\n",
        "gt_bboxes = []\n",
        "gt_classes = []\n",
        "# Loop through df and extract relevant info as lists\n",
        "for i, row in df.iterrows():\n",
        "    pred_bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax'], row['confidence']]\n",
        "    pred_class = row['class_name']\n",
        "    pred_conf = row['confidence']\n",
        "    gt_bbox = [row['xmin_gt'], row['ymin_gt'], row['xmax_gt'], row['ymax_gt']]\n",
        "    gt_class = row['class_name_gt']\n",
        "\n",
        "    pred_bboxes.append(pred_bbox)\n",
        "    pred_classes.append(pred_class)\n",
        "    pred_confs.append(pred_conf)\n",
        "    gt_bboxes.append(gt_bbox)\n",
        "    gt_classes.append(gt_class)\n",
        "\n",
        "# Reformatted bounding box data\n",
        "print(\"\\n\\n~~~Reformatted bounding box data for mAP analysis~~~\\n\")\n",
        "print(\"pred_bboxes\", pred_bboxes)\n",
        "print(\"pred_classes\", pred_classes)\n",
        "print(\"pred_confs\", pred_confs)\n",
        "print(\"gt_bboxes\", gt_bboxes)\n",
        "print(\"gt_classes\", gt_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te05bKOGGh7a"
      },
      "outputs": [],
      "source": [
        "# Define functions\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# Calculate mean averag precision\n",
        "def calculate_map(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
        "    aps = []\n",
        "    y_true = []\n",
        "    y_scores = []\n",
        "    ious = []\n",
        "    for i, gt_box in enumerate(gt_boxes):\n",
        "          iou = calculate_iou(gt_box, pred_boxes[i])\n",
        "          if iou > iou_threshold:\n",
        "                y_true.append(1)\n",
        "                y_scores.append(pred_boxes[i][4])  # Confidence score is at index 4\n",
        "                ious.append(iou)\n",
        "\n",
        "    if len(y_true) > 0:\n",
        "            print(\"\\n Number of true detections with iou > {} : {}: \".format(iou_threshold, len(y_true)))\n",
        "            print(\"Number of total detections: \", len(gt_boxes))\n",
        "            ap = average_precision_score(y_true, y_scores)\n",
        "            aps.append(ap)\n",
        "\n",
        "    return np.mean(aps), np.mean(ious)\n",
        "\n",
        "# Calculate intersection over union\n",
        "def calculate_iou(box1, box2):\n",
        "    # Calculate intersection area\n",
        "    print(\"box1: \", box1)\n",
        "    print(\"box2: \", box2)\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "    print(\"x1: \", x1)\n",
        "    print(\"y1: \", y1)\n",
        "    print(\"x2: \", x2)\n",
        "    print(\"y2: \", y2)\n",
        "    intersection_area = (x2 - x1) * (y2 - y1)\n",
        "    if intersection_area < 0:\n",
        "        intersection_area = 0\n",
        "    print(\"intersection: \", intersection_area)\n",
        "\n",
        "    # Calculate union area\n",
        "    box1_area = abs((box1[2] - box1[0]) * (box1[3] - box1[1]))\n",
        "    box2_area = abs((box2[2] - box2[0]) * (box2[3] - box2[1]))\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "    print(\"box1_area: \", box1_area)\n",
        "    print(\"box2_area: \", box2_area)\n",
        "    print(\"union: \", union_area)\n",
        "\n",
        "    # Calculate IoU\n",
        "    iou = intersection_area / union_area\n",
        "    print(\"iou: \", iou)\n",
        "    print(\"\\n\")\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkoRPIAYIP5_"
      },
      "outputs": [],
      "source": [
        "#@title Calculate mAP\n",
        "map, mIoU = calculate_map(gt_bboxes, pred_bboxes, iou_threshold=0.5)\n",
        "print(\"\\nmAP\", map)\n",
        "print(\"\\nmIoU\", mIoU)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPThnliKne3tfJLPRsEeZS5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}