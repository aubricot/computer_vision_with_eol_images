{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multitaxa_train_tf_rcnns.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "G7amcDYcQpf0",
        "VgG3-2LslczI",
        "Sz7aVQVUSecK",
        "kRwJ918UYdiL",
        "naEjBRiGu-3e",
        "kGx_08UcmtOF",
        "vv9c8NRXy4DM",
        "k3HJoT3kx0a3"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_train_tf_rcnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWrXhn1qKWm_",
        "colab_type": "text"
      },
      "source": [
        "# Training Tensorflow Faster-RCNN and R-FCN models to detect snakes & lizards (Squamata), beetles (Coleoptera), frogs (Anura), and carnivores (Carnivora) from EOL images\n",
        "---   \n",
        "*Last Updated 13 April 2020*  \n",
        "Use images and bounding box coordinates to train Faster-RCNN Object Detection Models (RCNN with Inceptionv2 or Resnet 50 architecture) implemented in Tensorflow to detect snakes & lizards, beetles, frogs and carnivores from EOL images.\n",
        "\n",
        "Datasets exported from [multitaxa_preprocessing.ipynb](https://github.com/aubricot/computer_vision_with_eol_images/blob/master/object_detection_for_image_cropping/multitaxa/multitaxa_preprocessing.ipynb) were already downloaded to Google Drive in multitaxa_preprocessing.ipynb. \n",
        "\n",
        "RCNN Resnet 50 trained for 12 hours to 200,000 steps, RCNN Inception v2 trained for 18 hours to 200,000 steps.\n",
        "\n",
        "Notes:   \n",
        "* For each 24 hour period on Google Colab, you have up to 12 hours of GPU access. Training the object detection model on bats took 30 hours split into 3 days.\n",
        "\n",
        "* Make sure to set the runtime to Python 2 with GPU Hardware Accelerator.  \n",
        "\n",
        "References:   \n",
        "* [Official Tensorflow Object Detection API Instructions](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html)   \n",
        "* [Medium Blog on training using Tensorflow Object Detection API in Colab](https://medium.com/analytics-vidhya/training-an-object-detection-model-with-tensorflow-api-using-google-colab-4f9a688d5e8b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smQWTwI7k4Bf",
        "colab_type": "text"
      },
      "source": [
        "## Installs (run this every time)\n",
        "---\n",
        "Install the Tensorflow Object Detection API directly to this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mAC7PfUrWX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount google drive to import/export files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No18RpXIAgGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download, compile and build all ingredients for the Tensorflow Object Detection API\n",
        "# These steps take a couple of minutes and print a lot of output\n",
        "\n",
        "# Set tensorflow to version 1x\n",
        "%tensorflow_version 1.0\n",
        "\n",
        "# Make a working directory train/ in your Google Drive and include the path here (all paths in other sections stay the same)\n",
        "import os\n",
        "%cd drive/My Drive/fall19_smithsonian_informatics/train\n",
        "if not os.path.exists(\"tf_models\"):\n",
        "  !mkdir tf_models\n",
        "%cd tf_models\n",
        "\n",
        "# Install Tensorflow Object Detection API\n",
        "import pathlib\n",
        "if not pathlib.Path(\"models\").exists():\n",
        "  #!git clone https://github.com/tensorflow/models.git\n",
        "  print('Need to re-download tensorflow model api from archive, see directions line 22-24')\n",
        "  # Archive of tensorflow object detection api used, new version on github doesn't work with current pipeline\n",
        "  #%cd drive/My Drive/fall19_smithsonian_informatics/train\n",
        "  #!unzip tf_models/models-20200216T051538Z-001.zip\n",
        "\n",
        "# Clone the COCO API repository to your Google Drive\n",
        "if not pathlib.Path(\"pycocotools\").exists():\n",
        "  !git clone https://github.com/cocodataset/cocoapi.git\n",
        "  # Move needed folders to tf_models/pycocotools and delete remaining contents of cocoapi/ to save space\n",
        "  !cd cocoapi/PythonAPI; make; cp -r pycocotools ../..\n",
        "  !rm -rf cocoapi\n",
        "\n",
        "# Install libraries\n",
        "!apt-get install -qq protobuf-compiler python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive\n",
        "\n",
        "# Compile object detection api using Google Protobuf\n",
        "%cd models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# Update system path variables\n",
        "os.environ['PYTHONPATH'] = ':/drive/My Drive/fall19_smithsonian_informatics/train/tf_models/models/research/:/drive/My Drive/fall19_smithsonian_informatics/train/tf_models/models/research/object_detection/:/drive/My Drive/fall19_smithsonian_informatics/train/tf_models/models/research/slim/:'+os.environ['PYTHONPATH']\n",
        "!echo $PYTHONPATH\n",
        "\n",
        "import sys\n",
        "print(sys.path)\n",
        "sys.path.append(\"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim\")\n",
        "print(sys.path)\n",
        "\n",
        "# Build slim\n",
        "!python slim/setup.py build\n",
        "!python slim/setup.py install\n",
        "\n",
        "# Copy slim to specified directories to avoid errors in model_builder_test.py\n",
        "!cp -R slim/ /usr/local/lib/python2.7/dist-packages/object_detection-0.1-py2.7.egg/\n",
        "if not os.path.exists(\"object_detection/slim/nets\"):\n",
        "  !cp -R slim/nets/ object_detection/\n",
        "# To get rid of errors about deployment when training using train.py instead of model_main.py\n",
        "if not os.path.exists(\"object_detection/slim/deployment\"):\n",
        "  !cp -R slim/deployment/ object_detection/\n",
        "\n",
        "# Test build of model\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7amcDYcQpf0",
        "colab_type": "text"
      },
      "source": [
        "## Model preparation (only run these once)\n",
        "---\n",
        "Upload needed files to Google Drive and generate tf.records used for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgG3-2LslczI",
        "colab_type": "text"
      },
      "source": [
        "### Download and extract pre-trained model   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaLxacTBZnM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download pre-trained model from Tensorflow Object Detection Model Zoo\n",
        "# Faster-RCNN and R-FCN both included as options below\n",
        "# modified from https://github.com/RomRoc/objdet_train_tensorflow_colab/blob/master/objdet_custom_tf_colab.ipynb\n",
        "\n",
        "# cd to train/\n",
        "%cd ../../..\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib\n",
        "import tarfile\n",
        "\n",
        "# Make folders for your training files for each model\n",
        "# RCNN Model with resnet 50\n",
        "if not (os.path.exists('tf_models/train_demo')):\n",
        "  !mkdir tf_models/train_demo\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn')):\n",
        "  !mkdir tf_models/train_demo/rcnn\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn/pretrained_model')):\n",
        "  !mkdir tf_models/train_demo/rcnn/pretrained_model\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn/finetuned_model')):\n",
        "  !mkdir tf_models/train_demo/rcnn/finetuned_model\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn/trained')):\n",
        "  !mkdir tf_models/train_demo/rcnn/trained\n",
        "# Download the model\n",
        "MODEL = 'faster_rcnn_resnet50_coco_2018_01_28'\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = 'tf_models/train_demo/rcnn/pretrained_model'\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "  opener = urllib.URLopener()\n",
        "  opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "  shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "\n",
        "# RCNN Model with Inception v2\n",
        "if not (os.path.exists('tf_models/train_demo')):\n",
        "  !mkdir tf_models/train_demo\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn_i')):\n",
        "  !mkdir tf_models/train_demo/rcnn_i\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn_i/pretrained_model')):\n",
        "  !mkdir tf_models/train_demo/rcnn_i/pretrained_model\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn_i/finetuned_model')):\n",
        "  !mkdir tf_models/train_demo/rcnn_i/finetuned_model\n",
        "if not (os.path.exists('tf_models/train_demo/rcnn_i/trained')):\n",
        "  !mkdir tf_models/train_demo/rcnn_i/trained\n",
        "# Download the model\n",
        "MODEL = 'faster_rcnn_inception_v2_coco_2018_01_28'\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = 'tf_models/train_demo/rcnn_i/pretrained_model'\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "  opener = urllib.URLopener()\n",
        "  opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "  shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz7aVQVUSecK",
        "colab_type": "text"
      },
      "source": [
        "### Convert training data to tf.record format\n",
        "Modified from https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.htmlrom and https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G57Xi6uSK46b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload multitaxa_generate_tfrecord.py to your Google Drive\n",
        "!gdown --id 1Qcld-6EVRcLLKKqtV9NVK_uHCpBN1fwb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DScMtAYQ_Vib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload label map for Multitaxa to your Google Drive\n",
        "!gdown --id 1qhyP8lH-0ppQPoaitujlacICaHGk3tej"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kFe9D21WGog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert crops_test to tf.record format for test data\n",
        "# All taxa pooled (multi-taxa)\n",
        "!python multitaxa_generate_tfrecord.py --csv_input='/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_test_notaug_fin.csv'  --output_path= \"/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images/tf.record\"  --image_dir=\"/content/drive/My Drive/fall19_smithsonian_informatics/train/test_images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHcHuvpD5diH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move tf.record (test images) -bc output path above doesn't work..\n",
        "!mv tf.record test_images/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRr1MLxSICf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert crops_train to tf.record format for train data\n",
        "!python multitaxa_generate_tfrecord.py --csv_input='/content/drive/My Drive/fall19_smithsonian_informatics/train/preprocessing/multitaxa_crops_train_aug_all_fin.csv'  --output_path= \"/content/drive/My Drive/fall19_smithsonian_informatics/train/tf.record\"  --image_dir=\"/content/drive/My Drive/fall19_smithsonian_informatics/train/images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRwJ918UYdiL",
        "colab_type": "text"
      },
      "source": [
        "### Upload modified config files for Faster-RCNN and R-FCN models to your Google Drive\n",
        "If you have errors with training, check the pipline_config_path and model_dir in the config files for R-FCN or Faster-RCNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGv6m4kdYgKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd tf_models/models/research/object_detection/samples/configs\n",
        "# Faster-RCNN faster_rcnn_resnet50_coco_2018_01_28_multitaxa.config\n",
        "#!gdown --id 1Fzs97G08YpMdCKyekqyYK1Bo6pDZTedi\n",
        "\n",
        "# Faster-RCNN faster_rcnn_inception_v2_coco_2018_01_28.config (supposed to be faster than rcnn with resnet)\n",
        "!gdown --id 1TY6GaPEbNL9AVL7ASQvl9FBN9SR63Ty2\n",
        "\n",
        "# cd back to train\n",
        "%cd ../../../../../.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naEjBRiGu-3e",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "--- \n",
        "To switch between training Faster-RCNN models (RCNN with Inceptionv2 or Resnet 50 architecture), see notes at the beginning of each code block on which lines to comment out and move to the bottom of the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cUfcn0kpRX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First time running this notebook: cd to train after running Model Preparation steps above\n",
        "#%cd train\n",
        "# Subsequent times running this notebook: cd to train\n",
        "%cd ../../.."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B55tKxoL5aRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Actual training\n",
        "# Modified from https://github.com/RomRoc/objdet_train_tensorflow_colab/blob/master/objdet_custom_tf_colab.ipynb\n",
        "# Change pipline_config_path and model_dir to the appropriate config file for Faster-RCNN models (rcnn_i and rcnn)\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "!python tf_models/models/research/object_detection/model_main.py \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps=200000 \\\n",
        "    --num_eval_steps=500 \\\n",
        "    --pipeline_config_path=tf_models/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco_2018_01_28.config \\\n",
        "    --model_dir=tf_models/train_demo/rcnn_i/trained/\n",
        "    #--pipeline_config_path=tf_models/models/research/object_detection/samples/configs/rfcn_resnet101_coco_2018_01_28_multitaxa.config \\\n",
        "    #--pipeline_config_path=tf_models/models/research/object_detection/samples/configs/faster_rcnn_resnet50_coco_2018_01_28_multitaxa.config \\\n",
        "    #--model_dir=tf_models/train_demo/rcnn/trained/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9jOTBgzYzMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Export trained model\n",
        "# Modified from https://github.com/RomRoc/objdet_train_tensorflow_colab/blob/master/objdet_custom_tf_colab.ipynb\n",
        "# Change os.listdir, pipeline_config_path, output_directory and trained_checkpoint_prefix when switching between Faster-RCNN models (rcnn_i and rcnn)\n",
        "%cd train\n",
        "\n",
        "lst = os.listdir('tf_models/train_demo/rcnn_i/trained')\n",
        "#lst = os.listdir('tf_models/train_demo/rcnn/trained')\n",
        "lf = filter(lambda k: 'model.ckpt-' in k, lst)\n",
        "last_model = sorted(lf)[-1].replace('.meta', '')\n",
        "\n",
        "!python tf_models/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=tf_models/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco_2018_01_28.config \\\n",
        "    --output_directory=tf_models/train_demo/rcnn_i/finetuned_model \\\n",
        "    --trained_checkpoint_prefix=tf_models/train_demo/rcnn_i/trained/$last_model \n",
        "    #--pipeline_config_path=tf_models/models/research/object_detection/samples/configs/faster_rcnn_resnet50_coco_2018_01_28_multitaxa.config \\\n",
        "    #--output_directory=tf_models/train_demo/rcnn/finetuned_model \\\n",
        "    #--trained_checkpoint_prefix=tf_models/train_demo/rcnn/trained/$last_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzgf0H5BM1I3",
        "colab_type": "text"
      },
      "source": [
        "### When finished training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roFjqjd-tVHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate trained model to get mAP and IoU stats\n",
        "# Change pipeline_config_path and checkpoint_dir when switching between R-FCN and Faster-RCNN models\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "!python tf_models/models/research/object_detection/model_main.py \\\n",
        "    --alsologtostderr \\\n",
        "    --pipeline_config_path=tf_models/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco_2018_01_28.config \\\n",
        "    --checkpoint_dir=tf_models/train_demo/rcnn_i/trained/\n",
        "    #--pipeline_config_path=tf_models/models/research/object_detection/samples/configs/faster_rcnn_resnet50_coco_2018_01_28_multitaxa.config \\\n",
        "    #--checkpoint_dir=tf_models/train_demo/rcnn/trained/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGx_08UcmtOF",
        "colab_type": "text"
      },
      "source": [
        "## Run test images through the trained object detector\n",
        "---\n",
        "Test image detection boxes are only needed for calculating mAP (mean average precision, a performance measure to compare models) and not for cropping. The functions below will only display resulting detection boxes on test images for visualization, but does not save their coordinates to a spreadsheet. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6h99mPKNH9L",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlg-GTnKKRa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cd to train\n",
        "%cd ../../..\n",
        "#%cd train\n",
        "\n",
        "%tensorflow_version 1.0\n",
        "\n",
        "import tensorflow as tf \n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "# For importing/exporting files, working with arrays, etc\n",
        "import os\n",
        "import pathlib\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import zipfile\n",
        "import numpy as np \n",
        "import csv\n",
        "import matplotlib\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# For downloading the images\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "\n",
        "# For drawing onto and plotting the images\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "import cv2\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "sys.path.append(\"tf_models/models/research/\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQpr26hJOv5y",
        "colab_type": "text"
      },
      "source": [
        "### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n_alUkLZ1gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change PATH_TO_CKPT below when switching between R-FCN and Faster-RCNN models\n",
        "%cd train\n",
        "# Faster RCNN Inception v2\n",
        "PATH_TO_CKPT = 'tf_models/train_demo/rcnn_i/finetuned_model' + '/frozen_inference_graph.pb'\n",
        "# Faster RCNN Resnet 50\n",
        "#PATH_TO_CKPT = 'tf_models/train_demo/rcnn/finetuned_model' + '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = 'labelmap.pbtxt'\n",
        "NUM_CLASSES = 4\n",
        "    \n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.compat.v1.GraphDef()\n",
        "  with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# For loading images into computer-readable format\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Function for loading images from urls\n",
        "def url_to_image(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image\n",
        "\n",
        "# Can change min_score_thresh and max_boxes_to_draw\n",
        "def show_inference(image_np_expanded):\n",
        "  with detection_graph.as_default():\n",
        "    with tf.Session(graph=detection_graph) as sess:\n",
        "      # Definite input and output Tensors for detection_graph\n",
        "      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "      # Each box represents a part of the image where a particular object was detected.\n",
        "      detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "      #max_boxes_to_draw = detection_boxes.shape[0] # add this line and remove (i) and (ii) below to show multiple detection boxes\n",
        "      # Each score represent how level of confidence for each of the objects.\n",
        "      # Score is shown on the result image, together with the class label.\n",
        "      detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "      detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "      num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "      #min_score_thresh = .7\n",
        "\n",
        "      # Actual detection\n",
        "      (boxes, scores, classes, num) = sess.run(\n",
        "          [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "          feed_dict={image_tensor: image_np_expanded})\n",
        "      \n",
        "      # Visualize detection results on images  \n",
        "      # Modified from https://github.com/tensorflow/models/issues/4682\n",
        "      newImage = np.copy(image_np)\n",
        "      im_height, im_width, im_depth = image_np.shape\n",
        "      ymin = int((boxes[0][0][0]*im_height))\n",
        "      xmin = int((boxes[0][0][1]*im_width))\n",
        "      ymax = int((boxes[0][0][2]*im_height))\n",
        "      xmax = int((boxes[0][0][3]*im_width))\n",
        "      # Add labels to boxes\n",
        "      label = np.squeeze(classes[0][0]).astype(np.int32)\n",
        "      fontScale = max(im_width,im_height)/(600)\n",
        "      if label == 1 :\n",
        "        newImage = cv2.putText(newImage, 'Squamata', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (255,199,15)\n",
        "      elif label == 2 :\n",
        "        newImage = cv2.putText(newImage, 'Coleoptera', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (255,127,0)\n",
        "      elif label == 3 :\n",
        "        newImage = cv2.putText(newImage, 'Anura', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (255,42,22)\n",
        "      elif label == 4 :\n",
        "        newImage = cv2.putText(newImage, 'Carnivora', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (0,191,255)  \n",
        "      # Draw detection boxes on images\n",
        "      newImage = cv2.rectangle(newImage, (xmin, ymax), (xmax, ymin), box_col, 3)\n",
        "      \n",
        "      return newImage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjE-Gh5ann4E",
        "colab_type": "text"
      },
      "source": [
        "### Run test images through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSPrkTKAglMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO: Update path to your test images\n",
        "PATH_TO_TEST_IMAGES_DIR = 'test_images/'\n",
        "PATH_TO_OUT_IMAGES_DIR = 'out/rcnn_i/'\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "OUT_IMAGE_PATHS = [os.path.join(PATH_TO_OUT_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS[:15], start=1):\n",
        " \n",
        "    # Load in image\n",
        "    image = Image.open(im_path)\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection and draw boxes on image\n",
        "    show_inference(image_np_expanded)\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 429 test images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(show_inference(image_np_expanded))\n",
        "    plt.title('{}) Inference time: {}'.format(im_num, format(end_time-start_time, '.2f')))\n",
        "\n",
        "    # Optional: Save image with detection boxes to Google Drive\n",
        "    #img = Image.fromarray(show_inference(image_np_expanded))\n",
        "    #img.save(OUT_IMAGE_PATHS[TEST_IMAGE_PATHS.index(im_path)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv9c8NRXy4DM",
        "colab_type": "text"
      },
      "source": [
        "### Get inference info for test images to compare object detection model times for YOLO, R-FCN, and Faster-RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJcda-CNy6TY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# For exporting inference times\n",
        "inf_time = []\n",
        "img_path = []\n",
        "im_dims = []\n",
        "\n",
        "# Update path to your test images\n",
        "PATH_TO_TEST_IMAGES_DIR = 'test_images/'\n",
        "names = os.listdir(PATH_TO_TEST_IMAGES_DIR)\n",
        "TEST_IMAGE_PATHS = [os.path.join(PATH_TO_TEST_IMAGES_DIR, name) for name in names]\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "#for im_num, im_path in enumerate(TEST_IMAGE_PATHS[:5], start=1):\n",
        "for im_num, im_path in enumerate(TEST_IMAGE_PATHS, start=1):\n",
        " \n",
        "    # Load in image\n",
        "    image = Image.open(im_path)\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection and draw boxes on image\n",
        "    show_inference(image_np_expanded)\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 429 test images'.format(im_num))\n",
        "\n",
        "    # Record inference time, image name and image dimensions to export\n",
        "    inf_time.append(end_time-start_time)\n",
        "    img_path.append(im_path)\n",
        "    im_dims.append(image_np.shape)\n",
        "    \n",
        "inf_times = pd.DataFrame(([inf_time, img_path, im_dims]))\n",
        "inf_times = inf_times.transpose()\n",
        "# TO DO: Change filepath if using RCNN Resnet 50 or RCNN Inception v2 \n",
        "inf_times.to_csv(\"results/multitaxa_inference_times_rcnn.csv\", index=False, header=(\"time (sec)\", \"filepath\", \"image_dims (h, w, d)\"))\n",
        "#inf_times.to_csv(\"results/multitaxa_inference_times_rcnn_i.csv\", index=False, header=(\"time (sec)\", \"filepath\", \"image_dims (h, w, d)\"))\n",
        "print(inf_times.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3HJoT3kx0a3",
        "colab_type": "text"
      },
      "source": [
        "### Run other images (from individual URLs) through object detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D585u93x4N2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test trained model on test images\n",
        "from PIL import Image\n",
        "\n",
        "# Put your urls here\n",
        "image_urls = [\"https://upload.wikimedia.org/wikipedia/commons/b/be/Batman_%28retouched%29.jpg\",\n",
        "              \"https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/Bela_Lugosi_as_Dracula%2C_anonymous_photograph_from_1931%2C_Universal_Studios.jpg/690px-Bela_Lugosi_as_Dracula%2C_anonymous_photograph_from_1931%2C_Universal_Studios.jpg\"]\n",
        "\n",
        "# Loops through image_urls\n",
        "for im_num, image_url in enumerate(image_urls, start=1):\n",
        "  try:\n",
        "    # Load in image\n",
        "    image_np = url_to_image(image_url)\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection and draw boxes on image\n",
        "    show_inference(image_np_expanded)\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 2 images'.format(im_num))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    _, ax = plt.subplots(figsize=(10, 10))\n",
        "    ax.imshow(show_inference(image_np_expanded))\n",
        "    plt.title('{}) Inference time: {}'.format(im_num, format(end_time-start_time, '.2f')))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4xGqHxjE0mC",
        "colab_type": "text"
      },
      "source": [
        "## Run EOL image bundles through the trained object detector & save results for cropping\n",
        "---\n",
        "Display resulting detection boxes on images and save their coordinates to chiroptera_det_crops.tsv for use cropping EOL images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txd2gUW1btH-",
        "colab_type": "text"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GyG1sxwbsAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cd to train\n",
        "%cd ../../..\n",
        "#%cd train\n",
        "\n",
        "%tensorflow_version 1.0\n",
        "\n",
        "import tensorflow as tf \n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "# For importing/exporting files, working with arrays, etc\n",
        "import os\n",
        "import pathlib\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import zipfile\n",
        "import numpy as np \n",
        "import csv\n",
        "import matplotlib\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# For downloading the images\n",
        "import tempfile\n",
        "from six.moves.urllib.request import urlopen\n",
        "from six import BytesIO\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "\n",
        "# For drawing onto and plotting the images\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from PIL import ImageColor\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from PIL import ImageOps\n",
        "\n",
        "import cv2\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "sys.path.append(\"tf_models/models/research/\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgSOHELob9ir",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare object detection functions and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGdW08UaN1Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change PATH_TO_CKPT below when switching between R-FCN and Faster-RCNN models\n",
        "%cd train\n",
        "# Faster RCNN on Resnet 50\n",
        "#PATH_TO_CKPT = 'tf_models/train_demo/rcnn/finetuned_model' + '/frozen_inference_graph.pb'\n",
        "# Faster RCNN on Inception v2\n",
        "PATH_TO_CKPT = 'tf_models/train_demo/rcnn_i/finetuned_model' + '/frozen_inference_graph.pb'\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = 'labelmap.pbtxt'\n",
        "NUM_CLASSES = 4\n",
        "    \n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "  od_graph_def = tf.compat.v1.GraphDef()\n",
        "  with tf.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "    serialized_graph = fid.read()\n",
        "    od_graph_def.ParseFromString(serialized_graph)\n",
        "    tf.import_graph_def(od_graph_def, name='')\n",
        "    \n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# For loading images into computer-readable format\n",
        "def load_image_into_numpy_array(image):\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Function for loading images from urls\n",
        "def url_to_image(url):\n",
        "  resp = urllib.request.urlopen(url)\n",
        "  image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "  image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  return image\n",
        "\n",
        "# Can change min_score_thresh and max_boxes_to_draw\n",
        "def show_inference(image_np_expanded):\n",
        "  with detection_graph.as_default():\n",
        "    with tf.Session(graph=detection_graph) as sess:\n",
        "      # Definite input and output Tensors for detection_graph\n",
        "      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "      # Each box represents a part of the image where a particular object was detected.\n",
        "      detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "      # Each score represents the level of confidence for each of the objects.\n",
        "      detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "      detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "      num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "      # Optional: adjust score confidence threshold for display\n",
        "      #min_score_thresh = .7\n",
        "\n",
        "      # Actual detection\n",
        "      (boxes, scores, classes, num) = sess.run(\n",
        "          [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "          feed_dict={image_tensor: image_np_expanded})\n",
        "      \n",
        "      # Visualize detection results on images  \n",
        "      # Modified from https://github.com/tensorflow/models/issues/4682\n",
        "      newImage = np.copy(image_np)\n",
        "      im_height, im_width, im_depth = image_np.shape\n",
        "      ymin = int((boxes[0][0][0]*im_height))\n",
        "      xmin = int((boxes[0][0][1]*im_width))\n",
        "      ymax = int((boxes[0][0][2]*im_height))\n",
        "      xmax = int((boxes[0][0][3]*im_width))\n",
        "      # Add labels to boxes\n",
        "      label = np.squeeze(classes[0][0]).astype(np.int32)\n",
        "      fontScale = max(im_width,im_height)/(600)\n",
        "      if label == 1 :\n",
        "        newImage = cv2.putText(newImage, 'Squamata', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (255,199,15)\n",
        "        taxon = \"Squamata\"\n",
        "      elif label == 2 :\n",
        "        newImage = cv2.putText(newImage, 'Coleoptera', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (255,127,0)\n",
        "        taxon = \"Coleoptera\"\n",
        "      elif label == 3 :\n",
        "        newImage = cv2.putText(newImage, 'Anura', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (255,42,22)\n",
        "        taxon = \"Anura\"\n",
        "      elif label == 4 :\n",
        "        newImage = cv2.putText(newImage, 'Carnivora', (xmin, ymax-5), cv2.FONT_HERSHEY_SIMPLEX, fontScale, (255,255,255), 2, cv2.LINE_AA)\n",
        "        box_col = (0,191,255)  \n",
        "        taxon = \"Carnivora\"\n",
        "      # Draw detection boxes on images\n",
        "      newImage = cv2.rectangle(newImage, (xmin, ymax), (xmax, ymin), box_col, 3)\n",
        "\n",
        "      # Export bounding boxes to drive\n",
        "      # TO DO: Change filepath for each taxon/runs abcd\n",
        "      with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/results/squamata_det_crops_20000_a.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  crop_width = xmax-xmin\n",
        "                  crop_height = ymax-ymin\n",
        "                  tsv_writer.writerow([image_url, im_height, im_width, \n",
        "                            xmin, ymin, xmax, ymax, taxon])\n",
        "      return newImage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvGZF-luElUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use URLs from EOL image URL bundles\n",
        "# TO DO: Run for 1 taxon at a time and comment out others\n",
        "# 20,000 Squamata images\n",
        "urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Squamata_20K_breakdown_download_000001.txt'\n",
        "# 20,000 Coleoptera images\n",
        "#urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Coleoptera_20K_breakdown_download_000001.txt'\n",
        "# 20,000 Anura images\n",
        "#urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Anura_20K_breakdown_download_000001.txt'\n",
        "# 20,000 Carnivora images\n",
        "#urls = 'https://editors.eol.org/other_files/bundle_images/files/images_for_Carnivora_20K_breakdown_download_000001.txt'\n",
        "\n",
        "df = pd.read_csv(urls)\n",
        "df.columns = [\"link\"]\n",
        "pd.DataFrame.head(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpT10kRyF69m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write header row of output crops file\n",
        "# TO DO: Change file name for each taxon/run abcd if doing 4 batches\n",
        "with open('/content/drive/My Drive/fall19_smithsonian_informatics/train/results/squamata_det_crops_20000_a.tsv', 'a') as out_file:\n",
        "                  tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "                  tsv_writer.writerow([\"image_url\", \"im_height\", \"im_width\", \n",
        "                            \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"class\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbbHfPXIE6QE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test trained model on test images\n",
        "from PIL import Image\n",
        "\n",
        "# Set number of seconds to timeout if image url taking too long to open\n",
        "import socket\n",
        "socket.setdefaulttimeout(10)\n",
        "\n",
        "# Loops through first 5 image urls from the text file\n",
        "for i, row in df.head(5).itertuples(index=True, name='Pandas'):\n",
        "\n",
        "# For ranges of rows or all rows, use df.iloc\n",
        "# Can be useful if running detection in batches\n",
        "#for i, row in df.iloc[:5000].iterrows():\n",
        "\n",
        "  try:\n",
        "    # Load in image\n",
        "    image_url = df.get_value(i, \"link\")\n",
        "    image_np = url_to_image(image_url)\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Record inference time\n",
        "    start_time = time.time()\n",
        "    # Detection and draw boxes on image\n",
        "    show_inference(image_np_expanded)\n",
        "    end_time = time.time()\n",
        "    # Display progress message after each image\n",
        "    print('Detection complete in {} of 20000 test images'.format(i+1))\n",
        "\n",
        "    # Plot and show detection boxes on images\n",
        "    # If running detection on >50 images, comment out this portion\n",
        "    #_, ax = plt.subplots(figsize=(10, 10))\n",
        "    #ax.imshow(show_inference(image_np_expanded))\n",
        "    #plt.title('{}) Inference time: {}'.format(i+1, format(end_time-start_time, '.2f')))\n",
        "\n",
        "  except:\n",
        "    print('Check if URL from {} is valid'.format(image_url))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}